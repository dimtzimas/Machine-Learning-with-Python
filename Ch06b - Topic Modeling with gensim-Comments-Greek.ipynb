{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Retrieval\n",
    "\n",
    "This may not work on windows so just use the extracted NIPS papers data from the __`nipstxt/`__ folder present in the same directory as this notebook which already has the data pre-downloaded and extracted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://cs.nyu.edu/~roweis/data/nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tar: Error opening archive: Failed to open 'nips12raw_str602.tgz'\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf nips12raw_str602.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['idx', 'MATLAB_NOTES', 'nips00', 'nips01', 'nips02', 'nips03', 'nips04', 'nips05', 'nips06', 'nips07', 'nips08', 'nips09', 'nips10', 'nips11', 'nips12', 'nips12raw_str602', 'nips16_01.txt', 'nips16_02.txt', 'nips16_03.txt', 'nips16_04.txt', 'orig', 'RAW_DATA_NOTES', 'README_yann']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'nipstxt/'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'nipstxt/idx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9f174e557243>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfile_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r+'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpapers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'nipstxt/idx'"
     ]
    }
   ],
   "source": [
    "# Read all texts into a list.\n",
    "# Each paper is in its own text file, hence we need to use file-reading functions from Python.\n",
    "papers = []\n",
    "\n",
    "file_names = os.listdir(DATA_PATH)\n",
    "for file_name in file_names:\n",
    "    with open(DATA_PATH + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "    papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-bdc504897478>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# look at a fragment of text from one of the research papers to get an idea.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpapers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# look at a fragment of text from one of the research papers to get an idea.\n",
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Wrangling (preprocessing)\n",
    "We keep things simple here and perform tokenization, lemmatizing nouns, and removing stopwords and any terms having a single character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'papers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'papers' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "            \n",
    "    return norm_papers\n",
    "    \n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nipstxt', 'ustar', 'roweis', 'rowegrp', 'nipstxt', 'nips00', 'ustar', 'roweis', 'rowegrp', 'nipstxt', 'nips00', 'txt', 'ustar', 'roweis', 'rowegrp', 'neural', 'net', 'traditional', 'classifier', 'william', 'huang', 'richard', 'lippmann', 'mit', 'lincoln', 'laboratory', 'lexington', 'usa', 'abstract', 'previous', 'work', 'net', 'continuous', 'valued', 'input', 'led', 'generative', 'procedure', 'construct', 'convex', 'decision', 'region', 'two', 'layer', 'percepttons', 'one', 'hidden', 'layer', 'arbitrary', 'decision']\n"
     ]
    }
   ],
   "source": [
    "# viewing a processed paper\n",
    "print(norm_papers[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation with Feature Engineering\n",
    "## gensim:\n",
    "Τhis framework was built for topic modeling.\n",
    "We can do amazing things with this framework, including text  similarity, semantic analytics, topic models, and text summarization. \n",
    "\n",
    "Besides this, Gensim offers a lot of capabilities and more flexibility than Scikit-Learn to build, evaluate, and tune topic models, which we will see very shortly. We build topic models using the following methods in this section.\n",
    "\n",
    "• Latent Semantic Indexing (LSI)\n",
    "• Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nipstxt', 'ustar_roweis', 'rowegrp', 'nipstxt_nips00', 'ustar_roweis', 'rowegrp', 'nipstxt_nips00', 'txt_ustar', 'roweis_rowegrp', 'neural_net', 'traditional', 'classifier', 'william', 'huang', 'richard_lippmann', 'mit_lincoln', 'laboratory', 'lexington', 'usa_abstract', 'previous_work', 'net', 'continuous_valued', 'input', 'led', 'generative', 'procedure', 'construct', 'convex', 'decision_region', 'two', 'layer_percepttons', 'one', 'hidden_layer', 'arbitrary', 'decision_region', 'three', 'layer_percepttons', 'two', 'hidden_layer', 'demonstrate', 'two', 'layer', 'perceptton', 'classifier', 'trained', 'back_propagation', 'form', 'convex', 'disjoint', 'decision_region']\n"
     ]
    }
   ],
   "source": [
    "# Before feature engineering and vectorization, we want to extract some useful bi-gram\n",
    "# based phrases from our research papers and remove some unnecessary terms.\n",
    "import gensim\n",
    "\n",
    "# We start by extracting and generating words and bi-grams as phrases for\n",
    "# each tokenized research paper. We can build this phrase generation model easily with\n",
    "# the following code and test it on a sample paper.\n",
    "\n",
    "bigram = gensim.models.Phrases(norm_papers, min_count=20, threshold=20, delimiter=b'_') # higher threshold fewer phrases.\n",
    "bigram_model = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "print(bigram_model[norm_papers[0]][:50])\n",
    "\n",
    "# we have single words as well as bi-grams (two words separated by an underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample word to number mappings: [(0, '000000e'), (1, '000b'), (2, '000branches'), (3, '000d'), (4, '000na'), (5, '000p'), (6, '000th'), (7, '000training'), (8, '001000_'), (9, '0016a'), (10, '002o'), (11, '002t'), (12, '00425z'), (13, '005m'), (14, '005r')]\n",
      "Total Vocabulary Size: 80976\n"
     ]
    }
   ],
   "source": [
    "# Let’s generate phrases for all our tokenized research papers and build a vocabulary\n",
    "# that will help us obtain a unique term/phrase to number mapping\n",
    "# (since machine ordeep learning only works on numeric tensors).\n",
    "\n",
    "norm_corpus_bigrams = [bigram_model[doc] for doc in norm_papers]\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = gensim.corpora.Dictionary(norm_corpus_bigrams)\n",
    "print('Sample word to number mappings:', list(dictionary.items())[:15])\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 80976\n"
     ]
    }
   ],
   "source": [
    "# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n",
    "# We removed all terms that occur fewer than 20 times across all documents and all\n",
    "# terms that occur in more than 60% of all the documents. We are interested in finding\n",
    "# different themes and topics and not recurring themes.\n",
    "\n",
    "## dictionary.filter_extremes(no_below=20, no_above=0.6)\n",
    "print('Total Vocabulary Size:', len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(189, 2), (981, 1), (1222, 1), (1268, 4), (2002, 1), (2157, 1), (2815, 1), (4646, 1), (4703, 4), (4718, 1), (4785, 1), (4975, 1), (5050, 1), (5074, 1), (5174, 1), (5223, 1), (5224, 1), (5245, 1), (5263, 1), (5270, 1), (5341, 1), (5360, 1), (5366, 1), (5549, 1), (5726, 1), (5804, 1), (5895, 1), (6069, 1), (6213, 18), (6356, 1), (6445, 2), (6452, 7), (6490, 1), (6491, 1), (6495, 1), (6528, 3), (6614, 1), (6682, 1), (6763, 1), (6777, 1), (6796, 1), (6917, 1), (7165, 1), (7410, 2), (7415, 1), (7424, 2), (7448, 3), (7453, 3), (7466, 1), (7490, 4)]\n"
     ]
    }
   ],
   "source": [
    "# Transforming corpus into bag of words vectors\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in norm_corpus_bigrams]\n",
    "print(bow_corpus[1][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0k', 2), ('1a', 1), ('1the', 1), ('1y', 4), ('2this', 1), ('30th', 1), ('4this', 1), ('able', 1), ('absolute', 4), ('abstract', 1), ('accelerated', 1), ('acknowledgement_research', 1), ('across', 1), ('acting', 1), ('adam', 1), ('add', 1), ('added', 1), ('additional', 1), ('address', 1), ('addressed', 1), ('adjusts', 1), ('admit', 1), ('admitting', 1), ('affect', 1), ('agree', 1), ('ai_bi', 1), ('aim', 1), ('ala', 1), ('algorithm', 18), ('allows', 1), ('already', 2), ('also', 7), ('alternative', 1), ('alternatively', 1), ('although', 1), ('always', 3), ('amenable', 1), ('amount', 1), ('analogously', 1), ('analysis', 1), ('analyzed', 1), ('andreas', 1), ('answered', 1), ('appear', 2), ('appears', 1), ('appendix', 2), ('applicable', 3), ('application', 3), ('apply', 1), ('approach', 4)]\n"
     ]
    }
   ],
   "source": [
    "# viewing actual terms and their counts\n",
    "print([(dictionary[idx] , freq) for idx, freq in bow_corpus[1][:50]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers: 5\n"
     ]
    }
   ],
   "source": [
    "# total papers in the corpus\n",
    "print('Total number of papers:', len(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our documents are now processed and have a good enough representation with the\n",
    "Bag of Words model to begin modeling.\n",
    "# Topic Models with Latent Semantic Indexing (LSI)\n",
    "A statistical technique to correlate semantically linked terms from corpora. \n",
    "Uses Singular Value Decomposition (SVD) technique.\n",
    "The main principle behind LSI is that similar terms tend to be used in the same context and hence tend to co-occur more.\n",
    "Ability to uncover latent hidden terms that correlate semantically to form topics.\n",
    "\n",
    "It is quite simple to build this model, thanks to Gensim’s clean and concise API (gensim.models.LsiModel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 16min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "TOTAL_TOPICS = 10 # We have explicitly set the number of topics to 10\n",
    "lsi_bow = gensim.models.LsiModel(bow_corpus, id2word=dictionary, num_topics=TOTAL_TOPICS,\n",
    "                                 onepass=True, chunksize=1740, power_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.275*\"network\" + 0.254*\"model\" + 0.202*\"learning\" + 0.196*\"input\" + 0.168*\"function\" + 0.151*\"system\" + 0.151*\"one\" + 0.142*\"data\" + 0.142*\"set\" + 0.140*\"algorithm\" + 0.132*\"time\" + 0.131*\"training\" + 0.128*\"output\" + 0.124*\"using\" + 0.121*\"wa\" + 0.119*\"two\" + 0.112*\"value\" + 0.111*\"weight\" + 0.107*\"state\" + 0.107*\"result\"\n",
      "\n",
      "Topic #2:\n",
      "0.323*\"state\" + -0.240*\"image\" + 0.221*\"algorithm\" + 0.215*\"function\" + 0.206*\"policy\" + 0.194*\"observation\" + -0.182*\"network\" + -0.164*\"caption\" + 0.164*\"model\" + 0.152*\"sample\" + 0.144*\"reinforcement_learning\" + 0.135*\"action\" + 0.132*\"assumption\" + 0.113*\"exploration\" + 0.112*\"complexity\" + -0.104*\"word\" + -0.096*\"data\" + 0.091*\"pomdps\" + -0.090*\"audio\" + -0.087*\"input\"\n",
      "\n",
      "Topic #3:\n",
      "-0.494*\"image\" + -0.360*\"caption\" + -0.197*\"audio\" + -0.183*\"word\" + -0.149*\"model\" + -0.145*\"color\" + 0.127*\"input\" + -0.117*\"segmentation\" + -0.108*\"similarity\" + -0.107*\"spoken\" + 0.096*\"output\" + -0.091*\"text\" + -0.088*\"spectrogram\" + -0.084*\"embedding\" + -0.083*\"observation\" + 0.080*\"system\" + -0.078*\"policy\" + -0.078*\"score\" + -0.078*\"language\" + -0.075*\"supervoxels\"\n",
      "\n",
      "Topic #4:\n",
      "0.228*\"state\" + -0.208*\"model\" + -0.208*\"pairwise\" + -0.205*\"inference\" + -0.180*\"edge\" + -0.171*\"submodular\" + -0.165*\"θi\" + -0.157*\"problem\" + 0.144*\"caption\" + -0.135*\"trwbp\" + 0.131*\"policy\" + -0.128*\"method\" + 0.119*\"observation\" + 0.118*\"learning\" + -0.113*\"cut\" + -0.110*\"marginals\" + -0.110*\"upper_bound\" + -0.109*\"xi\" + -0.105*\"partition\" + 0.105*\"network\"\n",
      "\n",
      "Topic #5:\n",
      "-0.381*\"color\" + -0.231*\"neuron\" + 0.206*\"caption\" + -0.204*\"supervoxels\" + -0.196*\"segmentation\" + -0.183*\"fig\" + 0.170*\"model\" + -0.163*\"voxels\" + -0.161*\"stack\" + -0.136*\"brainbow\" + -0.136*\"supp\" + -0.127*\"et_al\" + -0.126*\"channel\" + 0.121*\"word\" + 0.113*\"audio\" + -0.109*\"intensity\" + 0.106*\"network\" + -0.106*\"size\" + -0.090*\"single\" + -0.090*\"number\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Once the model is built, we can view the major topics or themes in our corpus by\n",
    "# using the following code.\n",
    "for topic_id, topic in lsi_bow.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on existing research and interpretations, considering we are reducing\n",
    "the dimensionality here to a 10-dimensional space based on the number of topics, the sign on each term indicates a sense of direction or orientation in the vector space for a particular topic. The higher the weight, the more important the contribution. So similar correlated terms have the same sign or direction. \n",
    "\n",
    "Hence, it is perfectly possible for a topic to have two different sub-themes based on the sign or orientation of terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('network', 0.275), ('model', 0.254), ('learning', 0.202), ('input', 0.196), ('function', 0.168), ('system', 0.151), ('one', 0.151), ('data', 0.142), ('set', 0.142), ('algorithm', 0.14), ('time', 0.132), ('training', 0.131), ('output', 0.128), ('using', 0.124), ('wa', 0.121), ('two', 0.119), ('value', 0.112), ('weight', 0.111), ('state', 0.107), ('result', 0.107)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('state', 0.323), ('algorithm', 0.221), ('function', 0.215), ('policy', 0.206), ('observation', 0.194), ('model', 0.164), ('sample', 0.152), ('reinforcement_learning', 0.144), ('action', 0.135), ('assumption', 0.132), ('exploration', 0.113), ('complexity', 0.112), ('pomdps', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.24), ('network', -0.182), ('caption', -0.164), ('word', -0.104), ('data', -0.096), ('audio', -0.09), ('input', -0.087)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('input', 0.127), ('output', 0.096), ('system', 0.08)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.494), ('caption', -0.36), ('audio', -0.197), ('word', -0.183), ('model', -0.149), ('color', -0.145), ('segmentation', -0.117), ('similarity', -0.108), ('spoken', -0.107), ('text', -0.091), ('spectrogram', -0.088), ('embedding', -0.084), ('observation', -0.083), ('policy', -0.078), ('score', -0.078), ('language', -0.078), ('supervoxels', -0.075)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('state', 0.228), ('caption', 0.144), ('policy', 0.131), ('observation', 0.119), ('learning', 0.118), ('network', 0.105)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('model', -0.208), ('pairwise', -0.208), ('inference', -0.205), ('edge', -0.18), ('submodular', -0.171), ('θi', -0.165), ('problem', -0.157), ('trwbp', -0.135), ('method', -0.128), ('cut', -0.113), ('marginals', -0.11), ('upper_bound', -0.11), ('xi', -0.109), ('partition', -0.105)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('caption', 0.206), ('model', 0.17), ('word', 0.121), ('audio', 0.113), ('network', 0.106)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('color', -0.381), ('neuron', -0.231), ('supervoxels', -0.204), ('segmentation', -0.196), ('fig', -0.183), ('voxels', -0.163), ('stack', -0.161), ('brainbow', -0.136), ('supp', -0.136), ('et_al', -0.127), ('channel', -0.126), ('intensity', -0.109), ('size', -0.106), ('single', -0.09), ('number', -0.09)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# separate these terms and try to interpret the topics again\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    for term, wt in lsi_bow.show_topic(n, topn=20):\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((80976, 5), (5,), (5, 5))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_topic = lsi_bow.projection.u\n",
    "singular_values = lsi_bow.projection.s\n",
    "topic_document = (gensim.matutils.corpus2dense(lsi_bow[bow_corpus], len(singular_values)).T / singular_values).T\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.316</td>\n",
       "      <td>-0.317</td>\n",
       "      <td>-0.847</td>\n",
       "      <td>0.287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001</td>\n",
       "      <td>0.850</td>\n",
       "      <td>-0.320</td>\n",
       "      <td>0.411</td>\n",
       "      <td>-0.077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.137</td>\n",
       "      <td>-0.405</td>\n",
       "      <td>-0.198</td>\n",
       "      <td>-0.882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.399</td>\n",
       "      <td>-0.795</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      T1     T2     T3     T4     T5\n",
       "0  1.000 -0.001  0.001  0.000  0.000\n",
       "1  0.001  0.316 -0.317 -0.847  0.287\n",
       "2  0.001  0.850 -0.320  0.411 -0.077\n",
       "3  0.001 -0.137 -0.405 -0.198 -0.882\n",
       "4  0.001 -0.399 -0.795  0.273  0.366"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOTAL_TOPICS = 5\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
    "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "document_topics.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #1:\n",
      "Dominant Topics (top 3): ['T4', 'T3', 'T2']\n",
      "Paper Summary:\n",
      "Cooperative Graphical Models\n",
      "Josip Djolonga\n",
      "Dept. of Computer Science, ETH Zurich ¨\n",
      "josipd@inf.ethz.ch\n",
      "Stefanie Jegelka\n",
      "CSAIL, MIT\n",
      "stefje@mit.edu\n",
      "Sebastian Tschiatschek\n",
      "Dept. of Computer Science, ETH Zurich ¨\n",
      "stschia@inf.ethz.ch\n",
      "Andreas Krause\n",
      "Dept. of Computer Science, ETH Zurich ¨\n",
      "krausea@inf.ethz.ch\n",
      "Abstract\n",
      "We study a rich family of distributions that capture variable interactions significantly more expressive than those representable with low-treewidth or pairwise\n",
      "graphical models, or log-s\n",
      "\n",
      "Document #3:\n",
      "Dominant Topics (top 3): ['T5', 'T3', 'T4']\n",
      "Paper Summary:\n",
      "Automated scalable segmentation of neurons from\n",
      "multispectral images\n",
      "Uygar Sümbül\n",
      "Grossman Center for the Statistics of Mind\n",
      "and Dept. of Statistics, Columbia University\n",
      "Douglas Roossien Jr.\n",
      "University of Michigan Medical School\n",
      "Fei Chen\n",
      "MIT Media Lab and McGovern Institute\n",
      "Nicholas Barry\n",
      "MIT Media Lab and McGovern Institute\n",
      "Edward S. Boyden\n",
      "MIT Media Lab and McGovern Institute\n",
      "Dawen Cai\n",
      "University of Michigan Medical School\n",
      "John P. Cunningham\n",
      "Grossman Center for the Statistics of Mind\n",
      "and Dept.\n",
      "\n",
      "Document #4:\n",
      "Dominant Topics (top 3): ['T3', 'T2', 'T5']\n",
      "Paper Summary:\n",
      "Unsupervised Learning of Spoken Language with\n",
      "Visual Context\n",
      "David Harwath, Antonio Torralba, and James R. Glass\n",
      "Computer Science and Artificial Intelligence Laboratory\n",
      "Massachusetts Institute of Technology\n",
      "Cambridge, MA 02115\n",
      "{dharwath, torralba, jrg}@csail.mit.edu\n",
      "Abstract\n",
      "Humans learn to speak before they can read or write, so why can’t computers\n",
      "do the same? In this paper, we present a deep neural network model capable of\n",
      "rudimentary spoken language acquisition using untranscribed audio trai\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document_numbers = [13, 250, 500]\n",
    "document_numbers = [1, 3, 4]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()\n",
    "    \n",
    "# the LSI model is quite effective, although a tad difficult to\n",
    "# interpret based on the positive and negative weights, which often make things confusing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing LSI Topic Models from Scratch\n",
    "Match the previous topic model’s output from Gensim very closely.\n",
    "The heart of LSI models involves Singular Value Decomposition (SVD).\n",
    "SVD is a very powerful mathematical operation and we will see more of this during document summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(80976, 5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[4., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 2.],\n",
       "       [0., 0., 0., 0., 2.]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td_matrix = gensim.matutils.corpus2dense(corpus=bow_corpus, num_terms=len(dictionary))\n",
    "print(td_matrix.shape)\n",
    "td_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total vocabulary size: 80976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['000000e', '000b', '000branches', ..., 'vinyals', 'website',\n",
       "       'zitnick'], dtype='<U71')"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = np.array(list(dictionary.values()))\n",
    "print('Total vocabulary size:', len(vocabulary))\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000000e'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "k must be between 1 and min(A.shape), k=5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-78-55cc2d66bded>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msvds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvds\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTOTAL_TOPICS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxiter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mterm_topic\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0msingular_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\linalg\\eigen\\arpack\\arpack.py\u001b[0m in \u001b[0;36msvds\u001b[1;34m(A, k, ncv, tol, which, v0, maxiter, return_singular_vectors, solver)\u001b[0m\n\u001b[0;32m   1815\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1816\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1817\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"k must be between 1 and min(A.shape), k=%d\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1819\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLinearOperator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: k must be between 1 and min(A.shape), k=5"
     ]
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "u, s, vt = svds(td_matrix, k=TOTAL_TOPICS, maxiter=10000)\n",
    "term_topic = u\n",
    "singular_values = s\n",
    "topic_document = vt\n",
    "term_topic.shape, singular_values.shape, topic_document.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 7756)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt_weights = term_topic.transpose() * singular_values[:, None]\n",
    "tt_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('training', 92.618), ('task', 80.732), ('pattern', 70.619), ('classifier', 56.989), ('control', 50.677), ('rule', 45.926), ('action', 41.202), ('neuron', 38.193)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -188.488), ('vector', -85.973), ('node', -54.376), ('recognition', -53.232), ('sequence', -50.351), ('circuit', -45.394), ('cell', -44.811), ('hmm', -34.086), ('character', -34.022), ('chip', -32.16), ('matrix', -32.093), ('structure', -30.993)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('word', 78.347), ('neuron', 69.793), ('stimulus', 63.234), ('feature', 53.819), ('distribution', 53.119), ('response', 30.954), ('state', 29.343), ('probability', 29.099), ('estimate', 28.908)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('node', -173.277), ('circuit', -93.0), ('chip', -73.593), ('classifier', -58.717), ('current', -55.844), ('voltage', -53.489), ('control', -51.708), ('rule', -45.293), ('layer', -40.265), ('analog', -38.344), ('tree', -33.483)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('pattern', 116.971), ('rule', 69.783), ('state', 68.605), ('class', 64.259), ('cell', 59.979), ('feature', 59.606), ('node', 49.175), ('neuron', 47.998), ('probability', 43.812), ('classifier', 42.612), ('image', 42.061)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('signal', -93.805), ('control', -91.041), ('training', -77.88), ('noise', -64.397), ('word', -62.392), ('motion', -56.699), ('task', -53.883), ('target', -44.765), ('circuit', -44.129)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('unit', 117.727), ('distribution', 66.719), ('vector', 58.881), ('approximation', 50.931), ('variable', 50.83), ('equation', 46.229), ('noise', 44.247), ('matrix', 42.214)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('word', -147.792), ('training', -113.693), ('classifier', -107.386), ('recognition', -73.948), ('feature', -63.454), ('state', -60.126), ('pattern', -59.562), ('cell', -53.768), ('task', -53.693), ('classification', -44.936), ('class', -43.161), ('neuron', -41.092)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 220.116), ('image', 92.39), ('chip', 44.422), ('unit', 41.922), ('object', 39.001), ('circuit', 30.444), ('memory', 26.475), ('analog', 25.207), ('activation', 24.953), ('bit', 22.997), ('net', 22.699)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cell', -285.803), ('response', -40.216), ('rat', -35.975), ('distribution', -33.085), ('probability', -29.79), ('stimulus', -27.789), ('class', -24.02), ('cortical', -22.185), ('firing', -21.66)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('image', 209.793), ('state', 170.207), ('unit', 129.108), ('object', 82.185), ('action', 72.136), ('visual', 59.502), ('motion', 50.605), ('feature', 48.665), ('control', 47.427), ('task', 46.496), ('cell', 42.366), ('representation', 40.564)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('neuron', -130.053), ('training', -88.668), ('class', -85.213), ('classifier', -81.921), ('vector', -57.532), ('node', -56.341), ('distribution', -51.622), ('classification', -47.645)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('image', 215.858), ('feature', 55.647), ('neuron', 48.494), ('pixel', 35.095), ('object', 33.585), ('state', 32.544), ('distribution', 29.977), ('face', 29.256), ('estimate', 27.555)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -341.829), ('pattern', -90.771), ('layer', -65.337), ('hidden_unit', -61.12), ('net', -60.035), ('training', -56.742), ('activation', -54.268), ('rule', -53.377), ('word', -38.903), ('connection', -34.618), ('architecture', -28.439)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('image', 229.287), ('feature', 121.397), ('unit', 79.44), ('object', 76.204), ('training', 75.152), ('classifier', 59.872), ('class', 52.527), ('classification', 46.696), ('layer', 45.149), ('recognition', 44.192), ('representation', 40.179), ('pattern', 39.252)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -364.388), ('neuron', -127.022), ('action', -109.245), ('control', -75.369), ('policy', -63.103), ('step', -47.226), ('dynamic', -46.907), ('reinforcement_learning', -42.747)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('neuron', 306.151), ('cell', 249.243), ('response', 119.758), ('stimulus', 106.762), ('activity', 73.499), ('spike', 62.039), ('pattern', 60.957), ('circuit', 60.602), ('synaptic', 60.282), ('signal', 56.665), ('firing', 56.597), ('visual', 55.571), ('cortical', 48.867)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('state', -161.465), ('training', -117.32), ('class', -68.732), ('vector', -59.558), ('classifier', -52.589), ('action', -52.113), ('word', -49.239)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: []\n",
      "--------------------------------------------------\n",
      "Direction 2: [('unit', -260.793), ('state', -258.146), ('training', -227.312), ('neuron', -215.681), ('pattern', -197.232), ('image', -175.735), ('vector', -170.154), ('feature', -151.547), ('cell', -148.138), ('layer', -133.593), ('task', -122.389), ('class', -117.849), ('probability', -110.526), ('signal', -108.232), ('step', -105.202), ('response', -104.465), ('representation', -103.255), ('noise', -100.573), ('rule', -99.611), ('distribution', -98.973)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_terms = 20\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(tt_weights), axis=1)[:, :top_terms]\n",
    "topic_keyterm_weights = np.array([tt_weights[row, columns] \n",
    "                             for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    terms, weights = topic_keyterms_weights[n]\n",
    "    term_weights = sorted([(t, w) for t, w in zip(terms, weights)], \n",
    "                          key=lambda row: -abs(row[1]))\n",
    "    for term, wt in term_weights:\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.005</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.077</td>\n",
       "      <td>0.009</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.011</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.014</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.048</td>\n",
       "      <td>-0.024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.061</td>\n",
       "      <td>0.050</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.020</td>\n",
       "      <td>-0.032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.137</td>\n",
       "      <td>-0.087</td>\n",
       "      <td>-0.307</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.022</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.183</td>\n",
       "      <td>-0.085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.042</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.008</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.019</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.071</td>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.027</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>-0.027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.031</td>\n",
       "      <td>-0.029</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.012</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.056</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.059</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>-0.017</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>0.030</td>\n",
       "      <td>-0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.010</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.003</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>-0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       T1     T2     T3     T4     T5     T6     T7     T8     T9    T10\n",
       "0  -0.005  0.000  0.072  0.030  0.018  0.010 -0.077  0.009 -0.002 -0.038\n",
       "1  -0.007  0.001  0.017  0.013  0.016 -0.011  0.008 -0.022 -0.000 -0.022\n",
       "2   0.014  0.008 -0.009 -0.008 -0.000  0.003  0.003 -0.004  0.028 -0.021\n",
       "3  -0.016 -0.003  0.011 -0.002 -0.044  0.023  0.003  0.011  0.048 -0.024\n",
       "4  -0.036 -0.061  0.050 -0.006 -0.058  0.003  0.013 -0.013  0.020 -0.032\n",
       "5  -0.020 -0.040  0.137 -0.087 -0.307  0.019 -0.022 -0.003  0.183 -0.085\n",
       "6   0.012 -0.002  0.014 -0.012  0.042 -0.017  0.003 -0.019  0.034 -0.020\n",
       "7   0.013 -0.003 -0.005 -0.004  0.001 -0.008 -0.018  0.007 -0.006 -0.012\n",
       "8   0.011  0.011  0.019 -0.020 -0.009 -0.018  0.003 -0.017  0.071 -0.033\n",
       "9  -0.006 -0.005 -0.006  0.036  0.008  0.016  0.041  0.027 -0.007 -0.027\n",
       "10 -0.023  0.015  0.060  0.009  0.031 -0.029 -0.033 -0.012  0.011 -0.046\n",
       "11 -0.021 -0.046 -0.021  0.028  0.004  0.016  0.031  0.006  0.003 -0.023\n",
       "12 -0.021 -0.056  0.024 -0.002  0.059 -0.023 -0.002 -0.017  0.034 -0.034\n",
       "13  0.011 -0.018 -0.009  0.018  0.056 -0.019 -0.021 -0.019  0.030 -0.041\n",
       "14 -0.010  0.005  0.013  0.020  0.003  0.003 -0.023 -0.000 -0.004 -0.015"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "document_topics = pd.DataFrame(np.round(topic_document.T, 3), \n",
    "                               columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "document_topics.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #13:\n",
      "Dominant Topics (top 3): ['T5', 'T10', 'T9']\n",
      "Paper Summary:\n",
      "9 \n",
      "Stochastic Learning Networks and their Electronic Implementation \n",
      "Joshua Alspector*, Robert B. Allen, Victor Hut, and Srinagesh Satyanarayana \n",
      "Bell Communications Research, Morristown, NJ 07960 \n",
      "ABSTRACT\n",
      "We describe a family of learning algorithms that operate on a recurrent, symmetrically \n",
      "connected, neuromorphic network that, like the Boltzmann machine, settles in the \n",
      "presence of noise. These networks learn by modifying synaptic connection strengths on \n",
      "the basis of correlations seen loca\n",
      "\n",
      "Document #250:\n",
      "Dominant Topics (top 3): ['T6', 'T8', 'T3']\n",
      "Paper Summary:\n",
      "266 Zemel, Mozer and Hinton \n",
      "TRAFFIC: Recognizing Objects Using \n",
      "Hierarchical Reference Frame Transformations \n",
      "Richard S. Zemel \n",
      "Computer Science Dept. \n",
      "University of Toronto \n",
      "Toronto, ONT M5S 1A4 \n",
      "Michael C. Mozer \n",
      "Computer Science Dept. \n",
      "University of Colorado \n",
      "Boulder, CO 80309-0430 \n",
      "Geoffrey E. Hinton \n",
      "Computer Science Dept. \n",
      "University of Toronto \n",
      "Toronto, ONT M5S 1A4 \n",
      "ABSTRACT \n",
      "We describe a model that can recognize two-dimensional shapes in \n",
      "an unsegmented image, independent of their orie\n",
      "\n",
      "Document #500:\n",
      "Dominant Topics (top 3): ['T2', 'T3', 'T1']\n",
      "Paper Summary:\n",
      "Constrained Optimization Applied to the \n",
      "Parameter Setting Problem for Analog Circuits \n",
      "David Kirk, Kurt Fleischer, Lloyd Watts Alan Bart \n",
      "Computer Graphics 350-74 \n",
      "California Institute of Technology \n",
      "Pasadena, CA 91125 \n",
      "Abstract \n",
      "We use constrained optimization to select operating parameters for two \n",
      "circuits: a simple 3-transistor square root circuit, and an analog VLSI \n",
      "artificial cochlea. This automated method uses computer controlled mea- \n",
      "surement and test equipment to choose chip paramet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_numbers = [13, 250, 500]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(document_topics.columns[np.argsort(-np.absolute(document_topics.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models with Latent Dirichlet Allocation (LDA)\n",
    "Generative probabilistic model in which each document is assumed to have a combination of topics similar to a probabilistic..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.LdaModel(corpus=bow_corpus, id2word=dictionary, chunksize=1740, \n",
    "                                   alpha='auto', eta='auto', random_state=42,\n",
    "                                   iterations=500, num_topics=TOTAL_TOPICS, \n",
    "                                   passes=20, eval_every=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "0.003*\"color\" + 0.002*\"image\" + 0.002*\"supervoxels\" + 0.002*\"segmentation\" + 0.001*\"voxels\" + 0.001*\"stack\" + 0.001*\"neuron\" + 0.001*\"fig\" + 0.001*\"brainbow\" + 0.001*\"supp\" + 0.001*\"channel\" + 0.001*\"intensity\" + 0.001*\"network\" + 0.001*\"size\" + 0.001*\"et_al\" + 0.001*\"method\" + 0.001*\"voxel\" + 0.001*\"model\" + 0.001*\"input\" + 0.001*\"set\"\n",
      "\n",
      "Topic #2:\n",
      "0.009*\"network\" + 0.008*\"model\" + 0.006*\"learning\" + 0.006*\"input\" + 0.005*\"function\" + 0.005*\"one\" + 0.005*\"system\" + 0.005*\"set\" + 0.005*\"data\" + 0.004*\"algorithm\" + 0.004*\"time\" + 0.004*\"training\" + 0.004*\"output\" + 0.004*\"using\" + 0.004*\"wa\" + 0.004*\"two\" + 0.004*\"value\" + 0.004*\"weight\" + 0.003*\"state\" + 0.003*\"result\"\n",
      "\n",
      "Topic #3:\n",
      "0.004*\"caption\" + 0.002*\"audio\" + 0.002*\"pairwise\" + 0.002*\"image\" + 0.002*\"inference\" + 0.002*\"submodular\" + 0.001*\"θi\" + 0.001*\"trwbp\" + 0.001*\"spoken\" + 0.001*\"marginals\" + 0.001*\"spectrogram\" + 0.001*\"pmap\" + 0.001*\"edge\" + 0.001*\"embedding\" + 0.001*\"word\" + 0.001*\"cut\" + 0.001*\"sdp\" + 0.001*\"annotation\" + 0.001*\"model\" + 0.001*\"convex\"\n",
      "\n",
      "Topic #4:\n",
      "0.002*\"policy\" + 0.002*\"observation\" + 0.001*\"exploration\" + 0.001*\"reinforcement_learning\" + 0.001*\"pomdps\" + 0.001*\"dfs\" + 0.001*\"state\" + 0.001*\"elimination\" + 0.001*\"bandit\" + 0.001*\"assumption\" + 0.001*\"consensus\" + 0.001*\"contextual\" + 0.001*\"reactive\" + 0.001*\"near_optimal\" + 0.001*\"action\" + 0.001*\"mdps\" + 0.001*\"sh\" + 0.001*\"lsvee\" + 0.001*\"sample\" + 0.001*\"pac\"\n",
      "\n",
      "Topic #5:\n",
      "0.000*\"network\" + 0.000*\"model\" + 0.000*\"learning\" + 0.000*\"data\" + 0.000*\"input\" + 0.000*\"function\" + 0.000*\"time\" + 0.000*\"algorithm\" + 0.000*\"one\" + 0.000*\"training\" + 0.000*\"output\" + 0.000*\"using\" + 0.000*\"wa\" + 0.000*\"system\" + 0.000*\"two\" + 0.000*\"weight\" + 0.000*\"problem\" + 0.000*\"set\" + 0.000*\"result\" + 0.000*\"state\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Viewing the topics in our trained topic model is quite easy and we can generate them with the following code.\n",
    "for topic_id, topic in lda_model.print_topics(num_topics=10, num_words=20):\n",
    "    print('Topic #'+str(topic_id+1)+':')\n",
    "    print(topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics are definitely easier to understand and interpret than the LSI model, since\n",
    "all the weights are the same sign and tell us the importance of each term in the topic. \n",
    "\n",
    "We can also view the overall mean coherence score of the model. Topic coherence is a complex topic in its own and it can be used to measure the quality of topic models to some extent. Typically, a set of statements is said to be coherent if they support each other.\n",
    "\n",
    "Topic models are unsupervised learning based models that are trained on unstructured text data, making it difficult to measure the quality of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score: -0.8064614396659195\n"
     ]
    }
   ],
   "source": [
    "topics_coherences = lda_model.top_topics(bow_corpus, topn=20)\n",
    "avg_coherence_score = np.mean([item[1] for item in topics_coherences])\n",
    "print('Avg. Coherence Score:', avg_coherence_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics with Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "[('network', 0.0), ('model', 0.0), ('learning', 0.0), ('data', 0.0), ('input', 0.0), ('function', 0.0), ('time', 0.0), ('algorithm', 0.0), ('one', 0.0), ('training', 0.0), ('output', 0.0), ('using', 0.0), ('wa', 0.0), ('system', 0.0), ('two', 0.0), ('weight', 0.0), ('problem', 0.0), ('set', 0.0), ('result', 0.0), ('state', 0.0)]\n",
      "\n",
      "Topic #2:\n",
      "[('network', 0.009), ('model', 0.008), ('learning', 0.006), ('input', 0.006), ('function', 0.005), ('one', 0.005), ('system', 0.005), ('set', 0.005), ('data', 0.005), ('algorithm', 0.004), ('time', 0.004), ('training', 0.004), ('output', 0.004), ('using', 0.004), ('wa', 0.004), ('two', 0.004), ('value', 0.004), ('weight', 0.004), ('state', 0.003), ('result', 0.003)]\n",
      "\n",
      "Topic #3:\n",
      "[('policy', 0.002), ('observation', 0.002), ('exploration', 0.001), ('reinforcement_learning', 0.001), ('pomdps', 0.001), ('dfs', 0.001), ('state', 0.001), ('elimination', 0.001), ('bandit', 0.001), ('assumption', 0.001), ('consensus', 0.001), ('contextual', 0.001), ('reactive', 0.001), ('near_optimal', 0.001), ('action', 0.001), ('mdps', 0.001), ('sh', 0.001), ('lsvee', 0.001), ('sample', 0.001), ('pac', 0.001)]\n",
      "\n",
      "Topic #4:\n",
      "[('color', 0.003), ('image', 0.002), ('supervoxels', 0.002), ('segmentation', 0.002), ('voxels', 0.001), ('stack', 0.001), ('neuron', 0.001), ('fig', 0.001), ('brainbow', 0.001), ('supp', 0.001), ('channel', 0.001), ('intensity', 0.001), ('network', 0.001), ('size', 0.001), ('et_al', 0.001), ('method', 0.001), ('voxel', 0.001), ('model', 0.001), ('input', 0.001), ('set', 0.001)]\n",
      "\n",
      "Topic #5:\n",
      "[('caption', 0.004), ('audio', 0.002), ('pairwise', 0.002), ('image', 0.002), ('inference', 0.002), ('submodular', 0.002), ('θi', 0.001), ('trwbp', 0.001), ('spoken', 0.001), ('marginals', 0.001), ('spectrogram', 0.001), ('pmap', 0.001), ('edge', 0.001), ('embedding', 0.001), ('word', 0.001), ('cut', 0.001), ('sdp', 0.001), ('annotation', 0.001), ('model', 0.001), ('convex', 0.001)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics_with_wts = [item[0] for item in topics_coherences]\n",
    "print('LDA Topics with Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([(term, round(wt, 3)) for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics without Weights\n",
      "==================================================\n",
      "Topic #1:\n",
      "['network', 'model', 'learning', 'data', 'input', 'function', 'time', 'algorithm', 'one', 'training', 'output', 'using', 'wa', 'system', 'two', 'weight', 'problem', 'set', 'result', 'state']\n",
      "\n",
      "Topic #2:\n",
      "['network', 'model', 'learning', 'input', 'function', 'one', 'system', 'set', 'data', 'algorithm', 'time', 'training', 'output', 'using', 'wa', 'two', 'value', 'weight', 'state', 'result']\n",
      "\n",
      "Topic #3:\n",
      "['policy', 'observation', 'exploration', 'reinforcement_learning', 'pomdps', 'dfs', 'state', 'elimination', 'bandit', 'assumption', 'consensus', 'contextual', 'reactive', 'near_optimal', 'action', 'mdps', 'sh', 'lsvee', 'sample', 'pac']\n",
      "\n",
      "Topic #4:\n",
      "['color', 'image', 'supervoxels', 'segmentation', 'voxels', 'stack', 'neuron', 'fig', 'brainbow', 'supp', 'channel', 'intensity', 'network', 'size', 'et_al', 'method', 'voxel', 'model', 'input', 'set']\n",
      "\n",
      "Topic #5:\n",
      "['caption', 'audio', 'pairwise', 'image', 'inference', 'submodular', 'θi', 'trwbp', 'spoken', 'marginals', 'spectrogram', 'pmap', 'edge', 'embedding', 'word', 'cut', 'sdp', 'annotation', 'model', 'convex']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# view the topics as a list of terms without the weights when we want to understand the context or theme conveyed by each topic\n",
    "print('LDA Topics without Weights')\n",
    "print('='*50)\n",
    "for idx, topic in enumerate(topics_with_wts):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for wt, term in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use perplexity and coherence scores as measures to evaluate the topic\n",
    "model. Typically, lower the perplexity, the better the model. Similarly, the lower the\n",
    "UMass score and the higher the Cv score in coherence, the better the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.374848486682118\n",
      "Avg. Coherence Score (UMass): -0.8064614396659195\n",
      "Model Perplexity: -8.494962470791346\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                      texts=norm_corpus_bigrams,\n",
    "                                                      dictionary=dictionary, \n",
    "                                                      coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda = gensim.models.CoherenceModel(model=lda_model, corpus=bow_corpus, \n",
    "                                                         texts=norm_corpus_bigrams,\n",
    "                                                         dictionary=dictionary, \n",
    "                                                         coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda.get_coherence()\n",
    "\n",
    "perplexity = lda_model.log_perplexity(bow_corpus)\n",
    "\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, but we have nothing to compare this against. Let’s try to build another LDA\n",
    "topic model based on a separate package called MALLET, which has Gensim wrappers to\n",
    "make it easy to use from Python!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Models with MALLET\n",
    "The MALLET framework is a Java-based package for statistical natural language processing, document classification, clustering, topic modeling, information extraction, and other machine learning applications to text. MALLET stands for MAchine Learning for LanguagE Toolkit. The MALLET topic modeling toolkit contains efficient, sampling-based implementations of Latent Dirichlet Allocation,\n",
    "Pachinko Allocation, and Hierarchical LDA. To use MALLET’s capabilities, we need to download the framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2018-11-08 20:06:13--  http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip\n",
      "Resolving mallet.cs.umass.edu (mallet.cs.umass.edu)... 128.119.246.70\n",
      "Connecting to mallet.cs.umass.edu (mallet.cs.umass.edu)|128.119.246.70|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 16184794 (15M) [application/zip]\n",
      "Saving to: ‘mallet-2.0.8.zip’\n",
      "\n",
      "mallet-2.0.8.zip    100%[===================>]  15.43M  1.35MB/s    in 12s     \n",
      "\n",
      "2018-11-08 20:06:25 (1.28 MB/s) - ‘mallet-2.0.8.zip’ saved [16184794/16184794]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -q mallet-2.0.8.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have multiple CPUs, Gensim can also use them for parallel processing and faster training.\n",
    "MALLET_PATH = 'mallet-2.0.8/bin/mallet'\n",
    "lda_mallet = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=bow_corpus, \n",
    "                                              num_topics=TOTAL_TOPICS, id2word=dictionary,\n",
    "                                              iterations=500, workers=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['neuron', 'cell', 'response', 'stimulus', 'activity', 'pattern', 'signal', 'spike', 'effect', 'synaptic', 'frequency', 'neural', 'unit', 'connection', 'layer', 'cortical', 'firing', 'et_al', 'brain', 'temporal']\n",
      "\n",
      "Topic #2:\n",
      "['prediction', 'control', 'trajectory', 'target', 'task', 'expert', 'training', 'nonlinear', 'dynamic', 'linear', 'local', 'change', 'adaptive', 'mapping', 'hand', 'movement', 'controller', 'position', 'motor', 'architecture']\n",
      "\n",
      "Topic #3:\n",
      "['vector', 'equation', 'linear', 'bound', 'solution', 'theory', 'matrix', 'convergence', 'theorem', 'defined', 'size', 'constant', 'optimal', 'class', 'eq', 'property', 'approximation', 'condition', 'rate', 'probability']\n",
      "\n",
      "Topic #4:\n",
      "['state', 'action', 'step', 'policy', 'environment', 'recurrent', 'transition', 'optimal', 'task', 'reinforcement_learning', 'control', 'path', 'goal', 'stochastic', 'sequence', 'current', 'cost', 'iteration', 'machine', 'update']\n",
      "\n",
      "Topic #5:\n",
      "['image', 'object', 'feature', 'motion', 'visual', 'map', 'direction', 'location', 'pixel', 'field', 'region', 'position', 'filter', 'local', 'view', 'representation', 'face', 'distance', 'surface', 'edge']\n",
      "\n",
      "Topic #6:\n",
      "['distribution', 'estimate', 'gaussian', 'variable', 'probability', 'prior', 'sample', 'approximation', 'variance', 'mixture', 'density', 'noise', 'kernel', 'component', 'estimation', 'bayesian', 'vector', 'matrix', 'log', 'linear']\n",
      "\n",
      "Topic #7:\n",
      "['rule', 'node', 'unit', 'structure', 'representation', 'tree', 'level', 'pattern', 'graph', 'cluster', 'activation', 'sequence', 'connectionist', 'symbol', 'similarity', 'instance', 'represented', 'string', 'memory', 'clustering']\n",
      "\n",
      "Topic #8:\n",
      "['neuron', 'circuit', 'chip', 'current', 'memory', 'signal', 'analog', 'bit', 'voltage', 'neural', 'code', 'implementation', 'noise', 'connection', 'parallel', 'attractor', 'design', 'element', 'operation', 'processor']\n",
      "\n",
      "Topic #9:\n",
      "['training', 'unit', 'pattern', 'hidden_unit', 'layer', 'net', 'classifier', 'class', 'training_set', 'classification', 'trained', 'test', 'task', 'back_propagation', 'hidden_layer', 'table', 'generalization', 'feature', 'size', 'architecture']\n",
      "\n",
      "Topic #10:\n",
      "['word', 'recognition', 'speech', 'sequence', 'feature', 'context', 'training', 'character', 'hmm', 'module', 'signal', 'letter', 'frame', 'trained', 'experiment', 'classification', 'architecture', 'speaker', 'window', 'class']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in lda_mallet.show_topic(n, topn=20)] \n",
    "                   for n in range(0, TOTAL_TOPICS)]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. Coherence Score (Cv): 0.5008326905758488\n",
      "Avg. Coherence Score (UMass): -1.0635635291342118\n",
      "Model Perplexity: -8.53533\n"
     ]
    }
   ],
   "source": [
    "cv_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, corpus=bow_corpus, \n",
    "                                                             texts=norm_corpus_bigrams,\n",
    "                                                             dictionary=dictionary, \n",
    "                                                             coherence='c_v')\n",
    "avg_coherence_cv = cv_coherence_model_lda_mallet.get_coherence()\n",
    "\n",
    "umass_coherence_model_lda_mallet = gensim.models.CoherenceModel(model=lda_mallet, corpus=bow_corpus, \n",
    "                                                                texts=norm_corpus_bigrams,\n",
    "                                                                dictionary=dictionary,  \n",
    "                                                                coherence='u_mass')\n",
    "avg_coherence_umass = umass_coherence_model_lda_mallet.get_coherence()\n",
    "\n",
    "# from STDOUT: <500> LL/token: -8.53533\n",
    "perplexity = -8.53533\n",
    "print('Avg. Coherence Score (Cv):', avg_coherence_cv)\n",
    "print('Avg. Coherence Score (UMass):', avg_coherence_umass)\n",
    "print('Model Perplexity:', perplexity)\n",
    "\n",
    "# model from MALLET is much better based on these metrics as compared to the default LDA model from Gensim\n",
    "# Can we find the optimal number of topics that maximizes the coherence? This is a tough problem, but we can try\n",
    "# doing it iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA Tuning - Finding Optimal Number of Topics\n",
    "Finding the optimal number of topics in a topic model is tough, given that it is like a model hyperparameter that you always have to set before training the model.\n",
    "\n",
    "We can use an iterative approach and build several models with differing numbers of topics and select the one that has the highest coherence score. To implement this method, we build the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def topic_model_coherence_generator(corpus, texts, dictionary, \n",
    "                                    start_topic_count=2, end_topic_count=10, step=1,\n",
    "                                    cpus=1):\n",
    "    \n",
    "    models = []\n",
    "    coherence_scores = []\n",
    "    for topic_nums in tqdm(range(start_topic_count, end_topic_count+1, step)):\n",
    "        mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=corpus,\n",
    "                                                            num_topics=topic_nums, id2word=dictionary,\n",
    "                                                            iterations=500, workers=cpus)\n",
    "        cv_coherence_model_mallet_lda = gensim.models.CoherenceModel(model=mallet_lda_model, corpus=corpus, \n",
    "                                                                     texts=texts, dictionary=dictionary, \n",
    "                                                                     coherence='c_v')\n",
    "        coherence_score = cv_coherence_model_mallet_lda.get_coherence()\n",
    "        coherence_scores.append(coherence_score)\n",
    "        models.append(mallet_lda_model)\n",
    "    \n",
    "    return models, coherence_scores\n",
    "\n",
    "# Note that this step might take some time to train, depending on your infrastructure since we will be training several\n",
    "# topic models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/29 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MALLET_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-87-b207ce316e82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, texts=norm_corpus_bigrams,\n\u001b[0;32m      2\u001b[0m                                                                \u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_topic_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m                                                                end_topic_count=30, step=1, cpus=16)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-86-f727324cfa34>\u001b[0m in \u001b[0;36mtopic_model_coherence_generator\u001b[1;34m(corpus, texts, dictionary, start_topic_count, end_topic_count, step, cpus)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mcoherence_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtopic_nums\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_topic_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_topic_count\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         mallet_lda_model = gensim.models.wrappers.LdaMallet(mallet_path=MALLET_PATH, corpus=corpus,\n\u001b[0m\u001b[0;32m     11\u001b[0m                                                             \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopic_nums\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m                                                             iterations=500, workers=cpus)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'MALLET_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "lda_models, coherence_scores = topic_model_coherence_generator(corpus=bow_corpus, texts=norm_corpus_bigrams,\n",
    "                                                               dictionary=dictionary, start_topic_count=2,\n",
    "                                                               end_topic_count=30, step=1, cpus=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Topics</th>\n",
       "      <th>Coherence Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>26</td>\n",
       "      <td>0.5461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>25</td>\n",
       "      <td>0.5427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>19</td>\n",
       "      <td>0.5419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>18</td>\n",
       "      <td>0.5412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>24</td>\n",
       "      <td>0.5405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>20</td>\n",
       "      <td>0.5401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>23</td>\n",
       "      <td>0.5375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>22</td>\n",
       "      <td>0.5369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21</td>\n",
       "      <td>0.5369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>29</td>\n",
       "      <td>0.5363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Number of Topics  Coherence Score\n",
       "24                26           0.5461\n",
       "23                25           0.5427\n",
       "17                19           0.5419\n",
       "16                18           0.5412\n",
       "22                24           0.5405\n",
       "18                20           0.5401\n",
       "21                23           0.5375\n",
       "20                22           0.5369\n",
       "19                21           0.5369\n",
       "27                29           0.5363"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coherence_df = pd.DataFrame({'Number of Topics': range(2, 31, 1),\n",
    "                             'Coherence Score': np.round(coherence_scores, 4)})\n",
    "coherence_df.sort_values(by=['Coherence Score'], ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let’s plot a graph showing the number of topics per model and their corresponding coherence scores \n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "%matplotlib inline\n",
    "\n",
    "x_ax = range(2, 31, 1)\n",
    "y_ax = coherence_scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(x_ax, y_ax, c='r')\n",
    "plt.axhline(y=0.535, c='k', linestyle='--', linewidth=2)\n",
    "plt.rcParams['figure.facecolor'] = 'white'\n",
    "xl = plt.xlabel('Number of Topics')\n",
    "yl = plt.ylabel('Coherence Score')\n",
    "\n",
    "# From Figure, it looks like the score starts increasing rapidly when the number of topics is five and gradually starts plateauing at 19 or 20. We choose the optimal number\n",
    "# of topics as 20, based on our intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can retrieve the best model now\n",
    "best_model_idx = coherence_df[coherence_df['Number of Topics'] == 20].index[0]\n",
    "best_lda_model = lda_models[best_model_idx]\n",
    "best_lda_model.num_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "['class', 'classification', 'classifier', 'training', 'pattern', 'feature', 'kernel', 'machine', 'training_set', 'test', 'sample', 'vector', 'database', 'error_rate', 'margin', 'experiment', 'support_vector', 'nearest_neighbor', 'decision', 'size']\n",
      "\n",
      "Topic #2:\n",
      "['neuron', 'memory', 'pattern', 'dynamic', 'connection', 'phase', 'attractor', 'capacity', 'state', 'hopfield', 'neural', 'fixed_point', 'oscillator', 'delay', 'stable', 'fig', 'oscillation', 'associative_memory', 'behavior', 'stored']\n",
      "\n",
      "Topic #3:\n",
      "['word', 'recognition', 'training', 'speech', 'character', 'context', 'hmm', 'letter', 'mlp', 'speaker', 'feature', 'frame', 'trained', 'speech_recognition', 'phoneme', 'experiment', 'hybrid', 'segmentation', 'vowel', 'level']\n",
      "\n",
      "Topic #4:\n",
      "['noise', 'rate', 'equation', 'curve', 'average', 'correlation', 'rule', 'distribution', 'theory', 'limit', 'solution', 'optimal', 'eq', 'teacher', 'effect', 'size', 'temperature', 'student', 'line', 'random']\n",
      "\n",
      "Topic #5:\n",
      "['bound', 'theorem', 'class', 'probability', 'size', 'threshold', 'proof', 'polynomial', 'theory', 'complexity', 'loss', 'approximation', 'linear', 'assume', 'definition', 'defined', 'hypothesis', 'constant', 'define', 'bounded']\n",
      "\n",
      "Topic #6:\n",
      "['vector', 'matrix', 'linear', 'equation', 'solution', 'gradient', 'constraint', 'convergence', 'optimization', 'nonlinear', 'optimal', 'eq', 'minimum', 'operator', 'gradient_descent', 'condition', 'constant', 'derivative', 'quadratic', 'energy']\n",
      "\n",
      "Topic #7:\n",
      "['search', 'task', 'experiment', 'table', 'instance', 'test', 'domain', 'target', 'query', 'feature', 'user', 'random', 'technique', 'run', 'accuracy', 'block', 'application', 'evaluation', 'strategy', 'important']\n",
      "\n",
      "Topic #8:\n",
      "['distribution', 'probability', 'prior', 'gaussian', 'variable', 'mixture', 'density', 'bayesian', 'estimate', 'approximation', 'log', 'likelihood', 'sample', 'component', 'expert', 'em', 'posterior', 'probabilistic', 'estimation', 'entropy']\n",
      "\n",
      "Topic #9:\n",
      "['visual', 'motion', 'cell', 'response', 'stimulus', 'direction', 'receptive_field', 'map', 'spatial', 'orientation', 'unit', 'eye', 'field', 'activity', 'location', 'velocity', 'center', 'contrast', 'cortical', 'pattern']\n",
      "\n",
      "Topic #10:\n",
      "['neuron', 'cell', 'spike', 'synaptic', 'activity', 'response', 'stimulus', 'firing', 'synapsis', 'et_al', 'effect', 'neural', 'neuronal', 'current', 'pattern', 'inhibitory', 'connection', 'brain', 'simulation', 'firing_rate']\n",
      "\n",
      "Topic #11:\n",
      "['state', 'sequence', 'step', 'recurrent', 'transition', 'stochastic', 'iteration', 'update', 'dynamic', 'probability', 'convergence', 'trajectory', 'xt', 'length', 'observation', 'continuous', 'rate', 'machine', 'current', 'string']\n",
      "\n",
      "Topic #12:\n",
      "['node', 'tree', 'structure', 'graph', 'code', 'level', 'bit', 'path', 'local', 'size', 'variable', 'stage', 'length', 'solution', 'edge', 'link', 'component', 'match', 'binary', 'coding']\n",
      "\n",
      "Topic #13:\n",
      "['image', 'object', 'feature', 'pixel', 'face', 'view', 'recognition', 'representation', 'shape', 'scale', 'part', 'visual', 'region', 'position', 'scene', 'surface', 'vision', 'frame', 'texture', 'location']\n",
      "\n",
      "Topic #14:\n",
      "['control', 'action', 'state', 'policy', 'environment', 'controller', 'reinforcement_learning', 'task', 'optimal', 'robot', 'goal', 'step', 'reward', 'td', 'agent', 'adaptive', 'cost', 'reinforcement', 'trial', 'exploration']\n",
      "\n",
      "Topic #15:\n",
      "['unit', 'layer', 'training', 'hidden_unit', 'net', 'architecture', 'pattern', 'activation', 'trained', 'task', 'back_propagation', 'hidden_layer', 'connection', 'hidden', 'backpropagation', 'learn', 'training_set', 'epoch', 'simulation', 'generalization']\n",
      "\n",
      "Topic #16:\n",
      "['circuit', 'chip', 'current', 'analog', 'voltage', 'implementation', 'processor', 'bit', 'design', 'device', 'computation', 'parallel', 'digital', 'operation', 'array', 'neural', 'synapse', 'element', 'hardware', 'transistor']\n",
      "\n",
      "Topic #17:\n",
      "['rule', 'representation', 'module', 'structure', 'human', 'movement', 'motor', 'target', 'language', 'subject', 'connectionist', 'position', 'task', 'context', 'trajectory', 'hand', 'role', 'symbol', 'learned', 'theory']\n",
      "\n",
      "Topic #18:\n",
      "['vector', 'map', 'distance', 'cluster', 'local', 'dimension', 'clustering', 'mapping', 'dimensional', 'region', 'structure', 'center', 'rbf', 'pca', 'basis_function', 'linear', 'representation', 'global', 'principal_component', 'projection']\n",
      "\n",
      "Topic #19:\n",
      "['signal', 'filter', 'frequency', 'source', 'channel', 'noise', 'component', 'response', 'temporal', 'sound', 'auditory', 'detection', 'phase', 'ica', 'adaptation', 'amplitude', 'subject', 'eeg', 'change', 'correlation']\n",
      "\n",
      "Topic #20:\n",
      "['prediction', 'training', 'estimate', 'regression', 'test', 'noise', 'selection', 'variance', 'training_set', 'sample', 'ensemble', 'estimation', 'average', 'nonlinear', 'linear', 'estimator', 'cross_validation', 'pruning', 'bias', 'risk']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let’s view all the 20 topics generated by our selected best model, similar to our previous models\n",
    "topics = [[(term, round(wt, 3)) \n",
    "               for term, wt in best_lda_model.show_topic(n, topn=20)] \n",
    "                   for n in range(0, best_lda_model.num_topics)]\n",
    "\n",
    "for idx, topic in enumerate(topics):\n",
    "    print('Topic #'+str(idx+1)+':')\n",
    "    print([term for term, wt in topic])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 1</th>\n",
       "      <th>Topic 2</th>\n",
       "      <th>Topic 3</th>\n",
       "      <th>Topic 4</th>\n",
       "      <th>Topic 5</th>\n",
       "      <th>Topic 6</th>\n",
       "      <th>Topic 7</th>\n",
       "      <th>Topic 8</th>\n",
       "      <th>Topic 9</th>\n",
       "      <th>Topic 10</th>\n",
       "      <th>Topic 11</th>\n",
       "      <th>Topic 12</th>\n",
       "      <th>Topic 13</th>\n",
       "      <th>Topic 14</th>\n",
       "      <th>Topic 15</th>\n",
       "      <th>Topic 16</th>\n",
       "      <th>Topic 17</th>\n",
       "      <th>Topic 18</th>\n",
       "      <th>Topic 19</th>\n",
       "      <th>Topic 20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Term1</th>\n",
       "      <td>class</td>\n",
       "      <td>neuron</td>\n",
       "      <td>word</td>\n",
       "      <td>noise</td>\n",
       "      <td>bound</td>\n",
       "      <td>vector</td>\n",
       "      <td>search</td>\n",
       "      <td>distribution</td>\n",
       "      <td>visual</td>\n",
       "      <td>neuron</td>\n",
       "      <td>state</td>\n",
       "      <td>node</td>\n",
       "      <td>image</td>\n",
       "      <td>control</td>\n",
       "      <td>unit</td>\n",
       "      <td>circuit</td>\n",
       "      <td>rule</td>\n",
       "      <td>vector</td>\n",
       "      <td>signal</td>\n",
       "      <td>prediction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term2</th>\n",
       "      <td>classification</td>\n",
       "      <td>memory</td>\n",
       "      <td>recognition</td>\n",
       "      <td>rate</td>\n",
       "      <td>theorem</td>\n",
       "      <td>matrix</td>\n",
       "      <td>task</td>\n",
       "      <td>probability</td>\n",
       "      <td>motion</td>\n",
       "      <td>cell</td>\n",
       "      <td>sequence</td>\n",
       "      <td>tree</td>\n",
       "      <td>object</td>\n",
       "      <td>action</td>\n",
       "      <td>layer</td>\n",
       "      <td>chip</td>\n",
       "      <td>representation</td>\n",
       "      <td>map</td>\n",
       "      <td>filter</td>\n",
       "      <td>training</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term3</th>\n",
       "      <td>classifier</td>\n",
       "      <td>pattern</td>\n",
       "      <td>training</td>\n",
       "      <td>equation</td>\n",
       "      <td>class</td>\n",
       "      <td>linear</td>\n",
       "      <td>experiment</td>\n",
       "      <td>prior</td>\n",
       "      <td>cell</td>\n",
       "      <td>spike</td>\n",
       "      <td>step</td>\n",
       "      <td>structure</td>\n",
       "      <td>feature</td>\n",
       "      <td>state</td>\n",
       "      <td>training</td>\n",
       "      <td>current</td>\n",
       "      <td>module</td>\n",
       "      <td>distance</td>\n",
       "      <td>frequency</td>\n",
       "      <td>estimate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term4</th>\n",
       "      <td>training</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>speech</td>\n",
       "      <td>curve</td>\n",
       "      <td>probability</td>\n",
       "      <td>equation</td>\n",
       "      <td>table</td>\n",
       "      <td>gaussian</td>\n",
       "      <td>response</td>\n",
       "      <td>synaptic</td>\n",
       "      <td>recurrent</td>\n",
       "      <td>graph</td>\n",
       "      <td>pixel</td>\n",
       "      <td>policy</td>\n",
       "      <td>hidden_unit</td>\n",
       "      <td>analog</td>\n",
       "      <td>structure</td>\n",
       "      <td>cluster</td>\n",
       "      <td>source</td>\n",
       "      <td>regression</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term5</th>\n",
       "      <td>pattern</td>\n",
       "      <td>connection</td>\n",
       "      <td>character</td>\n",
       "      <td>average</td>\n",
       "      <td>size</td>\n",
       "      <td>solution</td>\n",
       "      <td>instance</td>\n",
       "      <td>variable</td>\n",
       "      <td>stimulus</td>\n",
       "      <td>activity</td>\n",
       "      <td>transition</td>\n",
       "      <td>code</td>\n",
       "      <td>face</td>\n",
       "      <td>environment</td>\n",
       "      <td>net</td>\n",
       "      <td>voltage</td>\n",
       "      <td>human</td>\n",
       "      <td>local</td>\n",
       "      <td>channel</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term6</th>\n",
       "      <td>feature</td>\n",
       "      <td>phase</td>\n",
       "      <td>context</td>\n",
       "      <td>correlation</td>\n",
       "      <td>threshold</td>\n",
       "      <td>gradient</td>\n",
       "      <td>test</td>\n",
       "      <td>mixture</td>\n",
       "      <td>direction</td>\n",
       "      <td>response</td>\n",
       "      <td>stochastic</td>\n",
       "      <td>level</td>\n",
       "      <td>view</td>\n",
       "      <td>controller</td>\n",
       "      <td>architecture</td>\n",
       "      <td>implementation</td>\n",
       "      <td>movement</td>\n",
       "      <td>dimension</td>\n",
       "      <td>noise</td>\n",
       "      <td>noise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term7</th>\n",
       "      <td>kernel</td>\n",
       "      <td>attractor</td>\n",
       "      <td>hmm</td>\n",
       "      <td>rule</td>\n",
       "      <td>proof</td>\n",
       "      <td>constraint</td>\n",
       "      <td>domain</td>\n",
       "      <td>density</td>\n",
       "      <td>receptive_field</td>\n",
       "      <td>stimulus</td>\n",
       "      <td>iteration</td>\n",
       "      <td>bit</td>\n",
       "      <td>recognition</td>\n",
       "      <td>reinforcement_learning</td>\n",
       "      <td>pattern</td>\n",
       "      <td>processor</td>\n",
       "      <td>motor</td>\n",
       "      <td>clustering</td>\n",
       "      <td>component</td>\n",
       "      <td>selection</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term8</th>\n",
       "      <td>machine</td>\n",
       "      <td>capacity</td>\n",
       "      <td>letter</td>\n",
       "      <td>distribution</td>\n",
       "      <td>polynomial</td>\n",
       "      <td>convergence</td>\n",
       "      <td>target</td>\n",
       "      <td>bayesian</td>\n",
       "      <td>map</td>\n",
       "      <td>firing</td>\n",
       "      <td>update</td>\n",
       "      <td>path</td>\n",
       "      <td>representation</td>\n",
       "      <td>task</td>\n",
       "      <td>activation</td>\n",
       "      <td>bit</td>\n",
       "      <td>target</td>\n",
       "      <td>mapping</td>\n",
       "      <td>response</td>\n",
       "      <td>variance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term9</th>\n",
       "      <td>training_set</td>\n",
       "      <td>state</td>\n",
       "      <td>mlp</td>\n",
       "      <td>theory</td>\n",
       "      <td>theory</td>\n",
       "      <td>optimization</td>\n",
       "      <td>query</td>\n",
       "      <td>estimate</td>\n",
       "      <td>spatial</td>\n",
       "      <td>synapsis</td>\n",
       "      <td>dynamic</td>\n",
       "      <td>local</td>\n",
       "      <td>shape</td>\n",
       "      <td>optimal</td>\n",
       "      <td>trained</td>\n",
       "      <td>design</td>\n",
       "      <td>language</td>\n",
       "      <td>dimensional</td>\n",
       "      <td>temporal</td>\n",
       "      <td>training_set</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term10</th>\n",
       "      <td>test</td>\n",
       "      <td>hopfield</td>\n",
       "      <td>speaker</td>\n",
       "      <td>limit</td>\n",
       "      <td>complexity</td>\n",
       "      <td>nonlinear</td>\n",
       "      <td>feature</td>\n",
       "      <td>approximation</td>\n",
       "      <td>orientation</td>\n",
       "      <td>et_al</td>\n",
       "      <td>probability</td>\n",
       "      <td>size</td>\n",
       "      <td>scale</td>\n",
       "      <td>robot</td>\n",
       "      <td>task</td>\n",
       "      <td>device</td>\n",
       "      <td>subject</td>\n",
       "      <td>region</td>\n",
       "      <td>sound</td>\n",
       "      <td>sample</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term11</th>\n",
       "      <td>sample</td>\n",
       "      <td>neural</td>\n",
       "      <td>feature</td>\n",
       "      <td>solution</td>\n",
       "      <td>loss</td>\n",
       "      <td>optimal</td>\n",
       "      <td>user</td>\n",
       "      <td>log</td>\n",
       "      <td>unit</td>\n",
       "      <td>effect</td>\n",
       "      <td>convergence</td>\n",
       "      <td>variable</td>\n",
       "      <td>part</td>\n",
       "      <td>goal</td>\n",
       "      <td>back_propagation</td>\n",
       "      <td>computation</td>\n",
       "      <td>connectionist</td>\n",
       "      <td>structure</td>\n",
       "      <td>auditory</td>\n",
       "      <td>ensemble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term12</th>\n",
       "      <td>vector</td>\n",
       "      <td>fixed_point</td>\n",
       "      <td>frame</td>\n",
       "      <td>optimal</td>\n",
       "      <td>approximation</td>\n",
       "      <td>eq</td>\n",
       "      <td>random</td>\n",
       "      <td>likelihood</td>\n",
       "      <td>eye</td>\n",
       "      <td>neural</td>\n",
       "      <td>trajectory</td>\n",
       "      <td>stage</td>\n",
       "      <td>visual</td>\n",
       "      <td>step</td>\n",
       "      <td>hidden_layer</td>\n",
       "      <td>parallel</td>\n",
       "      <td>position</td>\n",
       "      <td>center</td>\n",
       "      <td>detection</td>\n",
       "      <td>estimation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term13</th>\n",
       "      <td>database</td>\n",
       "      <td>oscillator</td>\n",
       "      <td>trained</td>\n",
       "      <td>eq</td>\n",
       "      <td>linear</td>\n",
       "      <td>minimum</td>\n",
       "      <td>technique</td>\n",
       "      <td>sample</td>\n",
       "      <td>field</td>\n",
       "      <td>neuronal</td>\n",
       "      <td>xt</td>\n",
       "      <td>length</td>\n",
       "      <td>region</td>\n",
       "      <td>reward</td>\n",
       "      <td>connection</td>\n",
       "      <td>digital</td>\n",
       "      <td>task</td>\n",
       "      <td>rbf</td>\n",
       "      <td>phase</td>\n",
       "      <td>average</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term14</th>\n",
       "      <td>error_rate</td>\n",
       "      <td>delay</td>\n",
       "      <td>speech_recognition</td>\n",
       "      <td>teacher</td>\n",
       "      <td>assume</td>\n",
       "      <td>operator</td>\n",
       "      <td>run</td>\n",
       "      <td>component</td>\n",
       "      <td>activity</td>\n",
       "      <td>current</td>\n",
       "      <td>length</td>\n",
       "      <td>solution</td>\n",
       "      <td>position</td>\n",
       "      <td>td</td>\n",
       "      <td>hidden</td>\n",
       "      <td>operation</td>\n",
       "      <td>context</td>\n",
       "      <td>pca</td>\n",
       "      <td>ica</td>\n",
       "      <td>nonlinear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term15</th>\n",
       "      <td>margin</td>\n",
       "      <td>stable</td>\n",
       "      <td>phoneme</td>\n",
       "      <td>effect</td>\n",
       "      <td>definition</td>\n",
       "      <td>gradient_descent</td>\n",
       "      <td>accuracy</td>\n",
       "      <td>expert</td>\n",
       "      <td>location</td>\n",
       "      <td>pattern</td>\n",
       "      <td>observation</td>\n",
       "      <td>edge</td>\n",
       "      <td>scene</td>\n",
       "      <td>agent</td>\n",
       "      <td>backpropagation</td>\n",
       "      <td>array</td>\n",
       "      <td>trajectory</td>\n",
       "      <td>basis_function</td>\n",
       "      <td>adaptation</td>\n",
       "      <td>linear</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term16</th>\n",
       "      <td>experiment</td>\n",
       "      <td>fig</td>\n",
       "      <td>experiment</td>\n",
       "      <td>size</td>\n",
       "      <td>defined</td>\n",
       "      <td>condition</td>\n",
       "      <td>block</td>\n",
       "      <td>em</td>\n",
       "      <td>velocity</td>\n",
       "      <td>inhibitory</td>\n",
       "      <td>continuous</td>\n",
       "      <td>link</td>\n",
       "      <td>surface</td>\n",
       "      <td>adaptive</td>\n",
       "      <td>learn</td>\n",
       "      <td>neural</td>\n",
       "      <td>hand</td>\n",
       "      <td>linear</td>\n",
       "      <td>amplitude</td>\n",
       "      <td>estimator</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term17</th>\n",
       "      <td>support_vector</td>\n",
       "      <td>oscillation</td>\n",
       "      <td>hybrid</td>\n",
       "      <td>temperature</td>\n",
       "      <td>hypothesis</td>\n",
       "      <td>constant</td>\n",
       "      <td>application</td>\n",
       "      <td>posterior</td>\n",
       "      <td>center</td>\n",
       "      <td>connection</td>\n",
       "      <td>rate</td>\n",
       "      <td>component</td>\n",
       "      <td>vision</td>\n",
       "      <td>cost</td>\n",
       "      <td>training_set</td>\n",
       "      <td>synapse</td>\n",
       "      <td>role</td>\n",
       "      <td>representation</td>\n",
       "      <td>subject</td>\n",
       "      <td>cross_validation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term18</th>\n",
       "      <td>nearest_neighbor</td>\n",
       "      <td>associative_memory</td>\n",
       "      <td>segmentation</td>\n",
       "      <td>student</td>\n",
       "      <td>constant</td>\n",
       "      <td>derivative</td>\n",
       "      <td>evaluation</td>\n",
       "      <td>probabilistic</td>\n",
       "      <td>contrast</td>\n",
       "      <td>brain</td>\n",
       "      <td>machine</td>\n",
       "      <td>match</td>\n",
       "      <td>frame</td>\n",
       "      <td>reinforcement</td>\n",
       "      <td>epoch</td>\n",
       "      <td>element</td>\n",
       "      <td>symbol</td>\n",
       "      <td>global</td>\n",
       "      <td>eeg</td>\n",
       "      <td>pruning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term19</th>\n",
       "      <td>decision</td>\n",
       "      <td>behavior</td>\n",
       "      <td>vowel</td>\n",
       "      <td>line</td>\n",
       "      <td>define</td>\n",
       "      <td>quadratic</td>\n",
       "      <td>strategy</td>\n",
       "      <td>estimation</td>\n",
       "      <td>cortical</td>\n",
       "      <td>simulation</td>\n",
       "      <td>current</td>\n",
       "      <td>binary</td>\n",
       "      <td>texture</td>\n",
       "      <td>trial</td>\n",
       "      <td>simulation</td>\n",
       "      <td>hardware</td>\n",
       "      <td>learned</td>\n",
       "      <td>principal_component</td>\n",
       "      <td>change</td>\n",
       "      <td>bias</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Term20</th>\n",
       "      <td>size</td>\n",
       "      <td>stored</td>\n",
       "      <td>level</td>\n",
       "      <td>random</td>\n",
       "      <td>bounded</td>\n",
       "      <td>energy</td>\n",
       "      <td>important</td>\n",
       "      <td>entropy</td>\n",
       "      <td>pattern</td>\n",
       "      <td>firing_rate</td>\n",
       "      <td>string</td>\n",
       "      <td>coding</td>\n",
       "      <td>location</td>\n",
       "      <td>exploration</td>\n",
       "      <td>generalization</td>\n",
       "      <td>transistor</td>\n",
       "      <td>theory</td>\n",
       "      <td>projection</td>\n",
       "      <td>correlation</td>\n",
       "      <td>risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Topic 1             Topic 2             Topic 3  \\\n",
       "Term1              class              neuron                word   \n",
       "Term2     classification              memory         recognition   \n",
       "Term3         classifier             pattern            training   \n",
       "Term4           training             dynamic              speech   \n",
       "Term5            pattern          connection           character   \n",
       "Term6            feature               phase             context   \n",
       "Term7             kernel           attractor                 hmm   \n",
       "Term8            machine            capacity              letter   \n",
       "Term9       training_set               state                 mlp   \n",
       "Term10              test            hopfield             speaker   \n",
       "Term11            sample              neural             feature   \n",
       "Term12            vector         fixed_point               frame   \n",
       "Term13          database          oscillator             trained   \n",
       "Term14        error_rate               delay  speech_recognition   \n",
       "Term15            margin              stable             phoneme   \n",
       "Term16        experiment                 fig          experiment   \n",
       "Term17    support_vector         oscillation              hybrid   \n",
       "Term18  nearest_neighbor  associative_memory        segmentation   \n",
       "Term19          decision            behavior               vowel   \n",
       "Term20              size              stored               level   \n",
       "\n",
       "             Topic 4        Topic 5           Topic 6      Topic 7  \\\n",
       "Term1          noise          bound            vector       search   \n",
       "Term2           rate        theorem            matrix         task   \n",
       "Term3       equation          class            linear   experiment   \n",
       "Term4          curve    probability          equation        table   \n",
       "Term5        average           size          solution     instance   \n",
       "Term6    correlation      threshold          gradient         test   \n",
       "Term7           rule          proof        constraint       domain   \n",
       "Term8   distribution     polynomial       convergence       target   \n",
       "Term9         theory         theory      optimization        query   \n",
       "Term10         limit     complexity         nonlinear      feature   \n",
       "Term11      solution           loss           optimal         user   \n",
       "Term12       optimal  approximation                eq       random   \n",
       "Term13            eq         linear           minimum    technique   \n",
       "Term14       teacher         assume          operator          run   \n",
       "Term15        effect     definition  gradient_descent     accuracy   \n",
       "Term16          size        defined         condition        block   \n",
       "Term17   temperature     hypothesis          constant  application   \n",
       "Term18       student       constant        derivative   evaluation   \n",
       "Term19          line         define         quadratic     strategy   \n",
       "Term20        random        bounded            energy    important   \n",
       "\n",
       "              Topic 8          Topic 9     Topic 10     Topic 11   Topic 12  \\\n",
       "Term1    distribution           visual       neuron        state       node   \n",
       "Term2     probability           motion         cell     sequence       tree   \n",
       "Term3           prior             cell        spike         step  structure   \n",
       "Term4        gaussian         response     synaptic    recurrent      graph   \n",
       "Term5        variable         stimulus     activity   transition       code   \n",
       "Term6         mixture        direction     response   stochastic      level   \n",
       "Term7         density  receptive_field     stimulus    iteration        bit   \n",
       "Term8        bayesian              map       firing       update       path   \n",
       "Term9        estimate          spatial     synapsis      dynamic      local   \n",
       "Term10  approximation      orientation        et_al  probability       size   \n",
       "Term11            log             unit       effect  convergence   variable   \n",
       "Term12     likelihood              eye       neural   trajectory      stage   \n",
       "Term13         sample            field     neuronal           xt     length   \n",
       "Term14      component         activity      current       length   solution   \n",
       "Term15         expert         location      pattern  observation       edge   \n",
       "Term16             em         velocity   inhibitory   continuous       link   \n",
       "Term17      posterior           center   connection         rate  component   \n",
       "Term18  probabilistic         contrast        brain      machine      match   \n",
       "Term19     estimation         cortical   simulation      current     binary   \n",
       "Term20        entropy          pattern  firing_rate       string     coding   \n",
       "\n",
       "              Topic 13                Topic 14          Topic 15  \\\n",
       "Term1            image                 control              unit   \n",
       "Term2           object                  action             layer   \n",
       "Term3          feature                   state          training   \n",
       "Term4            pixel                  policy       hidden_unit   \n",
       "Term5             face             environment               net   \n",
       "Term6             view              controller      architecture   \n",
       "Term7      recognition  reinforcement_learning           pattern   \n",
       "Term8   representation                    task        activation   \n",
       "Term9            shape                 optimal           trained   \n",
       "Term10           scale                   robot              task   \n",
       "Term11            part                    goal  back_propagation   \n",
       "Term12          visual                    step      hidden_layer   \n",
       "Term13          region                  reward        connection   \n",
       "Term14        position                      td            hidden   \n",
       "Term15           scene                   agent   backpropagation   \n",
       "Term16         surface                adaptive             learn   \n",
       "Term17          vision                    cost      training_set   \n",
       "Term18           frame           reinforcement             epoch   \n",
       "Term19         texture                   trial        simulation   \n",
       "Term20        location             exploration    generalization   \n",
       "\n",
       "              Topic 16        Topic 17             Topic 18     Topic 19  \\\n",
       "Term1          circuit            rule               vector       signal   \n",
       "Term2             chip  representation                  map       filter   \n",
       "Term3          current          module             distance    frequency   \n",
       "Term4           analog       structure              cluster       source   \n",
       "Term5          voltage           human                local      channel   \n",
       "Term6   implementation        movement            dimension        noise   \n",
       "Term7        processor           motor           clustering    component   \n",
       "Term8              bit          target              mapping     response   \n",
       "Term9           design        language          dimensional     temporal   \n",
       "Term10          device         subject               region        sound   \n",
       "Term11     computation   connectionist            structure     auditory   \n",
       "Term12        parallel        position               center    detection   \n",
       "Term13         digital            task                  rbf        phase   \n",
       "Term14       operation         context                  pca          ica   \n",
       "Term15           array      trajectory       basis_function   adaptation   \n",
       "Term16          neural            hand               linear    amplitude   \n",
       "Term17         synapse            role       representation      subject   \n",
       "Term18         element          symbol               global          eeg   \n",
       "Term19        hardware         learned  principal_component       change   \n",
       "Term20      transistor          theory           projection  correlation   \n",
       "\n",
       "                Topic 20  \n",
       "Term1         prediction  \n",
       "Term2           training  \n",
       "Term3           estimate  \n",
       "Term4         regression  \n",
       "Term5               test  \n",
       "Term6              noise  \n",
       "Term7          selection  \n",
       "Term8           variance  \n",
       "Term9       training_set  \n",
       "Term10            sample  \n",
       "Term11          ensemble  \n",
       "Term12        estimation  \n",
       "Term13           average  \n",
       "Term14         nonlinear  \n",
       "Term15            linear  \n",
       "Term16         estimator  \n",
       "Term17  cross_validation  \n",
       "Term18           pruning  \n",
       "Term19              bias  \n",
       "Term20              risk  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A better way of visualizing the topics is to build a term-topic dataframe\n",
    "topics_df = pd.DataFrame([[term for term, wt in topic] \n",
    "                              for topic in topics], \n",
    "                         columns = ['Term'+str(i) for i in range(1, 21)],\n",
    "                         index=['Topic '+str(t) for t in range(1, best_lda_model.num_topics+1)]).T\n",
    "topics_df\n",
    "# Generated topics from our LDA topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'topics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-88-800bf5a1f9b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_colwidth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n\u001b[1;32m----> 5\u001b[1;33m                               for topic in topics],\n\u001b[0m\u001b[0;32m      6\u001b[0m                          \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'Terms per Topic'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                          \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Topic'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_lda_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'topics' is not defined"
     ]
    }
   ],
   "source": [
    "# Another easy way to view the topics is to create a topic-term dataframe, whereby each topic is represented in a row\n",
    "# with the terms of the topic being represented as a comma-separated string.\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame([', '.join([term for term, wt in topic])  \n",
    "                              for topic in topics],\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, best_lda_model.num_topics+1)]\n",
    "                         )\n",
    "topics_df\n",
    "# Viewing all the topics of our LDA topic model\n",
    "# The dataframe depicted in Figure gives us an easy way to visualize and understand the major themes in our corpus\n",
    "# of research papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Topic Model Results\n",
    "Let’s look at some interesting ways of diving deeper and interpreting results from our topic model. An interesting point to remember is, given a corpus of documents (in the form of features, e.g., Bag of Words) and a trained topic model, you can predict the distribution of topics in each document (research paper in this case) with the following code.\n",
    "\n",
    "tm_results = best_lda_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_lda_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-89-2565cfa22815>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtm_results\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbest_lda_model\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbow_corpus\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_lda_model' is not defined"
     ]
    }
   ],
   "source": [
    "tm_results = best_lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(16, 0.2115988756613756),\n",
       " (5, 0.29989652050187554),\n",
       " (9, 0.3307915758896151),\n",
       " (8, 0.5447463768115942),\n",
       " (9, 0.18093823158652983)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now get the most dominant topic per research paper with some intelligent sorting and indexing using the following code\n",
    "corpus_topics = [sorted(topics, key=lambda record: -record[1])[0] \n",
    "                     for topics in tm_results]\n",
    "corpus_topics[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_topic_df = pd.DataFrame()\n",
    "corpus_topic_df['Document'] = range(0, len(papers))\n",
    "corpus_topic_df['Dominant Topic'] = [item[0]+1 for item in corpus_topics]\n",
    "corpus_topic_df['Contribution %'] = [round(item[1]*100, 2) for item in corpus_topics]\n",
    "corpus_topic_df['Topic Desc'] = [topics_df.iloc[t[0]]['Terms per Topic'] for t in corpus_topics]\n",
    "corpus_topic_df['Paper'] = papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominant Topics Distribution across Corpus\n",
    "Let’s now take a look at various ways we can transform these results and extract meaningful insights from our research papers and their topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/lib/python3.6/site-packages/pandas/core/groupby/groupby.py:4656: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version\n",
      "  return super(DataFrameGroupBy, self).aggregate(arg, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Doc Count</th>\n",
       "      <th>% Total Docs</th>\n",
       "      <th>Topic Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>4.31</td>\n",
       "      <td>class, classification, classifier, training, pattern, feature, kernel, machine, training_set, test, sample, vector, database, error_rate, margin, experiment, support_vector, nearest_neighbor, deci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>3.97</td>\n",
       "      <td>neuron, memory, pattern, dynamic, connection, phase, attractor, capacity, state, hopfield, neural, fixed_point, oscillator, delay, stable, fig, oscillation, associative_memory, behavior, stored</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>78</td>\n",
       "      <td>4.48</td>\n",
       "      <td>word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>3.91</td>\n",
       "      <td>noise, rate, equation, curve, average, correlation, rule, distribution, theory, limit, solution, optimal, eq, teacher, effect, size, temperature, student, line, random</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>105</td>\n",
       "      <td>6.03</td>\n",
       "      <td>bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>4.60</td>\n",
       "      <td>vector, matrix, linear, equation, solution, gradient, constraint, convergence, optimization, nonlinear, optimal, eq, minimum, operator, gradient_descent, condition, constant, derivative, quadratic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>66</td>\n",
       "      <td>3.79</td>\n",
       "      <td>search, task, experiment, table, instance, test, domain, target, query, feature, user, random, technique, run, accuracy, block, application, evaluation, strategy, important</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>148</td>\n",
       "      <td>8.51</td>\n",
       "      <td>distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, ent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>117</td>\n",
       "      <td>6.72</td>\n",
       "      <td>visual, motion, cell, response, stimulus, direction, receptive_field, map, spatial, orientation, unit, eye, field, activity, location, velocity, center, contrast, cortical, pattern</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>141</td>\n",
       "      <td>8.10</td>\n",
       "      <td>neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>2.36</td>\n",
       "      <td>state, sequence, step, recurrent, transition, stochastic, iteration, update, dynamic, probability, convergence, trajectory, xt, length, observation, continuous, rate, machine, current, string</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>2.36</td>\n",
       "      <td>node, tree, structure, graph, code, level, bit, path, local, size, variable, stage, length, solution, edge, link, component, match, binary, coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>113</td>\n",
       "      <td>6.49</td>\n",
       "      <td>image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>110</td>\n",
       "      <td>6.32</td>\n",
       "      <td>control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>84</td>\n",
       "      <td>4.83</td>\n",
       "      <td>unit, layer, training, hidden_unit, net, architecture, pattern, activation, trained, task, back_propagation, hidden_layer, connection, hidden, backpropagation, learn, training_set, epoch, simulati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>104</td>\n",
       "      <td>5.98</td>\n",
       "      <td>circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>76</td>\n",
       "      <td>4.37</td>\n",
       "      <td>rule, representation, module, structure, human, movement, motor, target, language, subject, connectionist, position, task, context, trajectory, hand, role, symbol, learned, theory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>3.62</td>\n",
       "      <td>vector, map, distance, cluster, local, dimension, clustering, mapping, dimensional, region, structure, center, rbf, pca, basis_function, linear, representation, global, principal_component, projec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>69</td>\n",
       "      <td>3.97</td>\n",
       "      <td>signal, filter, frequency, source, channel, noise, component, response, temporal, sound, auditory, detection, phase, ica, adaptation, amplitude, subject, eeg, change, correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>92</td>\n",
       "      <td>5.29</td>\n",
       "      <td>prediction, training, estimate, regression, test, noise, selection, variance, training_set, sample, ensemble, estimation, average, nonlinear, linear, estimator, cross_validation, pruning, bias, risk</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dominant Topic  Doc Count  % Total Docs  \\\n",
       "0                1         75          4.31   \n",
       "1                2         69          3.97   \n",
       "2                3         78          4.48   \n",
       "3                4         68          3.91   \n",
       "4                5        105          6.03   \n",
       "5                6         80          4.60   \n",
       "6                7         66          3.79   \n",
       "7                8        148          8.51   \n",
       "8                9        117          6.72   \n",
       "9               10        141          8.10   \n",
       "10              11         41          2.36   \n",
       "11              12         41          2.36   \n",
       "12              13        113          6.49   \n",
       "13              14        110          6.32   \n",
       "14              15         84          4.83   \n",
       "15              16        104          5.98   \n",
       "16              17         76          4.37   \n",
       "17              18         63          3.62   \n",
       "18              19         69          3.97   \n",
       "19              20         92          5.29   \n",
       "\n",
       "                                                                                                                                                                                                 Topic Desc  \n",
       "0   class, classification, classifier, training, pattern, feature, kernel, machine, training_set, test, sample, vector, database, error_rate, margin, experiment, support_vector, nearest_neighbor, deci...  \n",
       "1         neuron, memory, pattern, dynamic, connection, phase, attractor, capacity, state, hopfield, neural, fixed_point, oscillator, delay, stable, fig, oscillation, associative_memory, behavior, stored  \n",
       "2                  word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level  \n",
       "3                                   noise, rate, equation, curve, average, correlation, rule, distribution, theory, limit, solution, optimal, eq, teacher, effect, size, temperature, student, line, random  \n",
       "4               bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded  \n",
       "5   vector, matrix, linear, equation, solution, gradient, constraint, convergence, optimization, nonlinear, optimal, eq, minimum, operator, gradient_descent, condition, constant, derivative, quadratic...  \n",
       "6                              search, task, experiment, table, instance, test, domain, target, query, feature, user, random, technique, run, accuracy, block, application, evaluation, strategy, important  \n",
       "7   distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, ent...  \n",
       "8                      visual, motion, cell, response, stimulus, direction, receptive_field, map, spatial, orientation, unit, eye, field, activity, location, velocity, center, contrast, cortical, pattern  \n",
       "9                  neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate  \n",
       "10          state, sequence, step, recurrent, transition, stochastic, iteration, update, dynamic, probability, convergence, trajectory, xt, length, observation, continuous, rate, machine, current, string  \n",
       "11                                                       node, tree, structure, graph, code, level, bit, path, local, size, variable, stage, length, solution, edge, link, component, match, binary, coding  \n",
       "12                                   image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location  \n",
       "13                  control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration  \n",
       "14  unit, layer, training, hidden_unit, net, architecture, pattern, activation, trained, task, back_propagation, hidden_layer, connection, hidden, backpropagation, learn, training_set, epoch, simulati...  \n",
       "15                circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor  \n",
       "16                      rule, representation, module, structure, human, movement, motor, target, language, subject, connectionist, position, task, context, trajectory, hand, role, symbol, learned, theory  \n",
       "17  vector, map, distance, cluster, local, dimension, clustering, mapping, dimensional, region, structure, center, rbf, pca, basis_function, linear, representation, global, principal_component, projec...  \n",
       "18                       signal, filter, frequency, source, channel, noise, component, response, temporal, sound, auditory, detection, phase, ica, adaptation, amplitude, subject, eeg, change, correlation  \n",
       "19   prediction, training, estimate, regression, test, noise, selection, variance, training_set, sample, ensemble, estimation, average, nonlinear, linear, estimator, cross_validation, pruning, bias, risk  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The first thing we can do is look at the overall distribution of each topic across the corpus of research papers.\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "topic_stats_df = corpus_topic_df.groupby('Dominant Topic').agg({\n",
    "                                                'Dominant Topic': {\n",
    "                                                    'Doc Count': np.size,\n",
    "                                                    '% Total Docs': np.size }\n",
    "                                              })\n",
    "topic_stats_df = topic_stats_df['Dominant Topic'].reset_index()\n",
    "topic_stats_df['% Total Docs'] = topic_stats_df['% Total Docs'].apply(lambda row: round((row*100) / len(papers), 2))\n",
    "topic_stats_df['Topic Desc'] = [topics_df.iloc[t]['Terms per Topic'] for t in range(len(topic_stats_df))]\n",
    "topic_stats_df\n",
    "# Viewing the distribution of dominant topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dominant Topics in Specific Research Papers\n",
    "Another interesting perspective is to select specific papers, view the most dominant topic\n",
    "in each of those papers, and see if that makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>29.10</td>\n",
       "      <td>image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location</td>\n",
       "      <td>622 \\nLEARNING A COLOR ALGORITHM FROM EXAMPLES \\nAnya C. Hurlbert and Tomaso A. Poggio \\nArtificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, \\nMassachusetts Institut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>27.12</td>\n",
       "      <td>circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor</td>\n",
       "      <td>9 \\nStochastic Learning Networks and their Electronic Implementation \\nJoshua Alspector*, Robert B. Allen, Victor Hut, and Srinagesh Satyanarayana \\nBell Communications Research, Morristown, NJ 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>23.00</td>\n",
       "      <td>bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded</td>\n",
       "      <td>338 \\nThe Connectivity Analysis of Simple Association \\nHow Many Connections Do You Need? \\nDan Hammerstrom * \\nOregon Graduate Center, Beaverton, OR 97006 \\nABSTRACT \\nThe efficient realization, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>392</td>\n",
       "      <td>14</td>\n",
       "      <td>67.03</td>\n",
       "      <td>control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration</td>\n",
       "      <td>Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>503</th>\n",
       "      <td>503</td>\n",
       "      <td>3</td>\n",
       "      <td>60.76</td>\n",
       "      <td>word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level</td>\n",
       "      <td>Multi-State Time Delay Neural Networks \\nfor Continuous Speech Recognition \\nPatrick Haffner \\nCNET Lannion A TSS/RCP \\n22301 LANNION, FRANCE \\nhaffnerlannion.cnet. fr \\nAlex Waibel \\nCarnegie Me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>681</th>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>67.58</td>\n",
       "      <td>word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level</td>\n",
       "      <td>Connected Letter Recognition with a \\nMulti-State Time Delay Neural Network \\nHermann Hild and Alex Waibel \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213-3891, US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>733</td>\n",
       "      <td>16</td>\n",
       "      <td>45.55</td>\n",
       "      <td>circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor</td>\n",
       "      <td>High Performance Neural Net Simulation \\non a Multiprocessor System with \\n\"Intelligent\" Communication \\nUrs A. Miiller, Michael Kocheisen, and Anton Gunzinger \\nElectronics Laboratory, Swiss Fede...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>906</th>\n",
       "      <td>906</td>\n",
       "      <td>10</td>\n",
       "      <td>56.44</td>\n",
       "      <td>neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate</td>\n",
       "      <td>A model of the hippocampus combining self- \\norganization and associative memory function. \\nMichael E. Hasselmo, Eric Schnell \\nJoshua Berke and Edi Barkai \\nDept. of Psychology, Harvard Universi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>19</td>\n",
       "      <td>65.69</td>\n",
       "      <td>signal, filter, frequency, source, channel, noise, component, response, temporal, sound, auditory, detection, phase, ica, adaptation, amplitude, subject, eeg, change, correlation</td>\n",
       "      <td>Using Feedforward Neural Networks to \\nMonitor Alertness from Changes in EEG \\nCorrelation and Coherence \\nScott Makeig \\nNaval Health Research Center, P.O. Box 85122 \\nSan Diego, CA 92186-5122 \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1622</th>\n",
       "      <td>1622</td>\n",
       "      <td>8</td>\n",
       "      <td>56.70</td>\n",
       "      <td>distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, ent...</td>\n",
       "      <td>The Infinite Gaussian Mixture Model \\nCarl Edward Rasmussen \\nDepartment of Mathematical Modelling \\nTechnical University of Denmark \\nBuilding 321, DK-2800 Kongens Lyngby, Denmark \\ncarl@imm.dtu....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Document  Dominant Topic  Contribution %  \\\n",
       "9            9              13           29.10   \n",
       "13          13              16           27.12   \n",
       "17          17               5           23.00   \n",
       "392        392              14           67.03   \n",
       "503        503               3           60.76   \n",
       "681        681               3           67.58   \n",
       "733        733              16           45.55   \n",
       "906        906              10           56.44   \n",
       "996        996              19           65.69   \n",
       "1622      1622               8           56.70   \n",
       "\n",
       "                                                                                                                                                                                                   Topic Desc  \\\n",
       "9                                      image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location   \n",
       "13                  circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor   \n",
       "17                bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded   \n",
       "392                   control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration   \n",
       "503                  word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level   \n",
       "681                  word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level   \n",
       "733                 circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor   \n",
       "906                  neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate   \n",
       "996                        signal, filter, frequency, source, channel, noise, component, response, temporal, sound, auditory, detection, phase, ica, adaptation, amplitude, subject, eeg, change, correlation   \n",
       "1622  distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, ent...   \n",
       "\n",
       "                                                                                                                                                                                                        Paper  \n",
       "9     622 \\nLEARNING A COLOR ALGORITHM FROM EXAMPLES \\nAnya C. Hurlbert and Tomaso A. Poggio \\nArtificial Intelligence Laboratory and Department of Brain and Cognitive Sciences, \\nMassachusetts Institut...  \n",
       "13    9 \\nStochastic Learning Networks and their Electronic Implementation \\nJoshua Alspector*, Robert B. Allen, Victor Hut, and Srinagesh Satyanarayana \\nBell Communications Research, Morristown, NJ 0...  \n",
       "17    338 \\nThe Connectivity Analysis of Simple Association \\nHow Many Connections Do You Need? \\nDan Hammerstrom * \\nOregon Graduate Center, Beaverton, OR 97006 \\nABSTRACT \\nThe efficient realization, ...  \n",
       "392   Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...  \n",
       "503   Multi-State Time Delay Neural Networks \\nfor Continuous Speech Recognition \\nPatrick Haffner \\nCNET Lannion A TSS/RCP \\n22301 LANNION, FRANCE \\nhaffnerlannion.cnet. fr \\nAlex Waibel \\nCarnegie Me...  \n",
       "681   Connected Letter Recognition with a \\nMulti-State Time Delay Neural Network \\nHermann Hild and Alex Waibel \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213-3891, US...  \n",
       "733   High Performance Neural Net Simulation \\non a Multiprocessor System with \\n\"Intelligent\" Communication \\nUrs A. Miiller, Michael Kocheisen, and Anton Gunzinger \\nElectronics Laboratory, Swiss Fede...  \n",
       "906   A model of the hippocampus combining self- \\norganization and associative memory function. \\nMichael E. Hasselmo, Eric Schnell \\nJoshua Berke and Edi Barkai \\nDept. of Psychology, Harvard Universi...  \n",
       "996   Using Feedforward Neural Networks to \\nMonitor Alertness from Changes in EEG \\nCorrelation and Coherence \\nScott Makeig \\nNaval Health Research Center, P.O. Box 85122 \\nSan Diego, CA 92186-5122 \\n...  \n",
       "1622  The Infinite Gaussian Mixture Model \\nCarl Edward Rasmussen \\nDepartment of Mathematical Modelling \\nTechnical University of Denmark \\nBuilding 321, DK-2800 Kongens Lyngby, Denmark \\ncarl@imm.dtu....  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing the dominance of topics in research papers\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "(corpus_topic_df[corpus_topic_df['Document']\n",
    "                 .isin([681, 9, 392, 1622, 17, \n",
    "                        906, 996, 503, 13, 733])])\n",
    "\n",
    "# they make perfect sense! This tells us that our topic model is working well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Research Papers per Topic based on Dominance\n",
    "A better way of representation is to try to retrieve the corresponding research paper that\n",
    "has the highest representation for each of the 20 topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1138</td>\n",
       "      <td>1</td>\n",
       "      <td>61.01</td>\n",
       "      <td>class, classification, classifier, training, pattern, feature, kernel, machine, training_set, test, sample, vector, database, error_rate, margin, experiment, support_vector, nearest_neighbor, deci...</td>\n",
       "      <td>Improving the Accuracy and Speed of \\nSupport Vector Machines \\nChris J.C. Burges \\nBell Laboratories \\nLucent Technologies, Room 3G429 \\n101 Crawford's Corner Road \\nHolmdel, NJ 07733-3030 \\nburg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131</td>\n",
       "      <td>2</td>\n",
       "      <td>56.71</td>\n",
       "      <td>neuron, memory, pattern, dynamic, connection, phase, attractor, capacity, state, hopfield, neural, fixed_point, oscillator, delay, stable, fig, oscillation, associative_memory, behavior, stored</td>\n",
       "      <td>568 \\nDYNAMICS OF ANALOG NEURAL \\nNETWORKS WITH TIME DELAY \\nC.M. Marcus and R.M. Westervelt \\nDivision of Applied Sciences and Department of Physics \\nHarvard University, Cambridge Massachusetts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>681</td>\n",
       "      <td>3</td>\n",
       "      <td>67.58</td>\n",
       "      <td>word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level</td>\n",
       "      <td>Connected Letter Recognition with a \\nMulti-State Time Delay Neural Network \\nHermann Hild and Alex Waibel \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213-3891, US...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1570</td>\n",
       "      <td>4</td>\n",
       "      <td>73.69</td>\n",
       "      <td>noise, rate, equation, curve, average, correlation, rule, distribution, theory, limit, solution, optimal, eq, teacher, effect, size, temperature, student, line, random</td>\n",
       "      <td>Dynamics of Supervised Learning with \\nRestricted Training Sets \\nA.C.C. Coolen \\nDept of Mathematics \\nKing's College London \\nStrand, London WC2R 2LS, UK \\ntcoolen @mth.kcl.ac.uk \\nD. Saad \\nNeu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>431</td>\n",
       "      <td>5</td>\n",
       "      <td>81.06</td>\n",
       "      <td>bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded</td>\n",
       "      <td>Polynomial Uniform Convergence of \\nRelative Frequencies to Probabilities \\nAlberto Bertoni, Paola Campadelll;' Anna Morpurgo, Sandra Panlzza \\nDipartimento di Scienze dell'Informazione \\nUniversi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>82</td>\n",
       "      <td>6</td>\n",
       "      <td>57.12</td>\n",
       "      <td>vector, matrix, linear, equation, solution, gradient, constraint, convergence, optimization, nonlinear, optimal, eq, minimum, operator, gradient_descent, condition, constant, derivative, quadratic...</td>\n",
       "      <td>612 \\nConstrained Differential Optimization \\nJohn C. Platt \\nAlan H. Ban' \\nCalifornia Institute of Technology, Pasadena, CA 91125 \\nAbstract \\nMany optimization models of neural networks need co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>741</td>\n",
       "      <td>7</td>\n",
       "      <td>62.82</td>\n",
       "      <td>search, task, experiment, table, instance, test, domain, target, query, feature, user, random, technique, run, accuracy, block, application, evaluation, strategy, important</td>\n",
       "      <td>When Will a Genetic Algorithm \\nOutperform Hill Climbing? \\nMelanie Mitchell \\nSanta Fe Institute \\n1660 Old Pecos Trail, Suite A \\nSanta Fe, NM 87501 \\nJohn H. Holland \\nDept. of Psychology \\nUni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1375</td>\n",
       "      <td>8</td>\n",
       "      <td>61.11</td>\n",
       "      <td>distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, ent...</td>\n",
       "      <td>Approximating Posterior Distributions \\nin Belief Networks using Mixtures \\nChristopher M. Bishop \\nNeil Lawrence \\nNeural Computing Research Group \\nDept. Computer Science &amp; Applied Mathematics \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>808</td>\n",
       "      <td>9</td>\n",
       "      <td>66.59</td>\n",
       "      <td>visual, motion, cell, response, stimulus, direction, receptive_field, map, spatial, orientation, unit, eye, field, activity, location, velocity, center, contrast, cortical, pattern</td>\n",
       "      <td>Development of Orientation and Ocular \\nDominance Columns in Infant Macaques \\nKlaus Obermayer \\nHoward Hughes Medical Institute \\nSMk-Institute \\nLa Jolla, CA 92037 \\nLynne Kiorpes \\nCenter for N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>74.86</td>\n",
       "      <td>neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate</td>\n",
       "      <td>82 \\nSIMULATIONS SUGGEST \\nINFORMATION PROCESSING ROLES \\nFOR THE DIVERSE CURRENTS IN \\nHIPPOCAMPAL NEURONS \\nLyle J. Borg-Graham \\nHarvard-MIT Division of Health Sciences and Technology and \\nCen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1128</td>\n",
       "      <td>11</td>\n",
       "      <td>49.68</td>\n",
       "      <td>state, sequence, step, recurrent, transition, stochastic, iteration, update, dynamic, probability, convergence, trajectory, xt, length, observation, continuous, rate, machine, current, string</td>\n",
       "      <td>Finite State Automata that Recurrent \\nCascade-Correlation Cannot Represent \\nStefan C. Kremer \\nDepartment of Computing Science \\nUniversity of Alberta \\nEdmonton, Alberta, CANADA T6H 5B5 \\nAbstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1034</td>\n",
       "      <td>12</td>\n",
       "      <td>45.28</td>\n",
       "      <td>node, tree, structure, graph, code, level, bit, path, local, size, variable, stage, length, solution, edge, link, component, match, binary, coding</td>\n",
       "      <td>Prediction of Beta Sheets in Proteins \\nAnders Krogh \\nThe Sanger Centre \\nHinxton, Carnbs CB10 1RQ, UK. \\nEmail: krogh@sanger. ac.uk \\nSoren Kamaric Riis \\nElectronics Institute, Building 349 \\nT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>250</td>\n",
       "      <td>13</td>\n",
       "      <td>56.75</td>\n",
       "      <td>image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location</td>\n",
       "      <td>266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>392</td>\n",
       "      <td>14</td>\n",
       "      <td>67.03</td>\n",
       "      <td>control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration</td>\n",
       "      <td>Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>277</td>\n",
       "      <td>15</td>\n",
       "      <td>62.31</td>\n",
       "      <td>unit, layer, training, hidden_unit, net, architecture, pattern, activation, trained, task, back_propagation, hidden_layer, connection, hidden, backpropagation, learn, training_set, epoch, simulati...</td>\n",
       "      <td>524 Fahlman and Lebiere \\nThe Cascade-Correlation Learning Architecture \\nScott E. Fahlman and Christian Lebiere \\nSchool of Computer Science \\nCarnegie-Mellon University \\nPittsburgh, PA 15213 \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>895</td>\n",
       "      <td>16</td>\n",
       "      <td>70.96</td>\n",
       "      <td>circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor</td>\n",
       "      <td>Single Transistor Learning Synapses \\nPaul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead \\nCalifornia Institute of Technology \\nPasadena, CA 91125 \\n(SlS) 95- 2S12 \\npaul@hobiecat.pcmp.calt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>518</td>\n",
       "      <td>17</td>\n",
       "      <td>58.33</td>\n",
       "      <td>rule, representation, module, structure, human, movement, motor, target, language, subject, connectionist, position, task, context, trajectory, hand, role, symbol, learned, theory</td>\n",
       "      <td>A Connectionist Learning Approach to Analyzing \\nLinguistic Stress \\nPrahlad Gupta \\nDepartment of Psychology \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\nDavid S. Touretzky \\nSchool of C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1362</td>\n",
       "      <td>18</td>\n",
       "      <td>55.02</td>\n",
       "      <td>vector, map, distance, cluster, local, dimension, clustering, mapping, dimensional, region, structure, center, rbf, pca, basis_function, linear, representation, global, principal_component, projec...</td>\n",
       "      <td>Mapping a manifold of perceptual observations \\nJoshua B. Tenenbaum \\nDepartment of Brain and Cognitive Sciences \\nMassachusetts Institute of Technology, Cambridge, MA 02139 \\nj bt @psyche. mi t. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>996</td>\n",
       "      <td>19</td>\n",
       "      <td>65.69</td>\n",
       "      <td>signal, filter, frequency, source, channel, noise, component, response, temporal, sound, auditory, detection, phase, ica, adaptation, amplitude, subject, eeg, change, correlation</td>\n",
       "      <td>Using Feedforward Neural Networks to \\nMonitor Alertness from Changes in EEG \\nCorrelation and Coherence \\nScott Makeig \\nNaval Health Research Center, P.O. Box 85122 \\nSan Diego, CA 92186-5122 \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1249</td>\n",
       "      <td>20</td>\n",
       "      <td>64.87</td>\n",
       "      <td>prediction, training, estimate, regression, test, noise, selection, variance, training_set, sample, ensemble, estimation, average, nonlinear, linear, estimator, cross_validation, pruning, bias, risk</td>\n",
       "      <td>Balancing between bagging and bumping \\nTom Heskes \\nRWCP Novel Functions SNN Laboratory,* University of Nijmegen \\nGeert Grooteplein 21, 6525 EZ Nijmegen, The Netherlands \\ntom@mbfys.kun.nl \\nAbs...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Document  Dominant Topic  Contribution %  \\\n",
       "Dominant Topic                                             \n",
       "1                   1138               1           61.01   \n",
       "2                    131               2           56.71   \n",
       "3                    681               3           67.58   \n",
       "4                   1570               4           73.69   \n",
       "5                    431               5           81.06   \n",
       "6                     82               6           57.12   \n",
       "7                    741               7           62.82   \n",
       "8                   1375               8           61.11   \n",
       "9                    808               9           66.59   \n",
       "10                    28              10           74.86   \n",
       "11                  1128              11           49.68   \n",
       "12                  1034              12           45.28   \n",
       "13                   250              13           56.75   \n",
       "14                   392              14           67.03   \n",
       "15                   277              15           62.31   \n",
       "16                   895              16           70.96   \n",
       "17                   518              17           58.33   \n",
       "18                  1362              18           55.02   \n",
       "19                   996              19           65.69   \n",
       "20                  1249              20           64.87   \n",
       "\n",
       "                                                                                                                                                                                                             Topic Desc  \\\n",
       "Dominant Topic                                                                                                                                                                                                            \n",
       "1               class, classification, classifier, training, pattern, feature, kernel, machine, training_set, test, sample, vector, database, error_rate, margin, experiment, support_vector, nearest_neighbor, deci...   \n",
       "2                     neuron, memory, pattern, dynamic, connection, phase, attractor, capacity, state, hopfield, neural, fixed_point, oscillator, delay, stable, fig, oscillation, associative_memory, behavior, stored   \n",
       "3                              word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level   \n",
       "4                                               noise, rate, equation, curve, average, correlation, rule, distribution, theory, limit, solution, optimal, eq, teacher, effect, size, temperature, student, line, random   \n",
       "5                           bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded   \n",
       "6               vector, matrix, linear, equation, solution, gradient, constraint, convergence, optimization, nonlinear, optimal, eq, minimum, operator, gradient_descent, condition, constant, derivative, quadratic...   \n",
       "7                                          search, task, experiment, table, instance, test, domain, target, query, feature, user, random, technique, run, accuracy, block, application, evaluation, strategy, important   \n",
       "8               distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, ent...   \n",
       "9                                  visual, motion, cell, response, stimulus, direction, receptive_field, map, spatial, orientation, unit, eye, field, activity, location, velocity, center, contrast, cortical, pattern   \n",
       "10                             neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate   \n",
       "11                      state, sequence, step, recurrent, transition, stochastic, iteration, update, dynamic, probability, convergence, trajectory, xt, length, observation, continuous, rate, machine, current, string   \n",
       "12                                                                   node, tree, structure, graph, code, level, bit, path, local, size, variable, stage, length, solution, edge, link, component, match, binary, coding   \n",
       "13                                               image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location   \n",
       "14                              control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration   \n",
       "15              unit, layer, training, hidden_unit, net, architecture, pattern, activation, trained, task, back_propagation, hidden_layer, connection, hidden, backpropagation, learn, training_set, epoch, simulati...   \n",
       "16                            circuit, chip, current, analog, voltage, implementation, processor, bit, design, device, computation, parallel, digital, operation, array, neural, synapse, element, hardware, transistor   \n",
       "17                                  rule, representation, module, structure, human, movement, motor, target, language, subject, connectionist, position, task, context, trajectory, hand, role, symbol, learned, theory   \n",
       "18              vector, map, distance, cluster, local, dimension, clustering, mapping, dimensional, region, structure, center, rbf, pca, basis_function, linear, representation, global, principal_component, projec...   \n",
       "19                                   signal, filter, frequency, source, channel, noise, component, response, temporal, sound, auditory, detection, phase, ica, adaptation, amplitude, subject, eeg, change, correlation   \n",
       "20               prediction, training, estimate, regression, test, noise, selection, variance, training_set, sample, ensemble, estimation, average, nonlinear, linear, estimator, cross_validation, pruning, bias, risk   \n",
       "\n",
       "                                                                                                                                                                                                                  Paper  \n",
       "Dominant Topic                                                                                                                                                                                                           \n",
       "1               Improving the Accuracy and Speed of \\nSupport Vector Machines \\nChris J.C. Burges \\nBell Laboratories \\nLucent Technologies, Room 3G429 \\n101 Crawford's Corner Road \\nHolmdel, NJ 07733-3030 \\nburg...  \n",
       "2               568 \\nDYNAMICS OF ANALOG NEURAL \\nNETWORKS WITH TIME DELAY \\nC.M. Marcus and R.M. Westervelt \\nDivision of Applied Sciences and Department of Physics \\nHarvard University, Cambridge Massachusetts ...  \n",
       "3               Connected Letter Recognition with a \\nMulti-State Time Delay Neural Network \\nHermann Hild and Alex Waibel \\nSchool of Computer Science \\nCarnegie Mellon University \\nPittsburgh, PA 15213-3891, US...  \n",
       "4               Dynamics of Supervised Learning with \\nRestricted Training Sets \\nA.C.C. Coolen \\nDept of Mathematics \\nKing's College London \\nStrand, London WC2R 2LS, UK \\ntcoolen @mth.kcl.ac.uk \\nD. Saad \\nNeu...  \n",
       "5               Polynomial Uniform Convergence of \\nRelative Frequencies to Probabilities \\nAlberto Bertoni, Paola Campadelll;' Anna Morpurgo, Sandra Panlzza \\nDipartimento di Scienze dell'Informazione \\nUniversi...  \n",
       "6               612 \\nConstrained Differential Optimization \\nJohn C. Platt \\nAlan H. Ban' \\nCalifornia Institute of Technology, Pasadena, CA 91125 \\nAbstract \\nMany optimization models of neural networks need co...  \n",
       "7               When Will a Genetic Algorithm \\nOutperform Hill Climbing? \\nMelanie Mitchell \\nSanta Fe Institute \\n1660 Old Pecos Trail, Suite A \\nSanta Fe, NM 87501 \\nJohn H. Holland \\nDept. of Psychology \\nUni...  \n",
       "8               Approximating Posterior Distributions \\nin Belief Networks using Mixtures \\nChristopher M. Bishop \\nNeil Lawrence \\nNeural Computing Research Group \\nDept. Computer Science & Applied Mathematics \\...  \n",
       "9               Development of Orientation and Ocular \\nDominance Columns in Infant Macaques \\nKlaus Obermayer \\nHoward Hughes Medical Institute \\nSMk-Institute \\nLa Jolla, CA 92037 \\nLynne Kiorpes \\nCenter for N...  \n",
       "10              82 \\nSIMULATIONS SUGGEST \\nINFORMATION PROCESSING ROLES \\nFOR THE DIVERSE CURRENTS IN \\nHIPPOCAMPAL NEURONS \\nLyle J. Borg-Graham \\nHarvard-MIT Division of Health Sciences and Technology and \\nCen...  \n",
       "11              Finite State Automata that Recurrent \\nCascade-Correlation Cannot Represent \\nStefan C. Kremer \\nDepartment of Computing Science \\nUniversity of Alberta \\nEdmonton, Alberta, CANADA T6H 5B5 \\nAbstr...  \n",
       "12              Prediction of Beta Sheets in Proteins \\nAnders Krogh \\nThe Sanger Centre \\nHinxton, Carnbs CB10 1RQ, UK. \\nEmail: krogh@sanger. ac.uk \\nSoren Kamaric Riis \\nElectronics Institute, Building 349 \\nT...  \n",
       "13              266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...  \n",
       "14              Integrated Modeling and Control \\nBased on Reinforcement Learning \\nand Dynamic Programming \\nRichard S. Sutton \\nGTE Laboratories Incorporated \\nWaltham, MA 02254 \\nAbstract \\nThis is a summary o...  \n",
       "15              524 Fahlman and Lebiere \\nThe Cascade-Correlation Learning Architecture \\nScott E. Fahlman and Christian Lebiere \\nSchool of Computer Science \\nCarnegie-Mellon University \\nPittsburgh, PA 15213 \\n...  \n",
       "16              Single Transistor Learning Synapses \\nPaul Hasler, Chris Diorio, Bradley A. Minch, Carver Mead \\nCalifornia Institute of Technology \\nPasadena, CA 91125 \\n(SlS) 95- 2S12 \\npaul@hobiecat.pcmp.calt...  \n",
       "17              A Connectionist Learning Approach to Analyzing \\nLinguistic Stress \\nPrahlad Gupta \\nDepartment of Psychology \\nCarnegie Mellon University \\nPittsburgh, PA 15213 \\nDavid S. Touretzky \\nSchool of C...  \n",
       "18              Mapping a manifold of perceptual observations \\nJoshua B. Tenenbaum \\nDepartment of Brain and Cognitive Sciences \\nMassachusetts Institute of Technology, Cambridge, MA 02139 \\nj bt @psyche. mi t. ...  \n",
       "19              Using Feedforward Neural Networks to \\nMonitor Alertness from Changes in EEG \\nCorrelation and Coherence \\nScott Makeig \\nNaval Health Research Center, P.O. Box 85122 \\nSan Diego, CA 92186-5122 \\n...  \n",
       "20              Balancing between bagging and bumping \\nTom Heskes \\nRWCP Novel Functions SNN Laboratory,* University of Nijmegen \\nGeert Grooteplein 21, 6525 EZ Nijmegen, The Netherlands \\ntom@mbfys.kun.nl \\nAbs...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_topic_df.groupby('Dominant Topic').apply(lambda topic_set: (topic_set.sort_values(by=['Contribution %'], \n",
    "                                                                                         ascending=False)\n",
    "                                                                             .iloc[0]))\n",
    "\n",
    "# Viewing each topic and corresponding paper with its maximum contribution\n",
    "# It looks like our model has captured the relevant latent patterns and themes in our corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Topics for New Research Papers\n",
    "Even though topic models are unsupervised models, we can estimate or predict\n",
    "potential topics for new documents based on what it has learned previously on the so-called “training” corpus!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total New Papers: 4\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# papers manually downloaded from NIPS 16\n",
    "# https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016\n",
    "\n",
    "new_paper_files = glob.glob('test_data/nips16*.txt')\n",
    "new_papers = []\n",
    "for fn in new_paper_files:\n",
    "    with open(fn, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "        new_papers.append(data)\n",
    "              \n",
    "print('Total New Papers:', len(new_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bigram_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-b572eee969b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m norm_new_papers = text_preprocessing_pipeline(documents=new_papers, normalizer_fn=normalize_corpus, \n\u001b[1;32m---> 14\u001b[1;33m                                               bigram_model=bigram_model)\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mnorm_bow_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_features_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm_new_papers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdictionary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bigram_model' is not defined"
     ]
    }
   ],
   "source": [
    "# We need to build a text wrangling and feature engineering pipeline, which should\n",
    "# match the same steps we followed when training our topic model\n",
    "def text_preprocessing_pipeline(documents, normalizer_fn, bigram_model):\n",
    "    norm_docs = normalizer_fn(documents)\n",
    "    norm_docs_bigrams = bigram_model[norm_docs]\n",
    "    return norm_docs_bigrams\n",
    "\n",
    "def bow_features_pipeline(tokenized_docs, dictionary):\n",
    "    paper_bow_features = [dictionary.doc2bow(text) \n",
    "                              for text in tokenized_docs]\n",
    "    return paper_bow_features\n",
    "\n",
    "norm_new_papers = text_preprocessing_pipeline(documents=new_papers, normalizer_fn=normalize_corpus, \n",
    "                                              bigram_model=bigram_model)\n",
    "norm_bow_features = bow_features_pipeline(tokenized_docs=norm_new_papers, dictionary=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cooperative', 'graphical_model', 'josip', 'djolonga', 'dept_computer', 'science', 'eth', 'zurich', 'josipd', 'inf', 'ethz', 'ch', 'stefanie', 'jegelka', 'csail', 'mit', 'stefje', 'mit_edu', 'sebastian', 'tschiatschek', 'dept_computer', 'science', 'eth', 'zurich', 'stschia', 'inf', 'ethz', 'ch', 'andreas', 'krause']\n"
     ]
    }
   ],
   "source": [
    "# We can now validate if the transformations worked with the following code.\n",
    "print(norm_new_papers[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (6, 1), (17, 1), (18, 1), (19, 1), (25, 1), (31, 2), (36, 2), (38, 1), (39, 17), (41, 3), (43, 1), (45, 1), (49, 2), (50, 4), (51, 1), (52, 2), (54, 1), (60, 1), (65, 1), (66, 3), (68, 7), (71, 8), (76, 4), (77, 2), (87, 1), (88, 3), (105, 1), (106, 1)]\n"
     ]
    }
   ],
   "source": [
    "print(norm_bow_features[0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s now build a generic function that can extract the top N topics from any research\n",
    "# paper using our trained model.\n",
    "def get_topic_predictions(topic_model, corpus, topn=3):\n",
    "    topic_predictions = topic_model[corpus]\n",
    "    best_topics = [[(topic, round(wt, 3)) \n",
    "                        for topic, wt in sorted(topic_predictions[i], \n",
    "                                                key=lambda row: -row[1])[:topn]] \n",
    "                            for i in range(len(topic_predictions))]\n",
    "    return best_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(7, 0.241), (4, 0.199)],\n",
       " [(13, 0.293), (4, 0.248)],\n",
       " [(12, 0.238), (9, 0.113)],\n",
       " [(2, 0.263), (12, 0.145)]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get the top two topics for each research paper because a paper or document\n",
    "# can always be a mixture of multiple topics...\n",
    "topic_preds = get_topic_predictions(topic_model=best_lda_model, \n",
    "                                    corpus=norm_bow_features, topn=2)\n",
    "topic_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let’s view the results for each paper in an easy-to-understand format.\n",
    "results_df = pd.DataFrame()\n",
    "results_df['Papers'] = range(1, len(new_papers)+1)\n",
    "results_df['Dominant Topics'] = [[topic_num+1 for topic_num, wt in item] for item in topic_preds]\n",
    "res = results_df.set_index(['Papers'])['Dominant Topics'].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "results_df = pd.DataFrame({'Dominant Topics': res.values}, index=res.index)\n",
    "results_df['Contribution %'] = [topic_wt for topic_list in \n",
    "                                        [[round(wt*100, 2) \n",
    "                                              for topic_num, wt in item] \n",
    "                                                 for item in topic_preds] \n",
    "                                    for topic_wt in topic_list]\n",
    "\n",
    "results_df['Topic Desc'] = [topics_df.iloc[t-1]['Terms per Topic'] for t in results_df['Dominant Topics'].values]\n",
    "results_df['Paper Desc'] = [new_papers[i-1][:200] for i in results_df.index.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topics</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper Desc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Papers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, entropy</td>\n",
       "      <td>Cooperative Graphical Models\\nJosip Djolonga\\nDept. of Computer Science, ETH Zurich ¨\\njosipd@inf.ethz.ch\\nStefanie Jegelka\\nCSAIL, MIT\\nstefje@mit.edu\\nSebastian Tschiatschek\\nDept. of Computer Science, ETH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>19.9</td>\n",
       "      <td>bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded</td>\n",
       "      <td>Cooperative Graphical Models\\nJosip Djolonga\\nDept. of Computer Science, ETH Zurich ¨\\njosipd@inf.ethz.ch\\nStefanie Jegelka\\nCSAIL, MIT\\nstefje@mit.edu\\nSebastian Tschiatschek\\nDept. of Computer Science, ETH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14</td>\n",
       "      <td>29.3</td>\n",
       "      <td>control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration</td>\n",
       "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York, NY 10011\\na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>24.8</td>\n",
       "      <td>bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded</td>\n",
       "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York, NY 10011\\na</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>23.8</td>\n",
       "      <td>image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location</td>\n",
       "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\nUniversit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>11.3</td>\n",
       "      <td>neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate</td>\n",
       "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\nUniversit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>26.3</td>\n",
       "      <td>word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level</td>\n",
       "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute of Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>14.5</td>\n",
       "      <td>image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location</td>\n",
       "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute of Tech</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topics  Contribution %  \\\n",
       "Papers                                    \n",
       "1                     8            24.1   \n",
       "1                     5            19.9   \n",
       "2                    14            29.3   \n",
       "2                     5            24.8   \n",
       "3                    13            23.8   \n",
       "3                    10            11.3   \n",
       "4                     3            26.3   \n",
       "4                    13            14.5   \n",
       "\n",
       "                                                                                                                                                                                                      Topic Desc  \\\n",
       "Papers                                                                                                                                                                                                             \n",
       "1       distribution, probability, prior, gaussian, variable, mixture, density, bayesian, estimate, approximation, log, likelihood, sample, component, expert, em, posterior, probabilistic, estimation, entropy   \n",
       "1                    bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded   \n",
       "2                        control, action, state, policy, environment, controller, reinforcement_learning, task, optimal, robot, goal, step, reward, td, agent, adaptive, cost, reinforcement, trial, exploration   \n",
       "2                    bound, theorem, class, probability, size, threshold, proof, polynomial, theory, complexity, loss, approximation, linear, assume, definition, defined, hypothesis, constant, define, bounded   \n",
       "3                                         image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location   \n",
       "3                       neuron, cell, spike, synaptic, activity, response, stimulus, firing, synapsis, et_al, effect, neural, neuronal, current, pattern, inhibitory, connection, brain, simulation, firing_rate   \n",
       "4                       word, recognition, training, speech, character, context, hmm, letter, mlp, speaker, feature, frame, trained, speech_recognition, phoneme, experiment, hybrid, segmentation, vowel, level   \n",
       "4                                         image, object, feature, pixel, face, view, recognition, representation, shape, scale, part, visual, region, position, scene, surface, vision, frame, texture, location   \n",
       "\n",
       "                                                                                                                                                                                                              Paper Desc  \n",
       "Papers                                                                                                                                                                                                                    \n",
       "1       Cooperative Graphical Models\\nJosip Djolonga\\nDept. of Computer Science, ETH Zurich ¨\\njosipd@inf.ethz.ch\\nStefanie Jegelka\\nCSAIL, MIT\\nstefje@mit.edu\\nSebastian Tschiatschek\\nDept. of Computer Science, ETH   \n",
       "1       Cooperative Graphical Models\\nJosip Djolonga\\nDept. of Computer Science, ETH Zurich ¨\\njosipd@inf.ethz.ch\\nStefanie Jegelka\\nCSAIL, MIT\\nstefje@mit.edu\\nSebastian Tschiatschek\\nDept. of Computer Science, ETH   \n",
       "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York, NY 10011\\na  \n",
       "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York, NY 10011\\na  \n",
       "3         Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\nUniversit  \n",
       "3         Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\nUniversit  \n",
       "4           Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute of Tech  \n",
       "4           Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute of Tech  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting topics for new papers with our LDA model\n",
    "pd.set_option('display.max_colwidth', 300)\n",
    "results_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
