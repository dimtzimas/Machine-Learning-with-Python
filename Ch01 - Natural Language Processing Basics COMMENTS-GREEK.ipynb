{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: C:\\Users\\User\\anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - orange3\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    anyqt-0.0.10               |           py37_0          46 KB\n",
      "    baycomp-1.0.2              |             py_1          20 KB\n",
      "    cachecontrol-0.12.6        |             py_0         206 KB\n",
      "    commonmark-0.9.1           |             py_0          49 KB\n",
      "    conda-4.8.3                |           py37_0         2.8 MB\n",
      "    dictdiffer-0.8.0           |             py_0          18 KB\n",
      "    h11-0.9.0                  |             py_0          46 KB\n",
      "    h2-3.2.0                   |           py37_1          84 KB\n",
      "    hpack-3.0.0                |   py37h28b3542_0          47 KB\n",
      "    httpcore-0.10.1            |             py_0          29 KB\n",
      "    httpx-0.14.0               |             py_0          50 KB\n",
      "    hyperframe-5.2.0           |             py_0          15 KB\n",
      "    keyrings.alt-3.4.0         |           py37_0          54 KB\n",
      "    lockfile-0.12.2            |           py37_0          19 KB\n",
      "    opentsne-0.4.3             |   py37h74a9793_1         463 KB\n",
      "    orange-canvas-core-0.1.15  |             py_0         504 KB\n",
      "    orange-widget-base-4.7.0   |             py_1         356 KB\n",
      "    orange3-3.26.0             |   py37ha925a31_0         3.2 MB\n",
      "    pynndescent-0.4.8          |             py_1          35 KB\n",
      "    pyqtgraph-0.11.0           |             py_0         515 KB\n",
      "    python-louvain-0.14        |             py_0          13 KB\n",
      "    rfc3986-1.4.0              |             py_0          37 KB\n",
      "    serverfiles-0.3.0          |           py37_0          16 KB\n",
      "    sniffio-1.1.0              |           py37_2          20 KB\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         8.6 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  anyqt              pkgs/main/win-64::anyqt-0.0.10-py37_0\n",
      "  baycomp            pkgs/main/noarch::baycomp-1.0.2-py_1\n",
      "  cachecontrol       pkgs/main/noarch::cachecontrol-0.12.6-py_0\n",
      "  commonmark         pkgs/main/noarch::commonmark-0.9.1-py_0\n",
      "  dictdiffer         pkgs/main/noarch::dictdiffer-0.8.0-py_0\n",
      "  h11                pkgs/main/noarch::h11-0.9.0-py_0\n",
      "  h2                 pkgs/main/win-64::h2-3.2.0-py37_1\n",
      "  hpack              pkgs/main/win-64::hpack-3.0.0-py37h28b3542_0\n",
      "  httpcore           pkgs/main/noarch::httpcore-0.10.1-py_0\n",
      "  httpx              pkgs/main/noarch::httpx-0.14.0-py_0\n",
      "  hyperframe         pkgs/main/noarch::hyperframe-5.2.0-py_0\n",
      "  keyrings.alt       pkgs/main/win-64::keyrings.alt-3.4.0-py37_0\n",
      "  lockfile           pkgs/main/win-64::lockfile-0.12.2-py37_0\n",
      "  opentsne           pkgs/main/win-64::opentsne-0.4.3-py37h74a9793_1\n",
      "  orange-canvas-core pkgs/main/noarch::orange-canvas-core-0.1.15-py_0\n",
      "  orange-widget-base pkgs/main/noarch::orange-widget-base-4.7.0-py_1\n",
      "  orange3            pkgs/main/win-64::orange3-3.26.0-py37ha925a31_0\n",
      "  pynndescent        pkgs/main/noarch::pynndescent-0.4.8-py_1\n",
      "  pyqtgraph          pkgs/main/noarch::pyqtgraph-0.11.0-py_0\n",
      "  python-louvain     pkgs/main/noarch::python-louvain-0.14-py_0\n",
      "  rfc3986            pkgs/main/noarch::rfc3986-1.4.0-py_0\n",
      "  serverfiles        pkgs/main/win-64::serverfiles-0.3.0-py37_0\n",
      "  sniffio            pkgs/main/win-64::sniffio-1.1.0-py37_2\n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  conda                                            anaconda --> pkgs/main\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "\n",
      "keyrings.alt-3.4.0   | 54 KB     |            |   0% \n",
      "keyrings.alt-3.4.0   | 54 KB     | ##9        |  30% \n",
      "keyrings.alt-3.4.0   | 54 KB     | ########## | 100% \n",
      "\n",
      "pynndescent-0.4.8    | 35 KB     |            |   0% \n",
      "pynndescent-0.4.8    | 35 KB     | ####6      |  46% \n",
      "pynndescent-0.4.8    | 35 KB     | ########## | 100% \n",
      "\n",
      "commonmark-0.9.1     | 49 KB     |            |   0% \n",
      "commonmark-0.9.1     | 49 KB     | ###2       |  33% \n",
      "commonmark-0.9.1     | 49 KB     | ########## | 100% \n",
      "\n",
      "serverfiles-0.3.0    | 16 KB     |            |   0% \n",
      "serverfiles-0.3.0    | 16 KB     | ########## | 100% \n",
      "\n",
      "cachecontrol-0.12.6  | 206 KB    |            |   0% \n",
      "cachecontrol-0.12.6  | 206 KB    | 7          |   8% \n",
      "cachecontrol-0.12.6  | 206 KB    | ###8       |  39% \n",
      "cachecontrol-0.12.6  | 206 KB    | ########## | 100% \n",
      "\n",
      "rfc3986-1.4.0        | 37 KB     |            |   0% \n",
      "rfc3986-1.4.0        | 37 KB     | ####3      |  44% \n",
      "rfc3986-1.4.0        | 37 KB     | ########## | 100% \n",
      "\n",
      "httpcore-0.10.1      | 29 KB     |            |   0% \n",
      "httpcore-0.10.1      | 29 KB     | #####4     |  55% \n",
      "httpcore-0.10.1      | 29 KB     | ########## | 100% \n",
      "\n",
      "hyperframe-5.2.0     | 15 KB     |            |   0% \n",
      "hyperframe-5.2.0     | 15 KB     | ########## | 100% \n",
      "\n",
      "orange-canvas-core-0 | 504 KB    |            |   0% \n",
      "orange-canvas-core-0 | 504 KB    | 3          |   3% \n",
      "orange-canvas-core-0 | 504 KB    | ###1       |  32% \n",
      "orange-canvas-core-0 | 504 KB    | ####7      |  48% \n",
      "orange-canvas-core-0 | 504 KB    | ######6    |  67% \n",
      "orange-canvas-core-0 | 504 KB    | ########## | 100% \n",
      "\n",
      "dictdiffer-0.8.0     | 18 KB     |            |   0% \n",
      "dictdiffer-0.8.0     | 18 KB     | ########9  |  90% \n",
      "dictdiffer-0.8.0     | 18 KB     | ########## | 100% \n",
      "\n",
      "h11-0.9.0            | 46 KB     |            |   0% \n",
      "h11-0.9.0            | 46 KB     | ###4       |  35% \n",
      "h11-0.9.0            | 46 KB     | ########## | 100% \n",
      "\n",
      "h2-3.2.0             | 84 KB     |            |   0% \n",
      "h2-3.2.0             | 84 KB     | #9         |  19% \n",
      "h2-3.2.0             | 84 KB     | ########## | 100% \n",
      "\n",
      "orange-widget-base-4 | 356 KB    |            |   0% \n",
      "orange-widget-base-4 | 356 KB    | 4          |   5% \n",
      "orange-widget-base-4 | 356 KB    | ##2        |  23% \n",
      "orange-widget-base-4 | 356 KB    | ####9      |  50% \n",
      "orange-widget-base-4 | 356 KB    | #######2   |  72% \n",
      "orange-widget-base-4 | 356 KB    | ########## | 100% \n",
      "\n",
      "opentsne-0.4.3       | 463 KB    |            |   0% \n",
      "opentsne-0.4.3       | 463 KB    | 3          |   3% \n",
      "opentsne-0.4.3       | 463 KB    | ##         |  21% \n",
      "opentsne-0.4.3       | 463 KB    | #####8     |  59% \n",
      "opentsne-0.4.3       | 463 KB    | ########2  |  83% \n",
      "opentsne-0.4.3       | 463 KB    | ########## | 100% \n",
      "\n",
      "orange3-3.26.0       | 3.2 MB    |            |   0% \n",
      "orange3-3.26.0       | 3.2 MB    |            |   0% \n",
      "orange3-3.26.0       | 3.2 MB    | 1          |   1% \n",
      "orange3-3.26.0       | 3.2 MB    | #1         |  12% \n",
      "orange3-3.26.0       | 3.2 MB    | #8         |  19% \n",
      "orange3-3.26.0       | 3.2 MB    | ##4        |  24% \n",
      "orange3-3.26.0       | 3.2 MB    | ###4       |  35% \n",
      "orange3-3.26.0       | 3.2 MB    | ####       |  41% \n",
      "orange3-3.26.0       | 3.2 MB    | #####6     |  56% \n",
      "orange3-3.26.0       | 3.2 MB    | ######9    |  70% \n",
      "orange3-3.26.0       | 3.2 MB    | #######6   |  76% \n",
      "orange3-3.26.0       | 3.2 MB    | ########7  |  87% \n",
      "orange3-3.26.0       | 3.2 MB    | #########3 |  94% \n",
      "orange3-3.26.0       | 3.2 MB    | ########## | 100% \n",
      "\n",
      "httpx-0.14.0         | 50 KB     |            |   0% \n",
      "httpx-0.14.0         | 50 KB     | ###1       |  32% \n",
      "httpx-0.14.0         | 50 KB     | ########## | 100% \n",
      "\n",
      "pyqtgraph-0.11.0     | 515 KB    |            |   0% \n",
      "pyqtgraph-0.11.0     | 515 KB    | 3          |   3% \n",
      "pyqtgraph-0.11.0     | 515 KB    | ###4       |  34% \n",
      "pyqtgraph-0.11.0     | 515 KB    | ####9      |  50% \n",
      "pyqtgraph-0.11.0     | 515 KB    | ######2    |  62% \n",
      "pyqtgraph-0.11.0     | 515 KB    | #########9 |  99% \n",
      "pyqtgraph-0.11.0     | 515 KB    | ########## | 100% \n",
      "\n",
      "hpack-3.0.0          | 47 KB     |            |   0% \n",
      "hpack-3.0.0          | 47 KB     | ###3       |  34% \n",
      "hpack-3.0.0          | 47 KB     | ########## | 100% \n",
      "\n",
      "baycomp-1.0.2        | 20 KB     |            |   0% \n",
      "baycomp-1.0.2        | 20 KB     | ########1  |  82% \n",
      "baycomp-1.0.2        | 20 KB     | ########## | 100% \n",
      "\n",
      "lockfile-0.12.2      | 19 KB     |            |   0% \n",
      "lockfile-0.12.2      | 19 KB     | ########4  |  84% \n",
      "lockfile-0.12.2      | 19 KB     | ########## | 100% \n",
      "\n",
      "python-louvain-0.14  | 13 KB     |            |   0% \n",
      "python-louvain-0.14  | 13 KB     | ########## | 100% \n",
      "\n",
      "anyqt-0.0.10         | 46 KB     |            |   0% \n",
      "anyqt-0.0.10         | 46 KB     | ###5       |  35% \n",
      "anyqt-0.0.10         | 46 KB     | ########## | 100% \n",
      "\n",
      "conda-4.8.3          | 2.8 MB    |            |   0% \n",
      "conda-4.8.3          | 2.8 MB    | 1          |   2% \n",
      "conda-4.8.3          | 2.8 MB    | 9          |  10% \n",
      "conda-4.8.3          | 2.8 MB    | ##         |  20% \n",
      "conda-4.8.3          | 2.8 MB    | ##3        |  24% \n",
      "conda-4.8.3          | 2.8 MB    | ##7        |  27% \n",
      "conda-4.8.3          | 2.8 MB    | ###        |  31% \n",
      "conda-4.8.3          | 2.8 MB    | ###4       |  34% \n",
      "conda-4.8.3          | 2.8 MB    | ###7       |  37% \n",
      "conda-4.8.3          | 2.8 MB    | ####1      |  41% \n",
      "conda-4.8.3          | 2.8 MB    | ####5      |  45% \n",
      "conda-4.8.3          | 2.8 MB    | ####9      |  49% \n",
      "conda-4.8.3          | 2.8 MB    | #####4     |  54% \n",
      "conda-4.8.3          | 2.8 MB    | ######     |  60% \n",
      "conda-4.8.3          | 2.8 MB    | ######5    |  65% \n",
      "conda-4.8.3          | 2.8 MB    | ######9    |  70% \n",
      "conda-4.8.3          | 2.8 MB    | #######4   |  74% \n",
      "conda-4.8.3          | 2.8 MB    | #######8   |  79% \n",
      "conda-4.8.3          | 2.8 MB    | ########4  |  84% \n",
      "conda-4.8.3          | 2.8 MB    | ########9  |  89% \n",
      "conda-4.8.3          | 2.8 MB    | #########4 |  95% \n",
      "conda-4.8.3          | 2.8 MB    | ########## | 100% \n",
      "\n",
      "sniffio-1.1.0        | 20 KB     |            |   0% \n",
      "sniffio-1.1.0        | 20 KB     | #######9   |  79% \n",
      "sniffio-1.1.0        | 20 KB     | ########## | 100% \n",
      "Preparing transaction: ...working... done\n",
      "Verifying transaction: ...working... done\n",
      "Executing transaction: ...working... done\n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>set \"KERAS_BACKEND=\" \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>python C:\\Users\\User\\anaconda3\\etc\\keras\\load_config.py  1>temp.txt \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>set /p KERAS_BACKEND= 0<temp.txt \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>del temp.txt \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>python -c \"import keras\"  1>nul 2>&1 \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>if errorlevel 1 (\n",
      "ver  1>nul  \n",
      " set \"KERAS_BACKEND=theano\"  \n",
      " python -c \"import keras\"  1>nul 2>&1 \n",
      ") \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>SET DISTUTILS_USE_SDK=1 \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>SET MSSdk=1 \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>SET platform= \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>IF /I [AMD64] == [amd64] set \"platform=true\" \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>IF /I [] == [amd64] set \"platform=true\" \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>if defined platform (set \"VSREGKEY=HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\VisualStudio\\14.0\" )  ELSE (set \"VSREGKEY=HKEY_LOCAL_MACHINE\\SOFTWARE\\Microsoft\\VisualStudio\\14.0\" ) \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>for /F \"skip=2 tokens=2,*\" %A in ('reg query \"HKEY_LOCAL_MACHINE\\SOFTWARE\\Wow6432Node\\Microsoft\\VisualStudio\\14.0\" /v InstallDir') do SET \"VSINSTALLDIR=%B\" \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>if \"\" == \"\" (set \"VSINSTALLDIR=\" ) \n",
      "\n",
      "C:\\Users\\User\\text-analytics-w-python-2e-master - 2020\\Ch01 - Natural Language Processing Basics>if \"\" == \"\" (\n",
      "ECHO \"WARNING: Did not find VS in registry or in VS140COMNTOOLS env var - your compiler may not work\"  \n",
      " GOTO End \n",
      ") \n",
      "\"WARNING: Did not find VS in registry or in VS140COMNTOOLS env var - your compiler may not work\"\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: The system was unable to find the specified registry key or value.\n",
      "The system cannot find the batch label specified - End\n"
     ]
    }
   ],
   "source": [
    "conda install orange3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Η καφέ αλεπού είναι γρήγορη και πηδάει πάνω από το τεμπέλικο σκυλί'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following snippet shows us how the sentence looks in Python.\n",
    "sentence = \"Η καφέ αλεπού είναι γρήγορη και πηδάει πάνω από το τεμπέλικο σκυλί\"\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dependencies\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package abc is already up-to-date!\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package alpino is already up-to-date!\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package biocreative_ppi is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown is already up-to-date!\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package brown_tei is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_cat is already up-to-date!\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cess_esp is already up-to-date!\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package chat80 is already up-to-date!\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package city_database is already up-to-date!\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comparative_sentences is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package comtrans is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2000 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2002 is already up-to-date!\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package conll2007 is already up-to-date!\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package crubadan is already up-to-date!\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dependency_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package dolch is already up-to-date!\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package europarl_raw is already up-to-date!\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package floresta is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v15 is already up-to-date!\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package framenet_v17 is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ieer is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package indian is already up-to-date!\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package jeita is already up-to-date!\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package kimmo is already up-to-date!\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package knbc is already up-to-date!\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package lin_thesaurus is already up-to-date!\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mac_morpho is already up-to-date!\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package machado is already up-to-date!\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package masc_tagged is already up-to-date!\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package moses_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nombank.1.0 is already up-to-date!\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nps_chat is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package paradigms is already up-to-date!\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pil is already up-to-date!\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package pl196x is already up-to-date!\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ppattach is already up-to-date!\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package problem_reports is already up-to-date!\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package propbank is already up-to-date!\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ptb is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_1 is already up-to-date!\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package product_reviews_2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package pros_cons is already up-to-date!\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package qc is already up-to-date!\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package reuters is already up-to-date!\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rte is already up-to-date!\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package semcor is already up-to-date!\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package senseval is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sentence_polarity is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sinica_treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package smultron is already up-to-date!\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package state_union is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package subjectivity is already up-to-date!\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package switchboard is already up-to-date!\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package timit is already up-to-date!\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package toolbox is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr is already up-to-date!\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package udhr2 is already up-to-date!\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package unicode_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_treebanks_v20 is already up-to-\n",
      "[nltk_data]    |       date!\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package verbnet3 is already up-to-date!\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package webtext is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package ycoe is already up-to-date!\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package rslp is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_treebank_pos_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package universal_tagset is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package book_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package sample_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package spanish_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package basque_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package large_grammars is already up-to-date!\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package tagsets is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package bllip_wsj_no_aux is already up-to-date!\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package word2vec_sample is already up-to-date!\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package panlex_swadesh is already up-to-date!\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mte_teip5 is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger_ru is already\n",
      "[nltk_data]    |       up-to-date!\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package perluniprops is already up-to-date!\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package nonbreaking_prefixes is already up-to-date!\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]    |   Package porter_test is already up-to-date!\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wmt15_eval is already up-to-date!\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package mwa_ppdb is already up-to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all', halt_on_error=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed with initial frozen solve. Retrying with flexible solve.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - el_core_news_lg\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://spacy.io/models/el\n",
    "# !pip install el_core_news_sm\n",
    "# !pip install el_core_news_md\n",
    "!conda install el_core_news_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Αυτή', 'PRON'), ('είναι', 'AUX'), ('μια', 'DET'), ('πρόταση', 'NOUN'), ('.', 'PUNCT')]\n",
      "[('Η', 'DET'), ('καφέ', 'NOUN'), ('αλεπού', 'NOUN'), ('είναι', 'AUX'), ('γρήγορη', 'ADJ'), ('και', 'CCONJ'), ('πηδάει', 'VERB'), ('πάνω', 'ADV'), ('από', 'ADP'), ('το', 'DET'), ('τεμπέλικο', 'ADJ'), ('σκυλί', 'NOUN')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# following line is optional for custom vocabulary installation\n",
    "# you can use nlp = spacy.load('en')\n",
    "# nlp = spacy.load(\"el_core_news_sm\")\n",
    "import el_core_news_md\n",
    "nlp = el_core_news_md.load()\n",
    "\n",
    "doc = nlp(\"Αυτή είναι μια πρόταση.\")\n",
    "print([(w.text, w.pos_) for w in doc])\n",
    "\n",
    "doc1 = nlp(sentence)\n",
    "print([(w.text, w.pos_) for w in doc1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['καφέ', 'σκυλί', 'και', 'αλεπού', 'το', 'πηδάει', 'πάνω', 'από', 'τεμπέλικο', 'είναι', 'Η', 'γρήγορη']\n"
     ]
    }
   ],
   "source": [
    "# grammar and ordering of words definitely gives meaning\n",
    "words = sentence.split()\n",
    "np.random.shuffle(words)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('irritate.v.02'), Synset('harass.v.01'), Synset('tease.v.01')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet as wn\n",
    "# wn.synsets('ενοχλώ', lang='ell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original token: Τα , Lemma: τα\n",
      "Original token: σύμβολα , Lemma: σύμβολο\n",
      "Original token: του , Lemma: του\n",
      "Original token: αγώνα , Lemma: αγώνας\n",
      "Original token: . , Lemma: .\n"
     ]
    }
   ],
   "source": [
    "# annotate text\n",
    "# pos_tags = nltk.pos_tag(doc.split())\n",
    "# print (pos_tags)\n",
    "\n",
    "# from cltk.tag.pos import POSTag\n",
    "# from cltk.corpus.utils.importer import CorpusImporter\n",
    "# corpus_importer = CorpusImporter('greek')  # e.g., or CorpusImporter('latin')\n",
    "# corpus_importer.list_corpora\n",
    "# tagger = POSTag('greek')\n",
    "\n",
    "# Example: lemmatizer example\n",
    "# https://bit.ly/30N0leM - Daras\n",
    "\n",
    "text = '''Τα σύμβολα του αγώνα.'''\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(\"Original token: {} , Lemma: {}\".format(token,token.lemma_))\n",
    "\n",
    "# #pd.DataFrame(pos_tags).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token:η, DEP tag: det\n",
      "Token:δημοκρατία, DEP tag: nsubj\n",
      "Token:είναι, DEP tag: cop\n",
      "Token:το, DEP tag: det\n",
      "Token:πιο, DEP tag: advmod\n",
      "Token:ανθρώπινο, DEP tag: amod\n",
      "Token:πολίτευμα, DEP tag: ROOT\n",
      "Token:., DEP tag: punct\n",
      "          πολίτευμα                     \n",
      "   ___________|____________________      \n",
      "  |    |      |     δημοκρατία ανθρώπινο\n",
      "  |    |      |         |          |     \n",
      "είναι  το     .         η         πιο   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import Tree\n",
    "doc = nlp(\"η δημοκρατία είναι το πιο ανθρώπινο πολίτευμα.\")\n",
    "\n",
    "for token in doc:\n",
    "    print('Token:{}, DEP tag: {}'.format(token,token.dep_))\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "      return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "      return node.orth_\n",
    "[to_nltk_tree(sent.root).pretty_print() for sent in doc.sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Η</td>\n",
       "      <td>καφέ</td>\n",
       "      <td>αλεπού</td>\n",
       "      <td>είναι</td>\n",
       "      <td>γρήγορη</td>\n",
       "      <td>και</td>\n",
       "      <td>πηδάει</td>\n",
       "      <td>πάνω</td>\n",
       "      <td>από</td>\n",
       "      <td>το</td>\n",
       "      <td>τεμπέλικο</td>\n",
       "      <td>σκυλί</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DET</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>AUX</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ADV</td>\n",
       "      <td>ADP</td>\n",
       "      <td>DET</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>NOUN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>η</td>\n",
       "      <td>καφέ</td>\n",
       "      <td>αλεπός</td>\n",
       "      <td>είναι</td>\n",
       "      <td>γρήγορη</td>\n",
       "      <td>και</td>\n",
       "      <td>πηδάω</td>\n",
       "      <td>πάνω</td>\n",
       "      <td>από</td>\n",
       "      <td>το</td>\n",
       "      <td>τεμπέλικος</td>\n",
       "      <td>σκυλί</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>det</td>\n",
       "      <td>nsubj</td>\n",
       "      <td>flat</td>\n",
       "      <td>cop</td>\n",
       "      <td>ROOT</td>\n",
       "      <td>cc</td>\n",
       "      <td>conj</td>\n",
       "      <td>advmod</td>\n",
       "      <td>case</td>\n",
       "      <td>det</td>\n",
       "      <td>amod</td>\n",
       "      <td>obl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0      1       2      3        4      5       6       7     8    9  \\\n",
       "0    Η   καφέ  αλεπού  είναι  γρήγορη    και  πηδάει    πάνω   από   το   \n",
       "1  DET   NOUN    NOUN    AUX      ADJ  CCONJ    VERB     ADV   ADP  DET   \n",
       "2  DET   NOUN    NOUN    AUX      ADJ  CCONJ    VERB     ADV   ADP  DET   \n",
       "3    η   καφέ  αλεπός  είναι  γρήγορη    και   πηδάω    πάνω   από   το   \n",
       "4  det  nsubj    flat    cop     ROOT     cc    conj  advmod  case  det   \n",
       "\n",
       "           10     11  \n",
       "0   τεμπέλικο  σκυλί  \n",
       "1         ADJ   NOUN  \n",
       "2         ADJ   NOUN  \n",
       "3  τεμπέλικος  σκυλί  \n",
       "4        amod    obl  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# leverage spaCy to understand the high-level semantics of each tag annotation\n",
    "spacy_pos_tagged = [(word, word.tag_, word.pos_, word.lemma_, word.dep_) for word in nlp(sentence)]\n",
    "pd.DataFrame(spacy_pos_tagged).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Η/JJ καφέ/NNP)\n",
      "  (NP αλεπού/NNP)\n",
      "  (NP είναι/NNP)\n",
      "  (NP γρήγορη/NNP)\n",
      "  (NP και/NNP)\n",
      "  (NP πηδάει/NNP)\n",
      "  (NP πάνω/NNP)\n",
      "  (NP από/NNP)\n",
      "  (NP το/NNP)\n",
      "  (NP τεμπέλικο/NNP)\n",
      "  (NP σκυλί/NN))\n"
     ]
    }
   ],
   "source": [
    "# define your own rules for phrases and then enable shallow parsing or chunking using a lookup based parser similar to NLTK’s RegexpParser. It’s a grammar\n",
    "# based chunk parser and uses a set of regular expression patterns (defined grammar\n",
    "# rules) to specify the behavior of the parser\n",
    "grammar = '''\n",
    "            NP: {<DT>?<JJ>?<NN.*>}  \n",
    "            ADJP: {<JJ>}\n",
    "            ADVP: {<RB.*>}\n",
    "            PP: {<IN>}      \n",
    "            VP: {<MD>?<VB.*>+}\n",
    "          '''\n",
    "\n",
    "pos_tagged_sent = nltk.pos_tag(sentence.split())\n",
    "rp = nltk.RegexpParser(grammar)\n",
    "shallow_parsed_sent = rp.parse(pos_tagged_sent)\n",
    "print(shallow_parsed_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The Ghostscript executable isn't found.\n",
      "See http://web.mit.edu/ghostscript/www/Install.htm\n",
      "If you're using a Mac, you can try installing\n",
      "https://docs.brew.sh/Installation then `brew install ghostscript`\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    805\u001b[0m                             \u001b[0menv_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'PATH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 806\u001b[1;33m                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    807\u001b[0m                         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    696\u001b[0m         find_binary_iter(\n\u001b[1;32m--> 697\u001b[1;33m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_bin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_binary_iter\u001b[1;34m(name, path_to_bin, env_vars, searchpath, binary_names, url, verbose)\u001b[0m\n\u001b[0;32m    680\u001b[0m     for file in find_file_iter(\n\u001b[1;32m--> 681\u001b[1;33m         \u001b[0mpath_to_bin\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv_vars\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msearchpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    682\u001b[0m     ):\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_file_iter\u001b[1;34m(filename, env_vars, searchpath, file_names, url, verbose, finding_dir)\u001b[0m\n\u001b[0;32m    638\u001b[0m         \u001b[0mdiv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m75\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 639\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\n\\n%s\\n%s\\n%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdiv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdiv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n\n===========================================================================\nNLTK was unable to find the gs file!\nUse software specific configuration paramaters or set the PATH environment variable.\n===========================================================================",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 345\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    346\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    817\u001b[0m                                         \"https://docs.brew.sh/Installation then `brew install ghostscript`\")                \n\u001b[0;32m    818\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_error_message\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "Tree('S', [Tree('NP', [('The', 'DT'), ('brown', 'JJ'), ('fox', 'NN')]), Tree('VP', [('is', 'VBZ')]), Tree('ADJP', [('quick', 'JJ')]), ('and', 'CC'), ('he', 'PRP'), Tree('VP', [('is', 'VBZ'), ('jumping', 'VBG')]), Tree('PP', [('over', 'IN')]), Tree('NP', [('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')])])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize shallow parse tree\n",
    "shallow_parsed_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"el\" id=\"0e10f77759ac4e87855b7489d7816046-0\" class=\"displacy\" width=\"1250\" height=\"336.5\" direction=\"ltr\" style=\"max-width: none; height: 336.5px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Η</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">καφέ</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">αλεπού</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">είναι</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">γρήγορη</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">και</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">πηδάει</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">πάνω</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">από</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">το</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1050\">τεμπέλικο</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1050\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"246.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1150\">σκυλί</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-0\" stroke-width=\"1.5px\" d=\"M70,201.5 C70,151.5 135.0,151.5 135.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,203.5 L64,193.5 76,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-1\" stroke-width=\"1.5px\" d=\"M170,201.5 C170,51.5 445.0,51.5 445.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M170,203.5 L164,193.5 176,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-2\" stroke-width=\"1.5px\" d=\"M170,201.5 C170,151.5 235.0,151.5 235.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">flat</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M235.0,203.5 L241.0,193.5 229.0,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-3\" stroke-width=\"1.5px\" d=\"M370,201.5 C370,151.5 435.0,151.5 435.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cop</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M370,203.5 L364,193.5 376,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-4\" stroke-width=\"1.5px\" d=\"M570,201.5 C570,151.5 635.0,151.5 635.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M570,203.5 L564,193.5 576,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-5\" stroke-width=\"1.5px\" d=\"M470,201.5 C470,101.5 640.0,101.5 640.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M640.0,203.5 L646.0,193.5 634.0,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-6\" stroke-width=\"1.5px\" d=\"M670,201.5 C670,151.5 735.0,151.5 735.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M735.0,203.5 L741.0,193.5 729.0,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-7\" stroke-width=\"1.5px\" d=\"M870,201.5 C870,51.5 1145.0,51.5 1145.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M870,203.5 L864,193.5 876,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-8\" stroke-width=\"1.5px\" d=\"M970,201.5 C970,101.5 1140.0,101.5 1140.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M970,203.5 L964,193.5 976,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-9\" stroke-width=\"1.5px\" d=\"M1070,201.5 C1070,151.5 1135.0,151.5 1135.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1070,203.5 L1064,193.5 1076,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0e10f77759ac4e87855b7489d7816046-0-10\" stroke-width=\"1.5px\" d=\"M670,201.5 C670,1.5 1150.0,1.5 1150.0,201.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0e10f77759ac4e87855b7489d7816046-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1150.0,203.5 L1156.0,193.5 1144.0,193.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# leverage spacy\n",
    "from spacy import displacy\n",
    "\n",
    "displacy.render(nlp(sentence), jupyter=True, \n",
    "                options={'distance': 100,\n",
    "                         'arrow_stroke': 1.5,\n",
    "                         'arrow_width': 8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:6: DeprecationWarning: The StanfordParser will be deprecated\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPParser\u001b[0m instead.\n",
      "  \n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-parser\\.jar jar file at E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8b54ca6db060>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n\u001b[1;32m----> 6\u001b[1;33m                    path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m         )\n\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mStanfordParser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_tree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\parse\\stanford.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options, corenlp_options)\u001b[0m\n\u001b[0;32m     65\u001b[0m                 \u001b[0mis_regex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             ),\n\u001b[1;32m---> 67\u001b[1;33m             \u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m         )\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\nltk\\internals.py\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[1;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m             raise LookupError(\n\u001b[1;32m--> 740\u001b[1;33m                 \u001b[1;34m'Could not find %s jar file at %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mname_pattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath_to_jar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m             )\n\u001b[0;32m    742\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: Could not find stanford-parser\\.jar jar file at E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar"
     ]
    }
   ],
   "source": [
    "# leverage Stanford’s Core NLP-based parsers in NLTK to perform\n",
    "# constituency parsing\n",
    "from nltk.parse.stanford import StanfordParser\n",
    "\n",
    "scp = StanfordParser(path_to_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser.jar',\n",
    "                   path_to_models_jar='E:/stanford/stanford-parser-full-2015-04-20/stanford-parser-3.5.2-models.jar')\n",
    "                   \n",
    "result = list(scp.raw_parse(sentence))\n",
    "print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-0e6c11facb9c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# visualize constituency tree\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'result' is not defined"
     ]
    }
   ],
   "source": [
    "# visualize constituency tree\n",
    "result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories: 15\n"
     ]
    }
   ],
   "source": [
    "# load the Brown Corpus\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# total categories\n",
    "print('Total Categories:', len(brown.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies', 'humor', 'learned', 'lore', 'mystery', 'news', 'religion', 'reviews', 'romance', 'science_fiction']\n"
     ]
    }
   ],
   "source": [
    "# print the categories\n",
    "print(brown.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['There', 'were', 'thirty-eight', 'patients', 'on', 'the', 'bus', 'the', 'morning', 'I', 'left', 'for', 'Hanover', ',', 'most', 'of', 'them', 'disturbed', 'and', 'hallucinating', '.'], ['An', 'interne', ',', 'a', 'nurse', 'and', 'two', 'attendants', 'were', 'in', 'charge', 'of', 'us', '.'], ...]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenized sentences\n",
    "brown.sents(categories='mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('There', 'EX'), ('were', 'BED'), ('thirty-eight', 'CD'), ('patients', 'NNS'), ('on', 'IN'), ('the', 'AT'), ('bus', 'NN'), ('the', 'AT'), ('morning', 'NN'), ('I', 'PPSS'), ('left', 'VBD'), ('for', 'IN'), ('Hanover', 'NP'), (',', ','), ('most', 'AP'), ('of', 'IN'), ('them', 'PPO'), ('disturbed', 'VBN'), ('and', 'CC'), ('hallucinating', 'VBG'), ('.', '.')], [('An', 'AT'), ('interne', 'NN'), (',', ','), ('a', 'AT'), ('nurse', 'NN'), ('and', 'CC'), ('two', 'CD'), ('attendants', 'NNS'), ('were', 'BED'), ('in', 'IN'), ('charge', 'NN'), ('of', 'IN'), ('us', 'PPO'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS tagged sentences\n",
    "brown.tagged_sents(categories='mystery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['There were thirty-eight patients on the bus the morning I left for Hanover , most of them disturbed and hallucinating .',\n",
       " 'An interne , a nurse and two attendants were in charge of us .',\n",
       " \"I felt lonely and depressed as I stared out the bus window at Chicago's grim , dirty West Side .\",\n",
       " 'It seemed incredible , as I listened to the monotonous drone of voices and smelled the fetid odors coming from the patients , that technically I was a ward of the state of Illinois , going to a hospital for the mentally ill .',\n",
       " 'I suddenly thought of Mary Jane Brennan , the way her pretty eyes could flash with anger , her quiet competence , the gentleness and sweetness that lay just beneath the surface of her defenses .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get sentences in natural form\n",
    "sentences = brown.sents(categories='mystery')\n",
    "sentences = [' '.join(sentence_token) for sentence_token in sentences]\n",
    "sentences[0:5] # viewing the first 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('patients', 'NNS'),\n",
       " ('bus', 'NN'),\n",
       " ('morning', 'NN'),\n",
       " ('Hanover', 'NP'),\n",
       " ('interne', 'NN'),\n",
       " ('nurse', 'NN'),\n",
       " ('attendants', 'NNS'),\n",
       " ('charge', 'NN'),\n",
       " ('bus', 'NN'),\n",
       " ('window', 'NN')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get tagged words\n",
    "tagged_words = brown.tagged_words(categories='mystery')\n",
    "\n",
    "# get nouns from tagged words\n",
    "nouns = [(word, tag) for word, tag in tagged_words if any(noun_tag in tag for noun_tag in ['NP', 'NN'])]\n",
    "\n",
    "nouns[0:10] # view the first 10 nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('man', 106),\n",
       " ('time', 82),\n",
       " ('door', 80),\n",
       " ('car', 69),\n",
       " ('room', 65),\n",
       " ('Mr.', 63),\n",
       " ('way', 61),\n",
       " ('office', 50),\n",
       " ('eyes', 48),\n",
       " ('hand', 46)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build frequency distribution for nouns\n",
    "nouns_freq = nltk.FreqDist([word for word, tag in nouns])\n",
    "\n",
    "# view top 10 occuring nouns\n",
    "nouns_freq.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Categories: 90\n"
     ]
    }
   ],
   "source": [
    "# load the Reuters Corpus\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# total categories\n",
    "print('Total Categories:', len(reuters.categories()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acq', 'alum', 'barley', 'bop', 'carcass', 'castor-oil', 'cocoa', 'coconut', 'coconut-oil', 'coffee', 'copper', 'copra-cake', 'corn', 'cotton', 'cotton-oil', 'cpi', 'cpu', 'crude', 'dfl', 'dlr', 'dmk', 'earn', 'fuel', 'gas', 'gnp', 'gold', 'grain', 'groundnut', 'groundnut-oil', 'heat', 'hog', 'housing', 'income', 'instal-debt', 'interest', 'ipi', 'iron-steel', 'jet', 'jobs', 'l-cattle', 'lead', 'lei', 'lin-oil', 'livestock', 'lumber', 'meal-feed', 'money-fx', 'money-supply', 'naphtha', 'nat-gas', 'nickel', 'nkr', 'nzdlr', 'oat', 'oilseed', 'orange', 'palladium', 'palm-oil', 'palmkernel', 'pet-chem', 'platinum', 'potato', 'propane', 'rand', 'rape-oil', 'rapeseed', 'reserves', 'retail', 'rice', 'rubber', 'rye', 'ship', 'silver', 'sorghum', 'soy-meal', 'soy-oil', 'soybean', 'strategic-metal', 'sugar', 'sun-meal', 'sun-oil', 'sunseed', 'tea', 'tin', 'trade', 'veg-oil', 'wheat', 'wpi', 'yen', 'zinc']\n"
     ]
    }
   ],
   "source": [
    "# print the categories\n",
    "print(reuters.categories())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"YUGOSLAV ECONOMY WORSENED IN 1986 , BANK DATA SHOWS National Bank economic data for 1986 shows that Yugoslavia ' s trade deficit grew , the inflation rate rose , wages were sharply higher , the money supply expanded and the value of the dinar fell .\",\n",
       " 'The trade deficit for 1986 was 2 . 012 billion dlrs , 25 . 7 pct higher than in 1985 .',\n",
       " 'The trend continued in the first three months of this year as exports dropped by 17 . 8 pct , in hard currency terms , to 2 . 124 billion dlrs .',\n",
       " 'Yugoslavia this year started quoting trade figures in dinars based on current exchange rates , instead of dollars based on a fixed exchange rate of 264 . 53 dinars per dollar .',\n",
       " \"Yugoslavia ' s balance of payments surplus with the convertible currency area fell to 245 mln dlrs in 1986 from 344 mln in 1985 .\"]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get sentences in housing and income categories\n",
    "sentences = reuters.sents(categories=['housing', 'income'])\n",
    "sentences = [' '.join(sentence_tokens) for sentence_tokens in sentences]\n",
    "sentences[0:5]  # view the first 5 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test/16118', 'test/18534', 'test/18540', 'test/18664', 'test/18665', 'test/18672', 'test/18911', 'test/19875', 'test/20106', 'test/20116', 'training/1035', 'training/1036', 'training/10602', 'training/10604', 'training/11170', 'training/11665', 'training/2618', 'training/29', 'training/3105', 'training/3708', 'training/3720', 'training/3723', 'training/3898', 'training/5883', 'training/5886', 'training/6000', 'training/6067', 'training/6197', 'training/7005', 'training/7006', 'training/7015', 'training/7036', 'training/7098', 'training/7099', 'training/9615']\n"
     ]
    }
   ],
   "source": [
    "# fileid based access\n",
    "print(reuters.fileids(categories=['housing', 'income']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['YUGOSLAV', 'ECONOMY', 'WORSENED', 'IN', '1986', ',', 'BANK', 'DATA', 'SHOWS', 'National', 'Bank', 'economic', 'data', 'for', '1986', 'shows', 'that', 'Yugoslavia', \"'\", 's', 'trade', 'deficit', 'grew', ',', 'the', 'inflation', 'rate', 'rose', ',', 'wages', 'were', 'sharply', 'higher', ',', 'the', 'money', 'supply', 'expanded', 'and', 'the', 'value', 'of', 'the', 'dinar', 'fell', '.'], ['The', 'trade', 'deficit', 'for', '1986', 'was', '2', '.', '012', 'billion', 'dlrs', ',', '25', '.', '7', 'pct', 'higher', 'than', 'in', '1985', '.'], ...]\n"
     ]
    }
   ],
   "source": [
    "print(reuters.sents(fileids=[u'test/16118', u'test/18534']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('following.s.02'),\n",
       " Synset('comparable_to.s.01'),\n",
       " Synset('different.s.02'),\n",
       " Synset('another.s.01'),\n",
       " Synset('opposite.s.04'),\n",
       " Synset('stranger.n.01')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the Wordnet Corpus\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "word = 'hike' # taking hike as our word of interest\n",
    "# get word synsets\n",
    "word_synsets = wn.synsets(word)\n",
    "word_synsets\n",
    "sorted(wn.langs())\n",
    "wn.synsets('διάλειμμα', lang='ell')\n",
    "wn.synsets('άμμος', lang='ell')\n",
    "wn.synsets('άλλος', lang='ell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset Name: hike.n.01\n",
      "POS Tag: n\n",
      "Definition: a long walk usually for exercise or pleasure\n",
      "Examples: ['she enjoys a hike in her spare time']\n",
      "\n",
      "Synset Name: rise.n.09\n",
      "POS Tag: n\n",
      "Definition: an increase in cost\n",
      "Examples: ['they asked for a 10% rise in rates']\n",
      "\n",
      "Synset Name: raise.n.01\n",
      "POS Tag: n\n",
      "Definition: the amount a salary is increased\n",
      "Examples: ['he got a 3% raise', 'he got a wage hike']\n",
      "\n",
      "Synset Name: hike.v.01\n",
      "POS Tag: v\n",
      "Definition: increase\n",
      "Examples: ['The landlord hiked up the rents']\n",
      "\n",
      "Synset Name: hike.v.02\n",
      "POS Tag: v\n",
      "Definition: walk a long way, as for pleasure or physical exercise\n",
      "Examples: ['We were hiking in Colorado', 'hike the Rockies']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get details for each synonym in synset\n",
    "for synset in word_synsets:\n",
    "    print(('Synset Name: {name}\\n'\n",
    "           'POS Tag: {tag}\\n'\n",
    "           'Definition: {defn}\\n'\n",
    "           'Examples: {ex}\\n').format(name=synset.name(),\n",
    "                                      tag=synset.pos(),\n",
    "                                      defn=synset.definition(),\n",
    "                                      ex=synset.examples()))\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
