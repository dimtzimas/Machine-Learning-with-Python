{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading some necessary dependencies and a corpus on which we will be computing collocations\n",
    "from nltk.corpus import gutenberg\n",
    "# import text_normalizer as tn\n",
    "import text_normalizer_el as tn\n",
    "import nltk\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-448877fbd925>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;31m# norm_alice = list(filter(None, tn.normalize_corpus(alice, text_lemmatization=False)))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# norm_alice = tn.normalize_corpus(alice, text_lemmatization=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mnorm_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext_lemmatization\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspecial_char_removal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[0mnorm_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\text-analytics-w-python-2e-master - 2020\\Ch06 - Text Summarization and Topic Models\\text_normalizer_el.py\u001b[0m in \u001b[0;36mnormalize_corpus\u001b[1;34m(corpus, html_stripping, contraction_expansion, accented_char_removal, text_lower_case, correct_spellings, text_stemming, text_lemmatization, special_char_removal, remove_digits, stopword_removal, stopwords)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;31m# remove extra whitespace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' +'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m          \u001b[1;31m# lowercase the text\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\re.py\u001b[0m in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[1;32m--> 192\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# load corpus\n",
    "# with open('temp.txt', 'w', encoding='utf-8') as fh:\n",
    "data = open('demo.txt', 'r+', encoding='utf-8').readlines()\n",
    "sentences = nltk.sent_tokenize(data[0])\n",
    "len(sentences)\n",
    "alice = sentences\n",
    "# alice = open('demo.txt')\n",
    "# alice.readlines()\n",
    "# alice = [' '.join(ts) for ts in alice]\n",
    "# alice = \"\"\"\n",
    "# Η Βρετανική Αυτοκρατορία (αγγλικά: British Empire) αποτελείτο από τις κτήσεις, τις αποικίες, τα προτεκτοράτα, τις εντολές και άλλα εδάφη που κυβερνήθηκαν ή διοικήθηκαν από το Ηνωμένο Βασίλειο και τα οποία προήλθαν από υπερπόντιες αποικίες και εμπορικούς σταθμούς ιδρυμένους από την Αγγλία στον ύστερο 16ο και πρώιμο 17ο αιώνα[1]. Κατά την ακμή της ήταν η μεγαλύτερη αυτοκρατορία στην ιστορία και για πάνω από ένα αιώνα ήταν η μεγαλύτερη παγκόσμια δύναμη. Το 1922 η Βρετανική Αυτοκρατορία διοικούσε έναν πληθυσμό περίπου 458 εκατομμυρίων, το ένα τέταρτο του παγκόσμιου πληθυσμού,[2] και κάλυπτε περισσότερο από 33.670.000 τετραγωνικά χιλιόμετρα, το ένα τέταρτο περίπου της συνολικής έκτασης της ξηράς της Γης.[3] Αυτό είχε ως αποτέλεσμα την έντονη πολιτική, γλωσσική και πολιτισμική της επίδραση παγκοσμίως. Στο ύψιστο σημείο της δύναμής της, συχνά λεγόταν ότι «ο ήλιος δεν δύει ποτέ στην Βρετανική Αυτοκρατορία», επειδή η εξάπλωσή της σε όλα σχεδόν τα γεωγραφικά μήκη του κόσμου σήμαινε ότι ο ήλιος πάντα έλαμπε σε τουλάχιστον ένα από τα πολυάριθμα εδάφη της.\n",
    "# Κατά την Εποχή των ανακαλύψεων το 15ο και 16ο αιώνα, η Πορτογαλία και η Ισπανία πρωτοστάτησαν στην ευρωπαϊκή εξερεύνηση του πλανήτη και εγκαθίδρυσαν τεράστιες πολυπληθείς αυτοκρατορίες. Εποφθαλμιώντας τον πλούτο που προσέφεραν αυτές οι αυτοκρατορίες, η Αγγλία, η Γαλλία και η Ολλανδία άρχισαν και αυτές να ιδρύουν αποικίες και εμπορικά δίκτυα στην Αμερική και την Ασία.[4] Μία σειρά πολέμων το 17ο και 18ο αιώνα ενάντια στην Ολλανδία και τη Γαλλία κατέστησε την Αγγλία (Βρετανία μετά την πράξη ένωσης με τη Σκωτία το 1707) κυρίαρχη αποικιακή δύναμη στη Βόρεια Αμερική και την Ινδία. Η απώλεια όμως των δεκατριών αποικιών στη Βόρεια Αμερική το 1783 μετά από την Αμερικανική Επανάσταση ήταν βαρύ πλήγμα για τη Βρετανία, καθώς της στέρησε τις πιο πολυπληθείς αποικίες της. Παρά το γεγονός αυτό, το βρετανικό ενδιαφέρον σύντομα στράφηκε προς την Αφρική, την Ασία και τον Ειρηνικό. Μετά την ήττα της Ναπολεόντειας Γαλλίας το 1815, η Βρετανία γνώρισε έναν αιώνα αδιαμφισβήτητης κυριαρχίας και επεξέτεινε την κυριαρχία της σε όλο τον κόσμο. Δόθηκε σταδιακή αυτονομία στις αποικίες με λευκό πληθυσμό, κάποιες από τις οποίες μετατράπηκαν σε κτήσεις.\n",
    "# Η ανάπτυξη της Γερμανίας και των ΗΠΑ περιόρισε την οικονομική κυριαρχία της Βρετανίας στο τέλος του 19ου αιώνα. Οι επακόλουθες στρατιωτικές και οικονομικές εντάσεις ανάμεσα στη Βρετανία και τη Γερμανία ήταν τα κύρια αίτια του Α΄ Παγκοσμίου Πολέμου, κατά τη διάρκεια του οποίου η Βρετανία στηρίχθηκε σε μεγάλο βαθμό στην αυτοκρατορία της. Ο πόλεμος επέφερε τεράστια οικονομική επιβάρυνση στη Βρετανία και παρόλο που η αυτοκρατορία πέτυχε τη μεγαλύτερη εδαφική επέκταση της αμέσως μετά τον πόλεμο, δεν ήταν πια μια απαράμιλλη βιομηχανική ή στρατιωτική δύναμη. Κατά το Β΄ Παγκόσμιο Πόλεμο, οι βρετανικές αποικίες της Νοτιοανατολικής Ασίας καταλήφθηκαν από την Ιαπωνία, γεγονός που έπληξε το γόητρό της και επιτάχυνε την παρακμή της αυτοκρατορίας παρά τη νίκη της στον πόλεμο. Δυο χρόνια μετά τον πόλεμο η Βρετανία παραχώρησε ανεξαρτησία στην Ινδία, την πολυπληθέστερη και πολυτιμότερη αποικία της.\n",
    "# \"\"\"\n",
    "# handy text_normalizer module\n",
    "# norm_alice = list(filter(None, tn.normalize_corpus(alice, text_lemmatization=False)))\n",
    "# norm_alice = tn.normalize_corpus(alice, text_lemmatization=False)\n",
    "norm_sentences = tn.normalize_corpus(sentences, text_lemmatization=False, special_char_removal=False)\n",
    "norm_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Alice ' s Adventures in Wonderland by Lewis Carroll 1865 ] \n",
      " alice adventures wonderland lewis carroll\n"
     ]
    }
   ],
   "source": [
    "print(alice[0], '\\n', norm_alice[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collocations\n",
    "Collocation can be defined as a sequence or group of words that tend to occur frequently and this frequency tends to be more than what could be termed a random or chance occurrence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute n-grams\n",
    "def compute_ngrams(sequence, n):\n",
    "    return list(\n",
    "            zip(*(sequence[index:] \n",
    "                     for index in range(n)))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (2, 3), (3, 4)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ngrams([1,2,3,4], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (2, 3, 4)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_ngrams([1,2,3,4], 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the corpus into one big string of text\n",
    "def flatten_corpus(corpus):\n",
    "    return ' '.join([document.strip() \n",
    "                     for document in corpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the top n-grams from a corpus of text\n",
    "# We use NLTK’s FreqDist class to create a counter of all the n-grams based on their frequency and then we sort them based on\n",
    "# their frequency and return the top n-grams based on the specified user limit.\n",
    "def get_top_ngrams(corpus, ngram_val=1, limit=5):\n",
    "\n",
    "    corpus = flatten_corpus(corpus)\n",
    "    tokens = nltk.word_tokenize(corpus)\n",
    "\n",
    "    ngrams = compute_ngrams(tokens, ngram_val)\n",
    "    ngrams_freq_dist = nltk.FreqDist(ngrams)\n",
    "    sorted_ngrams_fd = sorted(ngrams_freq_dist.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    sorted_ngrams = sorted_ngrams_fd[0:limit]\n",
    "    sorted_ngrams = [(' '.join(text), freq) \n",
    "                     for text, freq in sorted_ngrams]\n",
    "\n",
    "    return sorted_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said alice', 123),\n",
       " ('mock turtle', 56),\n",
       " ('march hare', 31),\n",
       " ('said king', 29),\n",
       " ('thought alice', 26),\n",
       " ('white rabbit', 22),\n",
       " ('said hatter', 22),\n",
       " ('said mock', 20),\n",
       " ('said caterpillar', 18),\n",
       " ('said gryphon', 18)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the top 10 bi-grams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=2,\n",
    "               limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said mock turtle', 20),\n",
       " ('said march hare', 10),\n",
       " ('poor little thing', 6),\n",
       " ('little golden key', 5),\n",
       " ('certainly said alice', 5),\n",
       " ('white kid gloves', 5),\n",
       " ('march hare said', 5),\n",
       " ('mock turtle said', 5),\n",
       " ('know said alice', 4),\n",
       " ('might well say', 4)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute the top 10 tri-grams\n",
    "get_top_ngrams(corpus=norm_alice, ngram_val=3,\n",
    "               limit=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams and Pointwise Mututal Information\n",
    "We now look at NLTK’s collocation finders, which enable us to find collocations using various measures like raw frequencies, pointwise mutual information, and so on. Just to explain briefly, pointwise mutual information can be computed for two events or terms as the logarithm of the ratio of the probability of them occurring together by the product of their individual probabilities, assuming that they are independent of each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.collocations.BigramCollocationFinder at 0x1ce904a3688>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bigrams\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.collocations import BigramAssocMeasures\n",
    "\n",
    "finder = BigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])\n",
    "finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'alice'),\n",
       " ('mock', 'turtle'),\n",
       " ('march', 'hare'),\n",
       " ('said', 'king'),\n",
       " ('thought', 'alice'),\n",
       " ('said', 'hatter'),\n",
       " ('white', 'rabbit'),\n",
       " ('said', 'mock'),\n",
       " ('said', 'caterpillar'),\n",
       " ('said', 'gryphon')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw frequencies\n",
    "bigram_measures = BigramAssocMeasures()                                                \n",
    "finder.nbest(bigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('abide', 'figures'),\n",
       " ('acceptance', 'elegant'),\n",
       " ('accounting', 'tastes'),\n",
       " ('accustomed', 'usurpation'),\n",
       " ('act', 'crawling'),\n",
       " ('adjourn', 'immediate'),\n",
       " ('adoption', 'energetic'),\n",
       " ('affair', 'trusts'),\n",
       " ('agony', 'terror'),\n",
       " ('alarmed', 'proposal')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pointwise mutual information\n",
    "finder.nbest(bigram_measures.pmi, 10)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.collocations import TrigramAssocMeasures\n",
    "\n",
    "finder = TrigramCollocationFinder.from_documents([item.split() \n",
    "                                                for item \n",
    "                                                in norm_alice])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('said', 'mock', 'turtle'),\n",
       " ('said', 'march', 'hare'),\n",
       " ('poor', 'little', 'thing'),\n",
       " ('little', 'golden', 'key'),\n",
       " ('march', 'hare', 'said'),\n",
       " ('mock', 'turtle', 'said'),\n",
       " ('white', 'kid', 'gloves'),\n",
       " ('beau', 'ootiful', 'soo'),\n",
       " ('certainly', 'said', 'alice'),\n",
       " ('might', 'well', 'say')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_measures = TrigramAssocMeasures()\n",
    "# raw frequencies\n",
    "finder.nbest(trigram_measures.raw_freq, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('accustomed', 'usurpation', 'conquest'),\n",
       " ('adjourn', 'immediate', 'adoption'),\n",
       " ('adoption', 'energetic', 'remedies'),\n",
       " ('ancient', 'modern', 'seaography'),\n",
       " ('apple', 'roast', 'turkey'),\n",
       " ('arithmetic', 'ambition', 'distraction'),\n",
       " ('brother', 'latin', 'grammar'),\n",
       " ('canvas', 'bag', 'tied'),\n",
       " ('cherry', 'tart', 'custard'),\n",
       " ('circle', 'exact', 'shape')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pointwise mutual information\n",
    "finder.nbest(trigram_measures.pmi, 10)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you know how to compute collocations for a corpus using an n-gram generative\n",
    "approach. We look at a better way of generating keyphrases based on parts of speech\n",
    "(PoS) tagging and term weighing in the next section.\n",
    "# Weighted Tag-based Phrase Extraction\n",
    "We follow a two-step process in our algorithm, as follows:\n",
    "1. Extract all noun phrase chunks using shallow parsing.\n",
    "2. Compute TF-IDF weights for each chunk and return the top weighted phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# corpus taken from Wikipedia\n",
    "# data = open('elephants.txt', 'r+').readlines()\n",
    "data = open('demo.txt', 'r+', encoding='utf-8').readlines()\n",
    "sentences = nltk.sent_tokenize(data[0])\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Η Βρετανική Αυτοκρατορία (αγγλικά: British Empire) αποτελείτο από τις κτήσεις, τις αποικίες, τα προτεκτοράτα, τις εντολές και άλλα εδάφη που κυβερνήθηκαν ή διοικήθηκαν από το Ηνωμένο Βασίλειο και τα οποία προήλθαν από υπερπόντιες αποικίες και εμπορικούς σταθμούς ιδρυμένους από την Αγγλία στον ύστερο 16ο και πρώιμο 17ο αιώνα[1].',\n",
       " 'Κατά την ακμή της ήταν η μεγαλύτερη αυτοκρατορία στην ιστορία και για πάνω από ένα αιώνα ήταν η μεγαλύτερη παγκόσμια δύναμη.',\n",
       " 'Το 1922 η Βρετανική Αυτοκρατορία διοικούσε έναν πληθυσμό περίπου 458 εκατομμυρίων, το ένα τέταρτο του παγκόσμιου πληθυσμού,[2] και κάλυπτε περισσότερο από 33.670.000 τετραγωνικά χιλιόμετρα, το ένα τέταρτο περίπου της συνολικής έκτασης της ξηράς της Γης.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# viewing the first three lines\n",
    "sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-50eb9d35b3ac>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m norm_sentences = tn.normalize_corpus(sentences, text_lower_case=False, \n\u001b[1;32m----> 2\u001b[1;33m                                      text_stemming=False, text_lemmatization=False, stopword_removal=False)\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mnorm_sentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\text-analytics-w-python-2e-master - 2020\\Ch06 - Text Summarization and Topic Models\\text_normalizer_el.py\u001b[0m in \u001b[0;36mnormalize_corpus\u001b[1;34m(corpus, html_stripping, contraction_expansion, accented_char_removal, text_lower_case, correct_spellings, text_stemming, text_lemmatization, special_char_removal, remove_digits, stopword_removal, stopwords)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[1;31m# remove special characters and\\or digits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mspecial_char_removal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m             \u001b[1;31m# insert spaces between special characters to isolate them\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m             \u001b[0mspecial_char_pattern\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'([{.(-)!}])'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspecial_char_pattern\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \\\\1 \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "norm_sentences = tn.normalize_corpus(sentences, text_lower_case=False, \n",
    "                                     text_stemming=False, text_lemmatization=False, stopword_removal=False)\n",
    "norm_sentences[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# use the pattern \" NP: {<DT>? <JJ>* <NN.*>+}\" to extract all possible noun phrases from our corpus of documents/sentences\n",
    "\n",
    "def get_chunks(sentences, grammar = r'NP: {<DT>? <JJ>* <NN.*>+}', stopword_list=stopwords):\n",
    "    \n",
    "    all_chunks = []\n",
    "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        \n",
    "        tagged_sents = [nltk.pos_tag(nltk.word_tokenize(sentence))]   \n",
    "        \n",
    "        chunks = [chunker.parse(tagged_sent) \n",
    "                      for tagged_sent in tagged_sents]\n",
    "        \n",
    "        wtc_sents = [nltk.chunk.tree2conlltags(chunk)\n",
    "                         for chunk in chunks]    \n",
    "        \n",
    "        flattened_chunks = list(\n",
    "                            itertools.chain.from_iterable(\n",
    "                                wtc_sent for wtc_sent in wtc_sents)\n",
    "                           )\n",
    "        \n",
    "        valid_chunks_tagged = [(status, [wtc for wtc in chunk]) \n",
    "                                   for status, chunk \n",
    "                                       in itertools.groupby(flattened_chunks, \n",
    "                                                lambda word_pos_chunk: word_pos_chunk[2] != 'O')]\n",
    "        \n",
    "        valid_chunks = [' '.join(word.lower() \n",
    "                                for word, tag, chunk in wtc_group \n",
    "                                    if word.lower() not in stopword_list) \n",
    "                                        for status, wtc_group in valid_chunks_tagged\n",
    "                                            if status]\n",
    "                                            \n",
    "        all_chunks.append(valid_chunks)\n",
    "    \n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['elephants', 'large mammals', 'family elephantidae', 'order proboscidea'],\n",
       " ['species',\n",
       "  'african bush elephant loxodonta',\n",
       "  'african forest elephant l cyclotis',\n",
       "  'asian elephant elephas maximus'],\n",
       " ['elephants', 'subsaharan africa south asia', 'southeast asia'],\n",
       " ['elephantidae',\n",
       "  'family',\n",
       "  'order proboscidea',\n",
       "  'extinct members',\n",
       "  'order',\n",
       "  'deinotheres gomphotheres mammoths',\n",
       "  'mastodons'],\n",
       " ['elephants',\n",
       "  'several distinctive features',\n",
       "  'long trunk',\n",
       "  'proboscis',\n",
       "  'many purposes',\n",
       "  'water',\n",
       "  'grasping objects'],\n",
       " ['incisors', 'tusks', 'weapons', 'tools', 'objects'],\n",
       " ['elephants', 'flaps', 'body temperature'],\n",
       " ['pillarlike legs', 'great weight'],\n",
       " ['african elephants',\n",
       "  'ears',\n",
       "  'backs',\n",
       "  'asian elephants',\n",
       "  'ears',\n",
       "  'convex',\n",
       "  'level backs'],\n",
       " ['elephants', 'different habitats', 'savannahs forests deserts', 'marshes'],\n",
       " ['water'],\n",
       " ['keystone species', 'impact', 'environments'],\n",
       " ['animals',\n",
       "  'distance',\n",
       "  'elephants',\n",
       "  'predators',\n",
       "  'lions tigers hyenas',\n",
       "  'wild dogs',\n",
       "  'young elephants',\n",
       "  'calves'],\n",
       " ['elephants', 'fissionfusion society', 'multiple family groups'],\n",
       " ['females cows',\n",
       "  'family groups',\n",
       "  'female',\n",
       "  'calves',\n",
       "  'several related females'],\n",
       " ['groups', 'individual known', 'matriarch', 'cow'],\n",
       " ['males bulls', 'family groups', 'males'],\n",
       " ['adult',\n",
       "  'family groups',\n",
       "  'mate',\n",
       "  'enter state',\n",
       "  'increased testosterone',\n",
       "  'aggression',\n",
       "  'musth',\n",
       "  'dominance',\n",
       "  'reproductive success'],\n",
       " ['calves', 'centre', 'attention', 'family groups', 'mothers', 'years'],\n",
       " ['elephants', 'years', 'wild'],\n",
       " ['touch sight smell',\n",
       "  'sound elephants',\n",
       "  'infrasound',\n",
       "  'seismic communication',\n",
       "  'long distances'],\n",
       " ['elephant intelligence', 'primates', 'cetaceans'],\n",
       " ['selfawareness', 'dead individuals', 'kind'],\n",
       " ['african elephants',\n",
       "  'international union',\n",
       "  'conservation',\n",
       "  'nature iucn',\n",
       "  'asian elephant'],\n",
       " ['threats', 'populations', 'ivory trade', 'animals', 'ivory tusks'],\n",
       " ['threats', 'elephants', 'habitat destruction', 'conflicts', 'local people'],\n",
       " ['elephants', 'animals', 'asia'],\n",
       " ['past', 'war today', 'display', 'zoos', 'entertainment', 'circuses'],\n",
       " ['elephants', 'art folklore religion literature', 'popular culture']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the function in action\n",
    "# This output shows us all the valid keyphrases per sentence of our document.\n",
    "chunks = get_chunks(norm_sentences)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2, where we will build a TF-IDF based model on our keyphrases using Gensim\n",
    "# then compute TF-IDF based weights for each keyphrase ... sort these keyphrases\n",
    "# top N keyphrases, where top_n is specified by the user\n",
    "from gensim import corpora, models\n",
    "\n",
    "def get_tfidf_weighted_keyphrases(sentences, \n",
    "                                  grammar=r'NP: {<DT>? <JJ>* <NN.*>+}',\n",
    "                                  top_n=10):\n",
    "    \n",
    "    valid_chunks = get_chunks(sentences, grammar=grammar)\n",
    "                                     \n",
    "    dictionary = corpora.Dictionary(valid_chunks)\n",
    "    corpus = [dictionary.doc2bow(chunk) for chunk in valid_chunks]\n",
    "    \n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    \n",
    "    weighted_phrases = {dictionary.get(idx): value \n",
    "                           for doc in corpus_tfidf \n",
    "                               for idx, value in doc}\n",
    "                            \n",
    "    weighted_phrases = sorted(weighted_phrases.items(), \n",
    "                              key=itemgetter(1), reverse=True)\n",
    "    weighted_phrases = [(term, round(wt, 3)) for term, wt in weighted_phrases]\n",
    "    \n",
    "    return weighted_phrases[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('water', 1.0),\n",
       " ('asia', 0.807),\n",
       " ('wild', 0.764),\n",
       " ('great weight', 0.707),\n",
       " ('pillarlike legs', 0.707),\n",
       " ('southeast asia', 0.693),\n",
       " ('subsaharan africa south asia', 0.693),\n",
       " ('body temperature', 0.693),\n",
       " ('flaps', 0.693),\n",
       " ('fissionfusion society', 0.693),\n",
       " ('multiple family groups', 0.693),\n",
       " ('art folklore religion literature', 0.693),\n",
       " ('popular culture', 0.693),\n",
       " ('ears', 0.681),\n",
       " ('males', 0.653),\n",
       " ('males bulls', 0.653),\n",
       " ('family elephantidae', 0.607),\n",
       " ('large mammals', 0.607),\n",
       " ('years', 0.607),\n",
       " ('environments', 0.577),\n",
       " ('impact', 0.577),\n",
       " ('keystone species', 0.577),\n",
       " ('cetaceans', 0.577),\n",
       " ('elephant intelligence', 0.577),\n",
       " ('primates', 0.577),\n",
       " ('dead individuals', 0.577),\n",
       " ('kind', 0.577),\n",
       " ('selfawareness', 0.577),\n",
       " ('different habitats', 0.57),\n",
       " ('marshes', 0.57)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now test this function on our toy corpus by using the following code snippet\n",
    "# to generate the top 30 keyphrases\n",
    "get_tfidf_weighted_keyphrases(sentences=norm_sentences, top_n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('την', 0.322),\n",
       " ('ακμη της', 0.254),\n",
       " ('εντολες και', 0.248),\n",
       " ('στην', 0.177),\n",
       " ('απο τις', 0.156),\n",
       " ('αυτοκρατορια', 0.114),\n",
       " ('αποικιες', 0.112),\n",
       " ('βρετανια', 0.105),\n",
       " ('τεταρτο του', 0.1),\n",
       " ('ηταν', 0.099),\n",
       " ('μετα', 0.089),\n",
       " ('στη', 0.084),\n",
       " ('εδαφη που', 0.082),\n",
       " ('αιωνα', 0.079),\n",
       " ('δυναμη', 0.077),\n",
       " ('πολεμο', 0.073),\n",
       " ('εποφθαλμιωντας τον', 0.071),\n",
       " ('μια', 0.067),\n",
       " ('κατα', 0.066),\n",
       " ('στο', 0.061),\n",
       " ('εποχη των', 0.061),\n",
       " ('πληθυσμο', 0.061),\n",
       " ('περιπου', 0.059),\n",
       " ('πολυπληθεις', 0.059),\n",
       " ('ενα', 0.058)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also leverage Gensim’s summarization module, which has a keywords\n",
    "# function that extracts keywords from the text. This uses a variation of the TextRank\n",
    "# algorithm, which we explore in the document summarization section\n",
    "data = \"\"\"\n",
    "Η Βρετανική Αυτοκρατορία (αγγλικά: British Empire) αποτελείτο από τις κτήσεις, τις αποικίες, τα προτεκτοράτα, τις εντολές και άλλα εδάφη που κυβερνήθηκαν ή διοικήθηκαν από το Ηνωμένο Βασίλειο και τα οποία προήλθαν από υπερπόντιες αποικίες και εμπορικούς σταθμούς ιδρυμένους από την Αγγλία στον ύστερο 16ο και πρώιμο 17ο αιώνα[1]. Κατά την ακμή της ήταν η μεγαλύτερη αυτοκρατορία στην ιστορία και για πάνω από ένα αιώνα ήταν η μεγαλύτερη παγκόσμια δύναμη. Το 1922 η Βρετανική Αυτοκρατορία διοικούσε έναν πληθυσμό περίπου 458 εκατομμυρίων, το ένα τέταρτο του παγκόσμιου πληθυσμού,[2] και κάλυπτε περισσότερο από 33.670.000 τετραγωνικά χιλιόμετρα, το ένα τέταρτο περίπου της συνολικής έκτασης της ξηράς της Γης.[3] Αυτό είχε ως αποτέλεσμα την έντονη πολιτική, γλωσσική και πολιτισμική της επίδραση παγκοσμίως. Στο ύψιστο σημείο της δύναμής της, συχνά λεγόταν ότι «ο ήλιος δεν δύει ποτέ στην Βρετανική Αυτοκρατορία», επειδή η εξάπλωσή της σε όλα σχεδόν τα γεωγραφικά μήκη του κόσμου σήμαινε ότι ο ήλιος πάντα έλαμπε σε τουλάχιστον ένα από τα πολυάριθμα εδάφη της.\n",
    "Κατά την Εποχή των ανακαλύψεων το 15ο και 16ο αιώνα, η Πορτογαλία και η Ισπανία πρωτοστάτησαν στην ευρωπαϊκή εξερεύνηση του πλανήτη και εγκαθίδρυσαν τεράστιες πολυπληθείς αυτοκρατορίες. Εποφθαλμιώντας τον πλούτο που προσέφεραν αυτές οι αυτοκρατορίες, η Αγγλία, η Γαλλία και η Ολλανδία άρχισαν και αυτές να ιδρύουν αποικίες και εμπορικά δίκτυα στην Αμερική και την Ασία.[4] Μία σειρά πολέμων το 17ο και 18ο αιώνα ενάντια στην Ολλανδία και τη Γαλλία κατέστησε την Αγγλία (Βρετανία μετά την πράξη ένωσης με τη Σκωτία το 1707) κυρίαρχη αποικιακή δύναμη στη Βόρεια Αμερική και την Ινδία. Η απώλεια όμως των δεκατριών αποικιών στη Βόρεια Αμερική το 1783 μετά από την Αμερικανική Επανάσταση ήταν βαρύ πλήγμα για τη Βρετανία, καθώς της στέρησε τις πιο πολυπληθείς αποικίες της. Παρά το γεγονός αυτό, το βρετανικό ενδιαφέρον σύντομα στράφηκε προς την Αφρική, την Ασία και τον Ειρηνικό. Μετά την ήττα της Ναπολεόντειας Γαλλίας το 1815, η Βρετανία γνώρισε έναν αιώνα αδιαμφισβήτητης κυριαρχίας και επεξέτεινε την κυριαρχία της σε όλο τον κόσμο. Δόθηκε σταδιακή αυτονομία στις αποικίες με λευκό πληθυσμό, κάποιες από τις οποίες μετατράπηκαν σε κτήσεις.\n",
    "Η ανάπτυξη της Γερμανίας και των ΗΠΑ περιόρισε την οικονομική κυριαρχία της Βρετανίας στο τέλος του 19ου αιώνα. Οι επακόλουθες στρατιωτικές και οικονομικές εντάσεις ανάμεσα στη Βρετανία και τη Γερμανία ήταν τα κύρια αίτια του Α΄ Παγκοσμίου Πολέμου, κατά τη διάρκεια του οποίου η Βρετανία στηρίχθηκε σε μεγάλο βαθμό στην αυτοκρατορία της. Ο πόλεμος επέφερε τεράστια οικονομική επιβάρυνση στη Βρετανία και παρόλο που η αυτοκρατορία πέτυχε τη μεγαλύτερη εδαφική επέκταση της αμέσως μετά τον πόλεμο, δεν ήταν πια μια απαράμιλλη βιομηχανική ή στρατιωτική δύναμη. Κατά το Β΄ Παγκόσμιο Πόλεμο, οι βρετανικές αποικίες της Νοτιοανατολικής Ασίας καταλήφθηκαν από την Ιαπωνία, γεγονός που έπληξε το γόητρό της και επιτάχυνε την παρακμή της αυτοκρατορίας παρά τη νίκη της στον πόλεμο. Δυο χρόνια μετά τον πόλεμο η Βρετανία παραχώρησε ανεξαρτησία στην Ινδία, την πολυπληθέστερη και πολυτιμότερη αποικία της.\n",
    "\"\"\"\n",
    "from gensim.summarization import keywords\n",
    "key_words = keywords(data, ratio=1.0, scores=True, lemmatize=True)\n",
    "[(item, round(score, 3)) for item, score in key_words][:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thus, you can see how keyphrase extraction can extract key important concepts from text documents and summarize them"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
