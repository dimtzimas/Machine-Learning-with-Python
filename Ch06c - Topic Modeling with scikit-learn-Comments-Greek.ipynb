{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Pre-process Data\n",
    "The flexibility in tuning or controlling these models is slightly limited as compared to Gensim!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0009.txt', '0174.txt', '0184.txt', '0192.txt', '0211.txt', '0223.txt', '0249.txt', '0278.txt', '0290.txt', '0317.txt', '0387.txt', '0422.txt', '0457.txt', '0495.txt', '0642.txt', '0652.txt', '0683.txt', '0693.txt', '0715.txt', '0750.txt', '0830.txt', '0840.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = 'nipstxt1'\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# folders = [\"nips{0:02}\".format(i) for i in range(0,13)]\n",
    "\n",
    "# Read all texts into a list.\n",
    "# Each paper is in its own text file, hence we need to use file-reading functions from Python.\n",
    "\n",
    "papers = []\n",
    "# for folder in folders:\n",
    "file_names = os.listdir(DATA_PATH)\n",
    "for file_name in file_names:\n",
    "    with open(DATA_PATH + '/' + file_name, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "    papers.append(data)\n",
    "len(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 \n",
      "Stochastic Learning Networks and their Electronic Implementation \n",
      "Joshua Alspector*, Robert B. Allen, Victor Hut, and Srinagesh Satyanarayana \n",
      "Bell Communications Research, Morristown, NJ 07960 \n",
      "We describe a family of learning algorithms that operate on a recurrent, symmetrically \n",
      "connected, neuromorphic network that, like the Boltzmann machine, settles in the \n",
      "presence of noise. These networks learn by modifying synaptic connection strengths on \n",
      "the basis of correlations seen locally by each synapse. We describe a version of the \n",
      "supervised learning algorilhm for a network with analog activation functions. We also \n",
      "demonstrate unsupervised competitive learning with this approach, where weight \n",
      "saturation and decay play an important role, and describe preliminary experiments in \n",
      "reinforcement !earning, where noise is used in the search procedure. We identify the \n",
      "above described phenomena as elements that can unify learning techniques at a physical \n",
      "microscopic level. \n",
      "These algor\n"
     ]
    }
   ],
   "source": [
    "print(papers[0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Text Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Wall time: 4.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import nltk\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "wtk = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "wnl = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def normalize_corpus(papers):\n",
    "    norm_papers = []\n",
    "    for paper in papers:\n",
    "        paper = paper.lower()\n",
    "        paper_tokens = [token.strip() for token in wtk.tokenize(paper)]\n",
    "        paper_tokens = [wnl.lemmatize(token) for token in paper_tokens if not token.isnumeric()]\n",
    "        paper_tokens = [token for token in paper_tokens if len(token) > 1]\n",
    "        paper_tokens = [token for token in paper_tokens if token not in stop_words]\n",
    "        paper_tokens = list(filter(None, paper_tokens))\n",
    "        if paper_tokens:\n",
    "            norm_papers.append(paper_tokens)\n",
    "            \n",
    "    return norm_papers\n",
    "    \n",
    "norm_papers = normalize_corpus(papers)\n",
    "print(len(norm_papers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Representation with Feature Engineering\n",
    "We represent our text data in the form of a Bag of Words model with uni-grams and bi-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22, 4274)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(min_df=2, max_df=0.6, ngram_range=(1,2),\n",
    "                     token_pattern=None, tokenizer=lambda doc: doc,\n",
    "                     preprocessor=lambda doc: doc)\n",
    "cv_features = cv.fit_transform(norm_papers)\n",
    "cv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Vocabulary Size: 4274\n"
     ]
    }
   ],
   "source": [
    "vocabulary = np.array(cv.get_feature_names())\n",
    "print('Total Vocabulary Size:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models with Latent Semantic Indexing (LSI)\n",
    "based on SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.87 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "TOTAL_TOPICS = 20\n",
    "\n",
    "lsi_model = TruncatedSVD(n_components=TOTAL_TOPICS, n_iter=500, random_state=42)\n",
    "document_topics = lsi_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 4274)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_terms = lsi_model.components_\n",
    "topic_terms.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "==================================================\n",
      "Direction 1: [('cell', 0.638), ('firing', 0.238), ('lot', 0.19), ('learning', 0.154), ('synapsis', 0.146), ('probability', 0.141), ('axon', 0.129), ('rule', 0.129), ('cell firing', 0.107), ('ii', 0.106), ('inhibitory', 0.104), ('category', 0.1), ('cortex', 0.1), ('activity', 0.099), ('active', 0.097), ('simulation', 0.092), ('burst', 0.091), ('pitiform', 0.088), ('spatial', 0.082), ('specific', 0.075)]\n",
      "--------------------------------------------------\n",
      "Direction 2: []\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #2:\n",
      "==================================================\n",
      "Direction 1: [('classifier', 0.344), ('region', 0.216), ('fig', 0.216), ('training', 0.206), ('decision', 0.204), ('node', 0.195), ('vector', 0.17), ('learning', 0.153), ('class', 0.135), ('reinforcement', 0.132), ('error', 0.127), ('net', 0.118), ('feature', 0.113), ('nat', 0.111), ('two layer', 0.103), ('back', 0.095), ('hopfield', 0.094), ('code', 0.092)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cell', -0.228), ('firing', -0.093)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #3:\n",
      "==================================================\n",
      "Direction 1: [('classifier', 0.378), ('region', 0.24), ('node', 0.197), ('decision', 0.185), ('cell', 0.128), ('two layer', 0.119), ('feature', 0.101), ('map', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('reinforcement', -0.261), ('learning', -0.259), ('nat', -0.216), ('element', -0.138), ('strategy', -0.136), ('cycle', -0.135), ('adaptive', -0.119), ('algorithm', -0.108), ('recurrence', -0.098), ('noise', -0.094), ('action', -0.091), ('failure', -0.089)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #4:\n",
      "==================================================\n",
      "Direction 1: [('vector', 0.453), ('image', 0.226), ('object', 0.2), ('associative', 0.188), ('associative memory', 0.185), ('matrix', 0.177), ('hopfield', 0.16), ('capacity', 0.16), ('code', 0.151), ('recall', 0.13), ('stored', 0.095)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('reinforcement', -0.171), ('nat', -0.151), ('training', -0.149), ('learning', -0.141), ('decision', -0.118), ('region', -0.114), ('node', -0.095), ('strategy', -0.086), ('classifier', -0.083)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #5:\n",
      "==================================================\n",
      "Direction 1: [('nat', 0.336), ('reinforcement', 0.217), ('strategy', 0.189), ('net', 0.147), ('probability', 0.134), ('training', 0.111), ('play', 0.104), ('adaptive', 0.101), ('produce', 0.092)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('learning', -0.359), ('recurrence', -0.205), ('failure', -0.183), ('algorithm', -0.176), ('action', -0.162), ('cycle', -0.139), ('noise', -0.119), ('search', -0.107), ('last', -0.096), ('control', -0.096), ('controller', -0.087)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #6:\n",
      "==================================================\n",
      "Direction 1: [('symbol', 0.398), ('capacity', 0.219), ('code', 0.172), ('hopfield', 0.166), ('scheme', 0.161), ('size', 0.138), ('bound', 0.107), ('binary', 0.1), ('tank', 0.094), ('field', 0.093), ('stored', 0.088), ('coded', 0.088), ('boltzmann', 0.087), ('energy', 0.086)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('image', -0.3), ('object', -0.266), ('vector', -0.204), ('recall', -0.125), ('complex', -0.112), ('log', -0.085)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #7:\n",
      "==================================================\n",
      "Direction 1: [('energy', 0.234), ('tank', 0.21), ('hopfield', 0.207), ('field', 0.205), ('boltzmann', 0.193), ('minimum', 0.164), ('cortical', 0.143), ('boltzmann machine', 0.137), ('average', 0.13), ('potential', 0.106), ('mean field', 0.099), ('connectivity', 0.096), ('search', 0.093)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('symbol', -0.179), ('capacity', -0.149), ('recurrence', -0.143), ('failure', -0.135), ('cycle', -0.113), ('associative', -0.112), ('associative memory', -0.109)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #8:\n",
      "==================================================\n",
      "Direction 1: [('code', 0.292), ('class', 0.206), ('matrix', 0.194), ('hopfield', 0.177), ('exemplar', 0.163), ('vector', 0.163), ('error', 0.128), ('classifier', 0.123), ('connection matrix', 0.106)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('symbol', -0.348), ('image', -0.172), ('size', -0.143), ('capacity', -0.138), ('node', -0.128), ('object', -0.128), ('scheme', -0.127), ('associative memory', -0.108), ('representation', -0.095), ('training', -0.094), ('associative', -0.092)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #9:\n",
      "==================================================\n",
      "Direction 1: [('connectivity', 0.216), ('potential', 0.215), ('simulation', 0.211), ('cortical', 0.188), ('module', 0.161), ('action', 0.138), ('synaptic', 0.131), ('artificial', 0.125), ('associative', 0.118), ('architecture', 0.113), ('associative memory', 0.108), ('fine', 0.106), ('transmission', 0.103), ('cell', 0.103)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('noise', -0.141), ('boltzmann', -0.124), ('tank', -0.121), ('firing', -0.12), ('lot', -0.115), ('energy', -0.107)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #10:\n",
      "==================================================\n",
      "Direction 1: [('associative', 0.268), ('associative memory', 0.263), ('address', 0.193), ('capacity', 0.184), ('kanerva', 0.166), ('fig', 0.13), ('rate', 0.126), ('word', 0.106), ('sequence', 0.105), ('exponential', 0.091)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('symbol', -0.37), ('scheme', -0.15), ('image', -0.135), ('code', -0.127), ('object', -0.121), ('size', -0.117), ('cycle', -0.112), ('recurrence', -0.097), ('class', -0.093), ('simulation', -0.09)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #11:\n",
      "==================================================\n",
      "Direction 1: [('fig', 0.218), ('learning', 0.179), ('synapse', 0.174), ('noise', 0.173), ('level', 0.167), ('procedure', 0.127), ('control', 0.127), ('symbol', 0.121), ('code', 0.115)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cycle', -0.176), ('energy', -0.166), ('tank', -0.162), ('recurrence', -0.161), ('failure', -0.147), ('node', -0.126), ('hopfield', -0.118), ('algorithm', -0.116), ('boltzmann', -0.105), ('field', -0.104), ('temporal', -0.097)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #12:\n",
      "==================================================\n",
      "Direction 1: [('temporal', 0.243), ('code', 0.178), ('rate', 0.162), ('node', 0.148), ('training', 0.141), ('error', 0.125), ('motion', 0.121), ('computed', 0.118), ('velocity', 0.105), ('step', 0.095)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('vector', -0.151), ('capacity', -0.15), ('cortical', -0.139), ('cell', -0.138), ('region', -0.134), ('label', -0.128), ('back', -0.109), ('fig', -0.104), ('associative', -0.094), ('mean field', -0.093)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #13:\n",
      "==================================================\n",
      "Direction 1: [('control', 0.295), ('training', 0.206), ('element', 0.144), ('human', 0.141), ('level', 0.138), ('controller', 0.129), ('fig', 0.128), ('goal', 0.117), ('action', 0.113), ('brain', 0.113), ('command', 0.112), ('vector', 0.107), ('speed', 0.106), ('expert', 0.098), ('label', 0.085)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('noise', -0.211), ('synapse', -0.146), ('reinforcement', -0.141), ('address', -0.098), ('analog', -0.092)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #14:\n",
      "==================================================\n",
      "Direction 1: [('fig', 0.25), ('temporal', 0.244), ('node', 0.239), ('vector', 0.201), ('label', 0.174), ('motion', 0.132), ('computed', 0.132), ('rate', 0.127), ('velocity', 0.126), ('time step', 0.114), ('front', 0.114), ('step', 0.111), ('back', 0.109)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('associative memory', -0.133), ('associative', -0.118), ('object', -0.117), ('training', -0.114), ('classifier', -0.112), ('error', -0.102), ('image', -0.1)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #15:\n",
      "==================================================\n",
      "Direction 1: [('training', 0.251), ('cortical', 0.219), ('synapsis', 0.163), ('mean field', 0.154), ('eye', 0.146), ('activity', 0.142), ('mi', 0.138), ('cell', 0.132), ('speed', 0.121), ('expert', 0.117), ('human', 0.115), ('field', 0.109), ('error', 0.1), ('block', 0.1), ('command', 0.1)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('simulation', -0.155), ('potential', -0.148), ('level', -0.118), ('module', -0.112), ('tank', -0.105)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #16:\n",
      "==================================================\n",
      "Direction 1: [('level', 0.164), ('controller', 0.131), ('code', 0.117), ('cortical', 0.112), ('fig', 0.109), ('brain', 0.096), ('control', 0.093)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('training', -0.224), ('vector', -0.19), ('label', -0.156), ('speed', -0.116), ('potential', -0.116), ('architecture', -0.11), ('front', -0.106), ('capacity', -0.106), ('expert', -0.106), ('connectivity', -0.104), ('block', -0.101), ('rule', -0.097), ('back', -0.096)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #17:\n",
      "==================================================\n",
      "Direction 1: [('velocity', 0.285), ('line', 0.269), ('motion', 0.189), ('flow', 0.184), ('resistive', 0.184), ('analog', 0.158), ('optical', 0.153), ('cycle', 0.149), ('voltage', 0.142), ('circuit', 0.128), ('gradient', 0.128), ('location', 0.11), ('field', 0.11), ('edge', 0.092), ('image', 0.087)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('temporal', -0.183), ('rate', -0.135), ('step', -0.104), ('computed', -0.096), ('time step', -0.088)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #18:\n",
      "==================================================\n",
      "Direction 1: [('fiber', 0.612), ('afferent', 0.272), ('physiological', 0.219), ('nucleus', 0.207), ('anatomical', 0.164), ('gain', 0.161), ('organization', 0.154), ('individual', 0.143), ('primary', 0.133), ('thick', 0.121), ('individual neuron', 0.12), ('afferent fiber', 0.105), ('characteristic', 0.083), ('information processing', 0.076), ('head', 0.075), ('anatomical physiological', 0.075), ('innervate', 0.074), ('degree', 0.073), ('tend', 0.073)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('cortical', -0.086)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #19:\n",
      "==================================================\n",
      "Direction 1: [('transition', 0.239), ('probability', 0.223), ('fig', 0.217), ('array', 0.212), ('phase', 0.204), ('fire', 0.178), ('critical', 0.167), ('cycle', 0.14), ('cell', 0.13), ('activity', 0.122), ('firing', 0.12), ('infinite', 0.112), ('finite', 0.108), ('spontaneous', 0.107), ('rule', 0.101)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('synapsis', -0.178), ('lot', -0.14), ('learning', -0.112), ('axon', -0.097), ('cortical', -0.097)]\n",
      "--------------------------------------------------\n",
      "\n",
      "Topic #20:\n",
      "==================================================\n",
      "Direction 1: [('cycle', 0.541), ('connectivity', 0.213), ('joint', 0.196), ('grid', 0.136), ('path', 0.117), ('activation', 0.101), ('connected', 0.098), ('robot', 0.094), ('length', 0.093), ('program', 0.089), ('implemented', 0.083), ('interface', 0.081), ('strength', 0.08), ('list', 0.08), ('tool', 0.079), ('sensor', 0.077)]\n",
      "--------------------------------------------------\n",
      "Direction 2: [('recurrence', -0.14), ('failure', -0.122), ('potential', -0.081), ('velocity', -0.079)]\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can now generate the topics by reusing some of the code we implemented previously to display the topics and terms.\n",
    "top_terms = 20\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterm_weights = np.array([topic_terms[row, columns] \n",
    "                             for row, columns in list(zip(np.arange(TOTAL_TOPICS), topic_key_term_idxs))])\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topic_keyterms_weights = list(zip(topic_keyterms, topic_keyterm_weights))\n",
    "for n in range(TOTAL_TOPICS):\n",
    "    print('Topic #'+str(n+1)+':')\n",
    "    print('='*50)\n",
    "    d1 = []\n",
    "    d2 = []\n",
    "    terms, weights = topic_keyterms_weights[n]\n",
    "    term_weights = sorted([(t, w) for t, w in zip(terms, weights)], \n",
    "                          key=lambda row: -abs(row[1]))\n",
    "    for term, wt in term_weights:\n",
    "        if wt >= 0:\n",
    "            d1.append((term, round(wt, 3)))\n",
    "        else:\n",
    "            d2.append((term, round(wt, 3)))\n",
    "\n",
    "    print('Direction 1:', d1)\n",
    "    print('-'*50)\n",
    "    print('Direction 2:', d2)\n",
    "    print('-'*50)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>29.791</td>\n",
       "      <td>17.686</td>\n",
       "      <td>9.807</td>\n",
       "      <td>51.822</td>\n",
       "      <td>7.724</td>\n",
       "      <td>14.526</td>\n",
       "      <td>24.805</td>\n",
       "      <td>7.612</td>\n",
       "      <td>9.976</td>\n",
       "      <td>251.805</td>\n",
       "      <td>...</td>\n",
       "      <td>17.492</td>\n",
       "      <td>12.633</td>\n",
       "      <td>30.351</td>\n",
       "      <td>15.166</td>\n",
       "      <td>55.424</td>\n",
       "      <td>15.331</td>\n",
       "      <td>52.467</td>\n",
       "      <td>14.303</td>\n",
       "      <td>18.021</td>\n",
       "      <td>31.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>43.696</td>\n",
       "      <td>61.084</td>\n",
       "      <td>25.675</td>\n",
       "      <td>-2.635</td>\n",
       "      <td>11.083</td>\n",
       "      <td>24.055</td>\n",
       "      <td>29.458</td>\n",
       "      <td>14.513</td>\n",
       "      <td>10.711</td>\n",
       "      <td>-50.073</td>\n",
       "      <td>...</td>\n",
       "      <td>0.259</td>\n",
       "      <td>34.657</td>\n",
       "      <td>43.218</td>\n",
       "      <td>19.044</td>\n",
       "      <td>-8.296</td>\n",
       "      <td>30.755</td>\n",
       "      <td>0.701</td>\n",
       "      <td>30.218</td>\n",
       "      <td>38.219</td>\n",
       "      <td>63.782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>-43.476</td>\n",
       "      <td>16.552</td>\n",
       "      <td>-12.048</td>\n",
       "      <td>3.366</td>\n",
       "      <td>-5.766</td>\n",
       "      <td>-17.867</td>\n",
       "      <td>-23.585</td>\n",
       "      <td>-5.073</td>\n",
       "      <td>-10.172</td>\n",
       "      <td>17.012</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.130</td>\n",
       "      <td>-8.486</td>\n",
       "      <td>-55.902</td>\n",
       "      <td>-10.984</td>\n",
       "      <td>3.744</td>\n",
       "      <td>-8.736</td>\n",
       "      <td>0.651</td>\n",
       "      <td>3.686</td>\n",
       "      <td>-16.388</td>\n",
       "      <td>-82.838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4</th>\n",
       "      <td>-12.433</td>\n",
       "      <td>53.639</td>\n",
       "      <td>43.674</td>\n",
       "      <td>-1.269</td>\n",
       "      <td>9.454</td>\n",
       "      <td>12.544</td>\n",
       "      <td>-0.695</td>\n",
       "      <td>11.263</td>\n",
       "      <td>-1.428</td>\n",
       "      <td>-3.616</td>\n",
       "      <td>...</td>\n",
       "      <td>1.928</td>\n",
       "      <td>51.688</td>\n",
       "      <td>-25.317</td>\n",
       "      <td>22.918</td>\n",
       "      <td>9.265</td>\n",
       "      <td>-5.565</td>\n",
       "      <td>2.018</td>\n",
       "      <td>-1.911</td>\n",
       "      <td>82.910</td>\n",
       "      <td>-52.155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>-40.689</td>\n",
       "      <td>12.299</td>\n",
       "      <td>8.781</td>\n",
       "      <td>4.134</td>\n",
       "      <td>-0.394</td>\n",
       "      <td>-13.443</td>\n",
       "      <td>-22.172</td>\n",
       "      <td>-2.673</td>\n",
       "      <td>-12.388</td>\n",
       "      <td>5.462</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.430</td>\n",
       "      <td>10.616</td>\n",
       "      <td>-74.620</td>\n",
       "      <td>11.367</td>\n",
       "      <td>-0.162</td>\n",
       "      <td>-2.755</td>\n",
       "      <td>-4.685</td>\n",
       "      <td>-2.710</td>\n",
       "      <td>2.736</td>\n",
       "      <td>81.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T6</th>\n",
       "      <td>-0.835</td>\n",
       "      <td>28.373</td>\n",
       "      <td>30.370</td>\n",
       "      <td>-0.542</td>\n",
       "      <td>6.804</td>\n",
       "      <td>34.193</td>\n",
       "      <td>-5.071</td>\n",
       "      <td>8.780</td>\n",
       "      <td>-0.804</td>\n",
       "      <td>-3.105</td>\n",
       "      <td>...</td>\n",
       "      <td>0.582</td>\n",
       "      <td>-6.504</td>\n",
       "      <td>0.561</td>\n",
       "      <td>69.399</td>\n",
       "      <td>6.388</td>\n",
       "      <td>-8.211</td>\n",
       "      <td>2.024</td>\n",
       "      <td>4.220</td>\n",
       "      <td>-68.081</td>\n",
       "      <td>-9.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T7</th>\n",
       "      <td>21.931</td>\n",
       "      <td>-3.669</td>\n",
       "      <td>-29.226</td>\n",
       "      <td>6.422</td>\n",
       "      <td>8.731</td>\n",
       "      <td>66.567</td>\n",
       "      <td>1.444</td>\n",
       "      <td>2.008</td>\n",
       "      <td>-2.627</td>\n",
       "      <td>-16.384</td>\n",
       "      <td>...</td>\n",
       "      <td>5.958</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>-37.765</td>\n",
       "      <td>-26.778</td>\n",
       "      <td>34.892</td>\n",
       "      <td>2.303</td>\n",
       "      <td>34.098</td>\n",
       "      <td>3.333</td>\n",
       "      <td>-3.915</td>\n",
       "      <td>-0.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T8</th>\n",
       "      <td>-8.423</td>\n",
       "      <td>71.967</td>\n",
       "      <td>-18.137</td>\n",
       "      <td>-2.458</td>\n",
       "      <td>-2.587</td>\n",
       "      <td>0.823</td>\n",
       "      <td>-14.198</td>\n",
       "      <td>5.127</td>\n",
       "      <td>1.263</td>\n",
       "      <td>5.007</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.764</td>\n",
       "      <td>14.084</td>\n",
       "      <td>22.545</td>\n",
       "      <td>-51.205</td>\n",
       "      <td>-1.375</td>\n",
       "      <td>-11.849</td>\n",
       "      <td>-6.465</td>\n",
       "      <td>-16.208</td>\n",
       "      <td>-27.622</td>\n",
       "      <td>7.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T9</th>\n",
       "      <td>-27.110</td>\n",
       "      <td>1.332</td>\n",
       "      <td>26.188</td>\n",
       "      <td>1.382</td>\n",
       "      <td>4.158</td>\n",
       "      <td>-35.000</td>\n",
       "      <td>22.827</td>\n",
       "      <td>-0.774</td>\n",
       "      <td>11.798</td>\n",
       "      <td>-19.307</td>\n",
       "      <td>...</td>\n",
       "      <td>1.328</td>\n",
       "      <td>0.458</td>\n",
       "      <td>8.817</td>\n",
       "      <td>-12.647</td>\n",
       "      <td>26.404</td>\n",
       "      <td>6.237</td>\n",
       "      <td>69.702</td>\n",
       "      <td>12.726</td>\n",
       "      <td>-13.085</td>\n",
       "      <td>1.231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T10</th>\n",
       "      <td>18.167</td>\n",
       "      <td>-12.741</td>\n",
       "      <td>61.707</td>\n",
       "      <td>1.155</td>\n",
       "      <td>5.672</td>\n",
       "      <td>7.305</td>\n",
       "      <td>21.545</td>\n",
       "      <td>3.821</td>\n",
       "      <td>-7.362</td>\n",
       "      <td>7.417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.218</td>\n",
       "      <td>8.023</td>\n",
       "      <td>-20.986</td>\n",
       "      <td>-45.134</td>\n",
       "      <td>-12.423</td>\n",
       "      <td>3.033</td>\n",
       "      <td>-24.413</td>\n",
       "      <td>14.887</td>\n",
       "      <td>-22.709</td>\n",
       "      <td>-4.182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T11</th>\n",
       "      <td>54.597</td>\n",
       "      <td>15.761</td>\n",
       "      <td>-18.530</td>\n",
       "      <td>3.410</td>\n",
       "      <td>-3.610</td>\n",
       "      <td>-38.019</td>\n",
       "      <td>31.238</td>\n",
       "      <td>-2.033</td>\n",
       "      <td>-9.343</td>\n",
       "      <td>-2.256</td>\n",
       "      <td>...</td>\n",
       "      <td>2.262</td>\n",
       "      <td>11.587</td>\n",
       "      <td>-32.452</td>\n",
       "      <td>13.939</td>\n",
       "      <td>-7.526</td>\n",
       "      <td>1.691</td>\n",
       "      <td>10.175</td>\n",
       "      <td>-23.818</td>\n",
       "      <td>-11.278</td>\n",
       "      <td>-6.310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T12</th>\n",
       "      <td>1.877</td>\n",
       "      <td>25.806</td>\n",
       "      <td>-11.861</td>\n",
       "      <td>-9.231</td>\n",
       "      <td>0.141</td>\n",
       "      <td>-5.201</td>\n",
       "      <td>2.438</td>\n",
       "      <td>-1.438</td>\n",
       "      <td>0.884</td>\n",
       "      <td>5.953</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.405</td>\n",
       "      <td>-35.579</td>\n",
       "      <td>-12.444</td>\n",
       "      <td>-0.617</td>\n",
       "      <td>-32.116</td>\n",
       "      <td>27.626</td>\n",
       "      <td>6.813</td>\n",
       "      <td>48.035</td>\n",
       "      <td>2.789</td>\n",
       "      <td>-4.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T13</th>\n",
       "      <td>-36.106</td>\n",
       "      <td>-4.068</td>\n",
       "      <td>-20.542</td>\n",
       "      <td>3.796</td>\n",
       "      <td>0.175</td>\n",
       "      <td>14.971</td>\n",
       "      <td>47.623</td>\n",
       "      <td>-0.748</td>\n",
       "      <td>0.440</td>\n",
       "      <td>2.389</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>23.952</td>\n",
       "      <td>-0.469</td>\n",
       "      <td>4.078</td>\n",
       "      <td>-7.307</td>\n",
       "      <td>40.335</td>\n",
       "      <td>-15.057</td>\n",
       "      <td>-8.192</td>\n",
       "      <td>-9.681</td>\n",
       "      <td>-4.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T14</th>\n",
       "      <td>0.222</td>\n",
       "      <td>-8.022</td>\n",
       "      <td>-19.847</td>\n",
       "      <td>7.855</td>\n",
       "      <td>-2.785</td>\n",
       "      <td>-14.926</td>\n",
       "      <td>-1.317</td>\n",
       "      <td>-3.537</td>\n",
       "      <td>-3.654</td>\n",
       "      <td>-2.465</td>\n",
       "      <td>...</td>\n",
       "      <td>5.986</td>\n",
       "      <td>44.128</td>\n",
       "      <td>3.330</td>\n",
       "      <td>3.140</td>\n",
       "      <td>9.266</td>\n",
       "      <td>-24.414</td>\n",
       "      <td>-11.040</td>\n",
       "      <td>46.187</td>\n",
       "      <td>-15.714</td>\n",
       "      <td>-0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T15</th>\n",
       "      <td>9.239</td>\n",
       "      <td>5.735</td>\n",
       "      <td>2.892</td>\n",
       "      <td>5.348</td>\n",
       "      <td>0.592</td>\n",
       "      <td>-17.717</td>\n",
       "      <td>-16.808</td>\n",
       "      <td>-1.083</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>-6.888</td>\n",
       "      <td>...</td>\n",
       "      <td>4.870</td>\n",
       "      <td>-8.181</td>\n",
       "      <td>0.094</td>\n",
       "      <td>-0.059</td>\n",
       "      <td>50.011</td>\n",
       "      <td>39.964</td>\n",
       "      <td>-27.383</td>\n",
       "      <td>5.462</td>\n",
       "      <td>-2.605</td>\n",
       "      <td>-2.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T16</th>\n",
       "      <td>-10.953</td>\n",
       "      <td>13.823</td>\n",
       "      <td>-1.610</td>\n",
       "      <td>8.195</td>\n",
       "      <td>-0.982</td>\n",
       "      <td>-1.889</td>\n",
       "      <td>39.936</td>\n",
       "      <td>-3.451</td>\n",
       "      <td>-4.252</td>\n",
       "      <td>-4.087</td>\n",
       "      <td>...</td>\n",
       "      <td>10.356</td>\n",
       "      <td>-33.883</td>\n",
       "      <td>-0.643</td>\n",
       "      <td>0.756</td>\n",
       "      <td>22.409</td>\n",
       "      <td>-35.080</td>\n",
       "      <td>-21.331</td>\n",
       "      <td>1.495</td>\n",
       "      <td>9.214</td>\n",
       "      <td>4.392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T17</th>\n",
       "      <td>-5.230</td>\n",
       "      <td>1.843</td>\n",
       "      <td>5.100</td>\n",
       "      <td>-2.381</td>\n",
       "      <td>3.743</td>\n",
       "      <td>-9.237</td>\n",
       "      <td>-3.624</td>\n",
       "      <td>-0.821</td>\n",
       "      <td>12.095</td>\n",
       "      <td>0.858</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.539</td>\n",
       "      <td>2.934</td>\n",
       "      <td>-0.354</td>\n",
       "      <td>-1.995</td>\n",
       "      <td>-3.769</td>\n",
       "      <td>5.324</td>\n",
       "      <td>-0.899</td>\n",
       "      <td>-28.038</td>\n",
       "      <td>-11.157</td>\n",
       "      <td>0.528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T18</th>\n",
       "      <td>-1.429</td>\n",
       "      <td>0.715</td>\n",
       "      <td>1.337</td>\n",
       "      <td>1.798</td>\n",
       "      <td>-1.040</td>\n",
       "      <td>0.544</td>\n",
       "      <td>-5.637</td>\n",
       "      <td>-2.086</td>\n",
       "      <td>3.278</td>\n",
       "      <td>-2.248</td>\n",
       "      <td>...</td>\n",
       "      <td>63.077</td>\n",
       "      <td>0.698</td>\n",
       "      <td>1.459</td>\n",
       "      <td>-1.426</td>\n",
       "      <td>-10.014</td>\n",
       "      <td>3.796</td>\n",
       "      <td>1.797</td>\n",
       "      <td>-3.043</td>\n",
       "      <td>-0.889</td>\n",
       "      <td>-0.296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T19</th>\n",
       "      <td>0.268</td>\n",
       "      <td>1.619</td>\n",
       "      <td>1.673</td>\n",
       "      <td>54.756</td>\n",
       "      <td>-2.680</td>\n",
       "      <td>0.371</td>\n",
       "      <td>-5.020</td>\n",
       "      <td>-0.595</td>\n",
       "      <td>12.910</td>\n",
       "      <td>-8.104</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.920</td>\n",
       "      <td>-3.520</td>\n",
       "      <td>-0.369</td>\n",
       "      <td>-0.401</td>\n",
       "      <td>-10.604</td>\n",
       "      <td>1.834</td>\n",
       "      <td>-0.157</td>\n",
       "      <td>-0.287</td>\n",
       "      <td>0.684</td>\n",
       "      <td>-1.888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T20</th>\n",
       "      <td>3.502</td>\n",
       "      <td>0.296</td>\n",
       "      <td>-1.122</td>\n",
       "      <td>-11.266</td>\n",
       "      <td>-3.272</td>\n",
       "      <td>1.509</td>\n",
       "      <td>2.566</td>\n",
       "      <td>-9.178</td>\n",
       "      <td>51.053</td>\n",
       "      <td>2.382</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.394</td>\n",
       "      <td>1.744</td>\n",
       "      <td>-9.912</td>\n",
       "      <td>0.537</td>\n",
       "      <td>2.768</td>\n",
       "      <td>-4.705</td>\n",
       "      <td>-6.549</td>\n",
       "      <td>3.413</td>\n",
       "      <td>-0.256</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0       1       2       3       4       5       6       7       8   \\\n",
       "T1   29.791  17.686   9.807  51.822   7.724  14.526  24.805   7.612   9.976   \n",
       "T2   43.696  61.084  25.675  -2.635  11.083  24.055  29.458  14.513  10.711   \n",
       "T3  -43.476  16.552 -12.048   3.366  -5.766 -17.867 -23.585  -5.073 -10.172   \n",
       "T4  -12.433  53.639  43.674  -1.269   9.454  12.544  -0.695  11.263  -1.428   \n",
       "T5  -40.689  12.299   8.781   4.134  -0.394 -13.443 -22.172  -2.673 -12.388   \n",
       "T6   -0.835  28.373  30.370  -0.542   6.804  34.193  -5.071   8.780  -0.804   \n",
       "T7   21.931  -3.669 -29.226   6.422   8.731  66.567   1.444   2.008  -2.627   \n",
       "T8   -8.423  71.967 -18.137  -2.458  -2.587   0.823 -14.198   5.127   1.263   \n",
       "T9  -27.110   1.332  26.188   1.382   4.158 -35.000  22.827  -0.774  11.798   \n",
       "T10  18.167 -12.741  61.707   1.155   5.672   7.305  21.545   3.821  -7.362   \n",
       "T11  54.597  15.761 -18.530   3.410  -3.610 -38.019  31.238  -2.033  -9.343   \n",
       "T12   1.877  25.806 -11.861  -9.231   0.141  -5.201   2.438  -1.438   0.884   \n",
       "T13 -36.106  -4.068 -20.542   3.796   0.175  14.971  47.623  -0.748   0.440   \n",
       "T14   0.222  -8.022 -19.847   7.855  -2.785 -14.926  -1.317  -3.537  -3.654   \n",
       "T15   9.239   5.735   2.892   5.348   0.592 -17.717 -16.808  -1.083  -0.539   \n",
       "T16 -10.953  13.823  -1.610   8.195  -0.982  -1.889  39.936  -3.451  -4.252   \n",
       "T17  -5.230   1.843   5.100  -2.381   3.743  -9.237  -3.624  -0.821  12.095   \n",
       "T18  -1.429   0.715   1.337   1.798  -1.040   0.544  -5.637  -2.086   3.278   \n",
       "T19   0.268   1.619   1.673  54.756  -2.680   0.371  -5.020  -0.595  12.910   \n",
       "T20   3.502   0.296  -1.122 -11.266  -3.272   1.509   2.566  -9.178  51.053   \n",
       "\n",
       "          9   ...      12      13      14      15      16      17      18  \\\n",
       "T1   251.805  ...  17.492  12.633  30.351  15.166  55.424  15.331  52.467   \n",
       "T2   -50.073  ...   0.259  34.657  43.218  19.044  -8.296  30.755   0.701   \n",
       "T3    17.012  ...  -0.130  -8.486 -55.902 -10.984   3.744  -8.736   0.651   \n",
       "T4    -3.616  ...   1.928  51.688 -25.317  22.918   9.265  -5.565   2.018   \n",
       "T5     5.462  ...  -0.430  10.616 -74.620  11.367  -0.162  -2.755  -4.685   \n",
       "T6    -3.105  ...   0.582  -6.504   0.561  69.399   6.388  -8.211   2.024   \n",
       "T7   -16.384  ...   5.958  -0.044 -37.765 -26.778  34.892   2.303  34.098   \n",
       "T8     5.007  ...  -4.764  14.084  22.545 -51.205  -1.375 -11.849  -6.465   \n",
       "T9   -19.307  ...   1.328   0.458   8.817 -12.647  26.404   6.237  69.702   \n",
       "T10    7.417  ...   0.218   8.023 -20.986 -45.134 -12.423   3.033 -24.413   \n",
       "T11   -2.256  ...   2.262  11.587 -32.452  13.939  -7.526   1.691  10.175   \n",
       "T12    5.953  ...  -2.405 -35.579 -12.444  -0.617 -32.116  27.626   6.813   \n",
       "T13    2.389  ...  -0.513  23.952  -0.469   4.078  -7.307  40.335 -15.057   \n",
       "T14   -2.465  ...   5.986  44.128   3.330   3.140   9.266 -24.414 -11.040   \n",
       "T15   -6.888  ...   4.870  -8.181   0.094  -0.059  50.011  39.964 -27.383   \n",
       "T16   -4.087  ...  10.356 -33.883  -0.643   0.756  22.409 -35.080 -21.331   \n",
       "T17    0.858  ...  -0.539   2.934  -0.354  -1.995  -3.769   5.324  -0.899   \n",
       "T18   -2.248  ...  63.077   0.698   1.459  -1.426 -10.014   3.796   1.797   \n",
       "T19   -8.104  ...  -4.920  -3.520  -0.369  -0.401 -10.604   1.834  -0.157   \n",
       "T20    2.382  ...  -1.394   1.744  -9.912   0.537   2.768  -4.705  -6.549   \n",
       "\n",
       "         19      20      21  \n",
       "T1   14.303  18.021  31.772  \n",
       "T2   30.218  38.219  63.782  \n",
       "T3    3.686 -16.388 -82.838  \n",
       "T4   -1.911  82.910 -52.155  \n",
       "T5   -2.710   2.736  81.545  \n",
       "T6    4.220 -68.081  -9.912  \n",
       "T7    3.333  -3.915  -0.346  \n",
       "T8  -16.208 -27.622   7.151  \n",
       "T9   12.726 -13.085   1.231  \n",
       "T10  14.887 -22.709  -4.182  \n",
       "T11 -23.818 -11.278  -6.310  \n",
       "T12  48.035   2.789  -4.001  \n",
       "T13  -8.192  -9.681  -4.352  \n",
       "T14  46.187 -15.714  -0.943  \n",
       "T15   5.462  -2.605  -2.819  \n",
       "T16   1.495   9.214   4.392  \n",
       "T17 -28.038 -11.157   0.528  \n",
       "T18  -3.043  -0.889  -0.296  \n",
       "T19  -0.287   0.684  -1.888  \n",
       "T20   3.413  -0.256   0.091  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_df = pd.DataFrame(np.round(document_topics, 3), \n",
    "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document #3:\n",
      "Dominant Topics (top 3): ['T19', 'T1', 'T20']\n",
      "Paper Summary:\n",
      "192 \n",
      "PHASE TRANSITIONS IN NEURAL NETWORKS \n",
      "Joshua Chover \n",
      "University o Wisconsin, Madison, WI 53706 \n",
      "Various simulations o cortical subnetworks have evidenced \n",
      "something like phase transitions with respect to key parameters. \n",
      "We demonstrate that. such transitions must. indeed exist_ in analogous \n",
      "ininite array models. For related inite array models classical \n",
      "phase transit.ions (which describe steady-state behavior) may not. \n",
      "exist., but. there can be distinct. qualitative changes in \n",
      "(\"metastab\n",
      "\n",
      "Document #5:\n",
      "Dominant Topics (top 3): ['T7', 'T11', 'T9']\n",
      "Paper Summary:\n",
      "223 \n",
      "'Ensemble' Boltzmann Units \n",
      "have Collective Computational Properties \n",
      "like those of Hopfield and Tank Neurons \n",
      "Mark Derthick and Joe Tebelskis \n",
      "Department of Computer Science \n",
      "Carnegie-Mellon University \n",
      "1 Introduction \n",
      "There are three existing connection:,t models in which network states are assigned \n",
      "a computational energy. These models Hopfield nets, Hopfield and Tank nets, and \n",
      "Boltzmann Machines--search for states with minimal energy. Every link in the net- \n",
      "work can be thought of as \n",
      "\n",
      "Document #19:\n",
      "Dominant Topics (top 3): ['T12', 'T14', 'T2']\n",
      "Paper Summary:\n",
      "75O \n",
      "A DYNAMICAL APPROACH TO TEMPORAL PATTERN \n",
      "PROCESSING \n",
      "W. Scott Stornetta \n",
      "Stanford University, Physics Department, Stanford, Ca., 94305 \n",
      "Tad Hogg and B. A. Huberman \n",
      "Xerox Palo Alto Research Center, Palo Alto, Ca. 94304 \n",
      "ABSTRACT \n",
      "Recognizing patterns with temporal context is important for \n",
      "such tasks as speech recognition, motion detection and signature \n",
      "verification. We propose an architecture in which time serves as its \n",
      "own representation, and temporal context is encoded in the state of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# document_numbers = [13, 250, 500]\n",
    "document_numbers = [3, 5, 19]\n",
    "\n",
    "for document_number in document_numbers:\n",
    "    top_topics = list(dt_df.columns[np.argsort(-np.absolute(dt_df.iloc[document_number].values))[:3]])\n",
    "    print('Document #'+str(document_number)+':')\n",
    "    print('Dominant Topics (top 3):', top_topics)\n",
    "    print('Paper Summary:')\n",
    "    print(papers[document_number][:500])\n",
    "    print()\n",
    "# If you check out the terms in the topics we obtained in the preceding output, they\n",
    "# actually make sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models with Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components =TOTAL_TOPICS, max_iter=500, max_doc_update_iter=50,\n",
    "                                      learning_method='online', batch_size=1740, learning_offset=50., \n",
    "                                      random_state=42, n_jobs=16)\n",
    "document_topics = lda_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can then obtain the topic-term matrix and build a dataframe from it to showcase the topics and terms\n",
    "# in an easy-to-interpret format\n",
    "topic_terms = lda_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>cell, learning, decision, classifier, fig, region, node, class, map, trial, hold long, aip conference, vector, error, top layer, time step, size, synapsis, complex, two layer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>classifier, region, node, decision, feature, training, class, performance, fig, velocity, cell, map, back propagation, error, two layer, grid, hyperplanes, code, vector, line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>learning, cell, rule, fig, firing, probability, tank, level, vector, active, experiment, energy, rate, temporal, net, node, line, recurrence, brain, element</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>learning, reinforcement, symbol, noise, probability, synapse, algorithm, stochastic, search, procedure, trial, rule, activation, fig, control, global, hinton, performance, cell, adaptive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>learning, noise, synapse, fig, level, procedure, electronic, analog, reinforcement, gain, supervised, control, competitive, distribution, run, search, vector, technique, local, stochastic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>brain, fig, level, fiber, specifically, solution, changed, science, real number, chance, known, pattern figure, complicated, advanced research, trained, hippocampus pitiform, correct output, nucleus, classifier, association</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>node, classifier, rate, vector, training, cell, temporal, hopfield, fig, synapsis, class, computed, field, region, learning, decision, representation, trial, feature, capacity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>cell, reinforcement, learning, potential, technique, associative, fig, rule, architecture, hopfield, synapsis, average, cortical, well defined, lisp program, synaptic, array, distribution used, optimal, fruitful discussion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>capacity, associative memory, associative, vector, address, kanerva, bound, sequence, error, exponential, rate, cell, element, training, adaptive, radius, location, code, distance, let</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>cell, fig, net, vector, training, motion, cycle, classifier, decision, associative memory, feature, fiber, location, assume, decay, back, performance, mellon university, application, field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>vector, hopfield, image, matrix, associative, associative memory, code, object, capacity, class, energy, field, error, minimum, fig, component, recall, classifier, noise, tank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>reinforcement, nat, strategy, net, adaptive, probability, element, training, play, produce, deterministic, timing, learning, game, fig, negative, positive, adapt, sequential, sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>cell, sequence, learning, probability, rate, net, nat, associative, let, element, bound, component, training, upper, associative memory, output signal, cycle, radius, ii, step</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>cycle, learning, fig, reinforcement, simulation, capacity, nat, training, york wiley, algorithm, lisp, resistive, class, cell, region, classifier, goal, feature, vector, adaptive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>cell, firing, synapsis, activity, cortical, lot, simulation, probability, fiber, synaptic, cortex, inhibitory, rule, axon, potential, connectivity, ii, excitatory, cell firing, learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>learning, training, fig, classifier, node, symbol, cycle, decision, region, control, algorithm, rate, action, capacity, feature, recurrence, temporal, error, level, element</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>node, line, fig, cell, motion, training, velocity, decision, classifier, nat, energy, region, cycle, feature, class, image, net, rule, flow, shown fig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>hopfield, cell, symbol, probability, failure, learning, find, size, perhaps, classifier also, determine, anterior, yield different, model boltzmann, local, established, synapsis, al, strategy, classification</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>cell, cycle, firing, probability, lot, learning, rule, nat, connectivity, cortex, symbol, activity, inhibitory, ii, local, axon, boltzmann, simulation, path, performance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>learning, rule, algorithm, temporal, training, action, element, reinforcement, cell, ii, task, simulation, average, net, search, convergence, correlate, control, distribution, mead</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                         Terms per Topic\n",
       "Topic1   cell, learning, decision, classifier, fig, region, node, class, map, trial, hold long, aip conference, vector, error, top layer, time step, size, synapsis, complex, two layer                                                 \n",
       "Topic2   classifier, region, node, decision, feature, training, class, performance, fig, velocity, cell, map, back propagation, error, two layer, grid, hyperplanes, code, vector, line                                                 \n",
       "Topic3   learning, cell, rule, fig, firing, probability, tank, level, vector, active, experiment, energy, rate, temporal, net, node, line, recurrence, brain, element                                                                   \n",
       "Topic4   learning, reinforcement, symbol, noise, probability, synapse, algorithm, stochastic, search, procedure, trial, rule, activation, fig, control, global, hinton, performance, cell, adaptive                                     \n",
       "Topic5   learning, noise, synapse, fig, level, procedure, electronic, analog, reinforcement, gain, supervised, control, competitive, distribution, run, search, vector, technique, local, stochastic                                    \n",
       "Topic6   brain, fig, level, fiber, specifically, solution, changed, science, real number, chance, known, pattern figure, complicated, advanced research, trained, hippocampus pitiform, correct output, nucleus, classifier, association\n",
       "Topic7   node, classifier, rate, vector, training, cell, temporal, hopfield, fig, synapsis, class, computed, field, region, learning, decision, representation, trial, feature, capacity                                                \n",
       "Topic8   cell, reinforcement, learning, potential, technique, associative, fig, rule, architecture, hopfield, synapsis, average, cortical, well defined, lisp program, synaptic, array, distribution used, optimal, fruitful discussion \n",
       "Topic9   capacity, associative memory, associative, vector, address, kanerva, bound, sequence, error, exponential, rate, cell, element, training, adaptive, radius, location, code, distance, let                                       \n",
       "Topic10  cell, fig, net, vector, training, motion, cycle, classifier, decision, associative memory, feature, fiber, location, assume, decay, back, performance, mellon university, application, field                                   \n",
       "Topic11  vector, hopfield, image, matrix, associative, associative memory, code, object, capacity, class, energy, field, error, minimum, fig, component, recall, classifier, noise, tank                                                \n",
       "Topic12  reinforcement, nat, strategy, net, adaptive, probability, element, training, play, produce, deterministic, timing, learning, game, fig, negative, positive, adapt, sequential, sequence                                        \n",
       "Topic13  cell, sequence, learning, probability, rate, net, nat, associative, let, element, bound, component, training, upper, associative memory, output signal, cycle, radius, ii, step                                                \n",
       "Topic14  cycle, learning, fig, reinforcement, simulation, capacity, nat, training, york wiley, algorithm, lisp, resistive, class, cell, region, classifier, goal, feature, vector, adaptive                                             \n",
       "Topic15  cell, firing, synapsis, activity, cortical, lot, simulation, probability, fiber, synaptic, cortex, inhibitory, rule, axon, potential, connectivity, ii, excitatory, cell firing, learning                                      \n",
       "Topic16  learning, training, fig, classifier, node, symbol, cycle, decision, region, control, algorithm, rate, action, capacity, feature, recurrence, temporal, error, level, element                                                   \n",
       "Topic17  node, line, fig, cell, motion, training, velocity, decision, classifier, nat, energy, region, cycle, feature, class, image, net, rule, flow, shown fig                                                                         \n",
       "Topic18  hopfield, cell, symbol, probability, failure, learning, find, size, perhaps, classifier also, determine, anterior, yield different, model boltzmann, local, established, synapsis, al, strategy, classification                \n",
       "Topic19  cell, cycle, firing, probability, lot, learning, rule, nat, connectivity, cortex, symbol, activity, inhibitory, ii, local, axon, boltzmann, simulation, path, performance                                                      \n",
       "Topic20  learning, rule, algorithm, temporal, training, action, element, reinforcement, cell, ii, task, simulation, average, net, search, convergence, correlate, control, distribution, mead                                           "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics,\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df\n",
    "# Generated topics from our LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>T1</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T2</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T4</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T7</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T8</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T9</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T11</th>\n",
       "      <td>0.078</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.409</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T12</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T13</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T14</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T15</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.062</td>\n",
       "      <td>1.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.883</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T16</th>\n",
       "      <td>0.878</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.591</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.939</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.937</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.996</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.999</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T17</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T18</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T19</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T20</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2     3     4     5     6     7     8     9  ...    12  \\\n",
       "T1  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T4  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T5  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T6  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T7  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T8  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T9  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T11 0.078 0.999 0.409 0.000 0.999 0.999 0.000 0.999 0.000 0.000  ... 0.000   \n",
       "T12 0.043 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T15 0.000 0.000 0.000 0.999 0.000 0.000 0.060 0.000 0.062 1.000  ... 0.999   \n",
       "T16 0.878 0.000 0.591 0.000 0.000 0.000 0.939 0.000 0.937 0.000  ... 0.000   \n",
       "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T18 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "T20 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  ... 0.000   \n",
       "\n",
       "       13    14    15    16    17    18    19    20    21  \n",
       "T1  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T2  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T3  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T4  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T5  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T6  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T7  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T8  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T9  0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T10 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T11 0.999 0.000 0.003 0.116 0.000 0.000 0.000 0.999 0.000  \n",
       "T12 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 1.000  \n",
       "T13 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T14 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T15 0.000 0.000 0.000 0.883 0.000 0.999 0.000 0.000 0.000  \n",
       "T16 0.000 0.999 0.996 0.000 0.999 0.000 0.999 0.000 0.000  \n",
       "T17 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T18 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T19 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "T20 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000  \n",
       "\n",
       "[20 rows x 22 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "dt_df = pd.DataFrame(document_topics, \n",
    "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Contribution %</th>\n",
       "      <th>Paper Num</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Paper Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>T1</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>cell, learning, decision, classifier, fig, region, node, class, map, trial, hold long, aip conference, vector, error, top layer, time step, size, synapsis, complex, two layer</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>T2</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>classifier, region, node, decision, feature, training, class, performance, fig, velocity, cell, map, back propagation, error, two layer, grid, hyperplanes, code, vector, line</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>T3</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>learning, cell, rule, fig, firing, probability, tank, level, vector, active, experiment, energy, rate, temporal, net, node, line, recurrence, brain, element</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>T4</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>learning, reinforcement, symbol, noise, probability, synapse, algorithm, stochastic, search, procedure, trial, rule, activation, fig, control, global, hinton, performance, cell, adaptive</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>T5</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>learning, noise, synapse, fig, level, procedure, electronic, analog, reinforcement, gain, supervised, control, competitive, distribution, run, search, vector, technique, local, stochastic</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>T6</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>brain, fig, level, fiber, specifically, solution, changed, science, real number, chance, known, pattern figure, complicated, advanced research, trained, hippocampus pitiform, correct output, nucle...</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>T7</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>node, classifier, rate, vector, training, cell, temporal, hopfield, fig, synapsis, class, computed, field, region, learning, decision, representation, trial, feature, capacity</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>T8</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>cell, reinforcement, learning, potential, technique, associative, fig, rule, architecture, hopfield, synapsis, average, cortical, well defined, lisp program, synaptic, array, distribution used, op...</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>T9</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>capacity, associative memory, associative, vector, address, kanerva, bound, sequence, error, exponential, rate, cell, element, training, adaptive, radius, location, code, distance, let</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>T10</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>cell, fig, net, vector, training, motion, cycle, classifier, decision, associative memory, feature, fiber, location, assume, decay, back, performance, mellon university, application, field</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>T11</td>\n",
       "      <td>0.99941</td>\n",
       "      <td>20</td>\n",
       "      <td>vector, hopfield, image, matrix, associative, associative memory, code, object, capacity, class, energy, field, error, minimum, fig, component, recall, classifier, noise, tank</td>\n",
       "      <td>83O \\nInvariant Object Recognition Using a Distributed Associative Memory \\nHarry Wechsler and George Lee Zimmerman \\nDepartment of Electrical Engineering \\nUniversity of Minnesota \\nMinneapolis, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>T12</td>\n",
       "      <td>0.99958</td>\n",
       "      <td>21</td>\n",
       "      <td>reinforcement, nat, strategy, net, adaptive, probability, element, training, play, produce, deterministic, timing, learning, game, fig, negative, positive, adapt, sequential, sequence</td>\n",
       "      <td>840 \\nLEARNING IN NETWORKS OF \\nNONDETERMINISTIC ADAPTIVE LOGIC ELEMENTS \\nRichard C. Windecker* \\nAT&amp;T Bell Laboratories, Middletown, NJ 07748 \\nABSTRACT \\nThis paper presents a model of nondeter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>T13</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>cell, sequence, learning, probability, rate, net, nat, associative, let, element, bound, component, training, upper, associative memory, output signal, cycle, radius, ii, step</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>T14</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>cycle, learning, fig, reinforcement, simulation, capacity, nat, training, york wiley, algorithm, lisp, resistive, class, cell, region, classifier, goal, feature, vector, adaptive</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>T15</td>\n",
       "      <td>0.99975</td>\n",
       "      <td>9</td>\n",
       "      <td>cell, firing, synapsis, activity, cortical, lot, simulation, probability, fiber, synaptic, cortex, inhibitory, rule, axon, potential, connectivity, ii, excitatory, cell firing, learning</td>\n",
       "      <td>317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>T16</td>\n",
       "      <td>0.99948</td>\n",
       "      <td>10</td>\n",
       "      <td>learning, training, fig, classifier, node, symbol, cycle, decision, region, control, algorithm, rate, action, capacity, feature, recurrence, temporal, error, level, element</td>\n",
       "      <td>387 \\nNeural Net and Traditional Classifiers  \\nWilliam Y. Huang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173, USA \\nAbstract. Previous work on nets with continuous-valu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>T17</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>node, line, fig, cell, motion, training, velocity, decision, classifier, nat, energy, region, cycle, feature, class, image, net, rule, flow, shown fig</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>T18</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>hopfield, cell, symbol, probability, failure, learning, find, size, perhaps, classifier also, determine, anterior, yield different, model boltzmann, local, established, synapsis, al, strategy, cla...</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>T19</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>cell, cycle, firing, probability, lot, learning, rule, nat, connectivity, cortex, symbol, activity, inhibitory, ii, local, axon, boltzmann, simulation, path, performance</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>T20</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>12</td>\n",
       "      <td>learning, rule, algorithm, temporal, training, action, element, reinforcement, cell, ii, task, simulation, average, net, search, convergence, correlate, control, distribution, mead</td>\n",
       "      <td>457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topic  Contribution %  Paper Num  \\\n",
       "Topic1              T1         0.00007         12   \n",
       "Topic2              T2         0.00007         12   \n",
       "Topic3              T3         0.00007         12   \n",
       "Topic4              T4         0.00007         12   \n",
       "Topic5              T5         0.00007         12   \n",
       "Topic6              T6         0.00007         12   \n",
       "Topic7              T7         0.00007         12   \n",
       "Topic8              T8         0.00007         12   \n",
       "Topic9              T9         0.00007         12   \n",
       "Topic10            T10         0.00007         12   \n",
       "Topic11            T11         0.99941         20   \n",
       "Topic12            T12         0.99958         21   \n",
       "Topic13            T13         0.00007         12   \n",
       "Topic14            T14         0.00007         12   \n",
       "Topic15            T15         0.99975          9   \n",
       "Topic16            T16         0.99948         10   \n",
       "Topic17            T17         0.00007         12   \n",
       "Topic18            T18         0.00007         12   \n",
       "Topic19            T19         0.00007         12   \n",
       "Topic20            T20         0.00007         12   \n",
       "\n",
       "                                                                                                                                                                                                           Topic  \\\n",
       "Topic1                            cell, learning, decision, classifier, fig, region, node, class, map, trial, hold long, aip conference, vector, error, top layer, time step, size, synapsis, complex, two layer   \n",
       "Topic2                            classifier, region, node, decision, feature, training, class, performance, fig, velocity, cell, map, back propagation, error, two layer, grid, hyperplanes, code, vector, line   \n",
       "Topic3                                              learning, cell, rule, fig, firing, probability, tank, level, vector, active, experiment, energy, rate, temporal, net, node, line, recurrence, brain, element   \n",
       "Topic4                learning, reinforcement, symbol, noise, probability, synapse, algorithm, stochastic, search, procedure, trial, rule, activation, fig, control, global, hinton, performance, cell, adaptive   \n",
       "Topic5               learning, noise, synapse, fig, level, procedure, electronic, analog, reinforcement, gain, supervised, control, competitive, distribution, run, search, vector, technique, local, stochastic   \n",
       "Topic6   brain, fig, level, fiber, specifically, solution, changed, science, real number, chance, known, pattern figure, complicated, advanced research, trained, hippocampus pitiform, correct output, nucle...   \n",
       "Topic7                           node, classifier, rate, vector, training, cell, temporal, hopfield, fig, synapsis, class, computed, field, region, learning, decision, representation, trial, feature, capacity   \n",
       "Topic8   cell, reinforcement, learning, potential, technique, associative, fig, rule, architecture, hopfield, synapsis, average, cortical, well defined, lisp program, synaptic, array, distribution used, op...   \n",
       "Topic9                  capacity, associative memory, associative, vector, address, kanerva, bound, sequence, error, exponential, rate, cell, element, training, adaptive, radius, location, code, distance, let   \n",
       "Topic10             cell, fig, net, vector, training, motion, cycle, classifier, decision, associative memory, feature, fiber, location, assume, decay, back, performance, mellon university, application, field   \n",
       "Topic11                          vector, hopfield, image, matrix, associative, associative memory, code, object, capacity, class, energy, field, error, minimum, fig, component, recall, classifier, noise, tank   \n",
       "Topic12                  reinforcement, nat, strategy, net, adaptive, probability, element, training, play, produce, deterministic, timing, learning, game, fig, negative, positive, adapt, sequential, sequence   \n",
       "Topic13                          cell, sequence, learning, probability, rate, net, nat, associative, let, element, bound, component, training, upper, associative memory, output signal, cycle, radius, ii, step   \n",
       "Topic14                       cycle, learning, fig, reinforcement, simulation, capacity, nat, training, york wiley, algorithm, lisp, resistive, class, cell, region, classifier, goal, feature, vector, adaptive   \n",
       "Topic15                cell, firing, synapsis, activity, cortical, lot, simulation, probability, fiber, synaptic, cortex, inhibitory, rule, axon, potential, connectivity, ii, excitatory, cell firing, learning   \n",
       "Topic16                             learning, training, fig, classifier, node, symbol, cycle, decision, region, control, algorithm, rate, action, capacity, feature, recurrence, temporal, error, level, element   \n",
       "Topic17                                                   node, line, fig, cell, motion, training, velocity, decision, classifier, nat, energy, region, cycle, feature, class, image, net, rule, flow, shown fig   \n",
       "Topic18  hopfield, cell, symbol, probability, failure, learning, find, size, perhaps, classifier also, determine, anterior, yield different, model boltzmann, local, established, synapsis, al, strategy, cla...   \n",
       "Topic19                                cell, cycle, firing, probability, lot, learning, rule, nat, connectivity, cortex, symbol, activity, inhibitory, ii, local, axon, boltzmann, simulation, path, performance   \n",
       "Topic20                     learning, rule, algorithm, temporal, training, action, element, reinforcement, cell, ii, task, simulation, average, net, search, convergence, correlate, control, distribution, mead   \n",
       "\n",
       "                                                                                                                                                                                                      Paper Name  \n",
       "Topic1   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic2   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic3   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic4   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic5   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic6   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic7   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic8   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic9   457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic10  457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic11  83O \\nInvariant Object Recognition Using a Distributed Associative Memory \\nHarry Wechsler and George Lee Zimmerman \\nDepartment of Electrical Engineering \\nUniversity of Minnesota \\nMinneapolis, ...  \n",
       "Topic12  840 \\nLEARNING IN NETWORKS OF \\nNONDETERMINISTIC ADAPTIVE LOGIC ELEMENTS \\nRichard C. Windecker* \\nAT&T Bell Laboratories, Middletown, NJ 07748 \\nABSTRACT \\nThis paper presents a model of nondeter...  \n",
       "Topic13  457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic14  457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic15  317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...  \n",
       "Topic16  387 \\nNeural Net and Traditional Classifiers  \\nWilliam Y. Huang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173, USA \\nAbstract. Previous work on nets with continuous-valu...  \n",
       "Topic17  457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic18  457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic19  457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  \n",
       "Topic20  457 \\nDISTRIBUTED NEURAL INFORMATION PROCESSING \\nIN THE VESTIBULO-OCULAR SYSTEM \\nClifford Lau \\nOffice of Naval Research Detachment \\nPasadena, CA 91106 \\nVicente Honrubia* \\nUCLA Division of He...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_contrib_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_contrib_topics.index\n",
    "contrib_perc = max_contrib_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_contrib_topics.loc[t]].index[0]\n",
    "                       for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Contribution %': contrib_perc,\n",
    "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
    "                          'Paper Name': documents})\n",
    "results_df\n",
    "# Viewing each topic and corresponding paper with its maximum contribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Models with Non-Negative Matrix Factorization (NMF)\n",
    "Another matrix decomposition technique similar to SVD but operates on non-negative matrices and works well for multivariate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cv_features' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=TOTAL_TOPICS, solver='cd', max_iter=500,\n",
    "                random_state=42, alpha=.1, l1_ratio=.85)\n",
    "document_topics = nmf_model.fit_transform(cv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Terms per Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, decision</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, training set, learn, test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                     Terms per Topic\n",
       "Topic1   bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum                                            \n",
       "Topic2   neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing                                \n",
       "Topic3   state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, decision                 \n",
       "Topic4   image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface                                                 \n",
       "Topic5   hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, training set, learn, test\n",
       "Topic6   cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property                                                        \n",
       "Topic7   word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state                               \n",
       "Topic8   signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig                                                        \n",
       "Topic9   control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant                                             \n",
       "Topic10  circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp                                                   \n",
       "Topic11  spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution                                \n",
       "Topic12  rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned                                         \n",
       "Topic13  node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation                                        \n",
       "Topic14  feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra                    \n",
       "Topic15  classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian                       \n",
       "Topic16  distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum                     \n",
       "Topic17  motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed                                                       \n",
       "Topic18  object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint                                      \n",
       "Topic19  field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation                    \n",
       "Topic20  memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol                    "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now that we have our model trained, we can look at the generated topics using the following code\n",
    "topic_terms = nmf_model.components_\n",
    "topic_key_term_idxs = np.argsort(-np.absolute(topic_terms), axis=1)[:, :top_terms]\n",
    "topic_keyterms = vocabulary[topic_key_term_idxs]\n",
    "topics = [', '.join(topic) for topic in topic_keyterms]\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "topics_df = pd.DataFrame(topics,\n",
    "                         columns = ['Terms per Topic'],\n",
    "                         index=['Topic'+str(t) for t in range(1, TOTAL_TOPICS+1)])\n",
    "topics_df\n",
    "# Generated topics from our NMF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no major repetitions of topics and each topic talks about a clear and distinct theme.\n",
    "\n",
    "The results from the NMF topic model are definitely better than what we obtained from LDA in Scikit-Learn!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T1</th>\n",
       "      <th>T2</th>\n",
       "      <th>T3</th>\n",
       "      <th>T4</th>\n",
       "      <th>T5</th>\n",
       "      <th>T6</th>\n",
       "      <th>T7</th>\n",
       "      <th>T8</th>\n",
       "      <th>T9</th>\n",
       "      <th>T10</th>\n",
       "      <th>T11</th>\n",
       "      <th>T12</th>\n",
       "      <th>T13</th>\n",
       "      <th>T14</th>\n",
       "      <th>T15</th>\n",
       "      <th>T16</th>\n",
       "      <th>T17</th>\n",
       "      <th>T18</th>\n",
       "      <th>T19</th>\n",
       "      <th>T20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.444</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.394</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.032</td>\n",
       "      <td>0.619</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.378</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.024</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.656</td>\n",
       "      <td>0.277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.102</td>\n",
       "      <td>0.265</td>\n",
       "      <td>1.019</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.218</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>1.299</td>\n",
       "      <td>0.291</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.060</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.167</td>\n",
       "      <td>1.402</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.093</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.020</td>\n",
       "      <td>1.749</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.344</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.679</td>\n",
       "      <td>7.510</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.326</td>\n",
       "      <td>1.146</td>\n",
       "      <td>1.923</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.646</td>\n",
       "      <td>0.641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000</td>\n",
       "      <td>1.415</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.046</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.030</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.147</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.274</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.045</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.084</td>\n",
       "      <td>1.760</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.592</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.055</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.514</td>\n",
       "      <td>0.353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.395</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>1.258</td>\n",
       "      <td>0.127</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.075</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.017</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     T1    T2    T3    T4    T5    T6    T7    T8    T9   T10   T11   T12  \\\n",
       "0 0.444 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "1 0.394 0.595 0.463 0.019 0.187 0.037 0.000 0.228 0.130 0.029 0.000 0.254   \n",
       "2 0.032 0.619 0.003 0.067 0.016 0.378 0.029 0.027 0.448 0.000 0.075 0.036   \n",
       "3 0.000 0.274 0.000 0.102 0.265 1.019 0.000 0.000 0.000 0.000 0.000 0.000   \n",
       "4 0.060 0.188 0.682 0.257 0.167 1.402 0.000 0.093 0.000 0.001 0.000 0.020   \n",
       "5 0.000 0.383 0.000 0.000 0.679 7.510 0.016 0.000 0.000 0.326 1.146 1.923   \n",
       "6 0.000 1.415 0.020 0.000 0.046 0.044 0.000 0.114 0.333 0.040 0.000 0.032   \n",
       "7 0.147 0.029 0.000 0.000 0.274 0.008 0.042 0.000 0.045 0.080 0.008 0.025   \n",
       "8 0.084 1.760 0.013 0.012 0.000 1.592 0.000 0.000 0.257 0.068 0.273 0.055   \n",
       "9 0.395 0.000 0.040 1.258 0.127 0.000 0.000 0.370 0.075 0.076 0.000 0.042   \n",
       "\n",
       "    T13   T14   T15   T16   T17   T18   T19   T20  \n",
       "0 0.004 0.263 0.000 0.000 0.000 0.000 0.000 3.437  \n",
       "1 0.000 0.000 0.000 0.000 0.000 0.106 0.000 0.210  \n",
       "2 0.024 0.184 0.100 0.000 0.126 0.000 0.656 0.277  \n",
       "3 0.218 0.011 0.000 0.004 1.299 0.291 1.268 0.295  \n",
       "4 1.749 0.037 0.000 0.344 0.000 0.000 0.164 0.121  \n",
       "5 0.098 0.000 0.000 0.202 0.000 0.426 0.646 0.641  \n",
       "6 0.124 0.000 0.041 0.041 0.075 0.030 0.000 0.615  \n",
       "7 0.022 0.009 0.000 0.023 0.000 0.007 0.000 0.096  \n",
       "8 0.122 0.000 0.119 0.000 0.000 0.027 0.514 0.353  \n",
       "9 0.000 0.017 0.000 0.053 0.041 0.133 0.427 0.000  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.float_format = '{:,.3f}'.format\n",
    "dt_df = pd.DataFrame(document_topics, \n",
    "                     columns=['T'+str(i) for i in range(1, TOTAL_TOPICS+1)])\n",
    "dt_df.head(10)\n",
    "# Viewing topic dominance per document using the document-topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topic</th>\n",
       "      <th>Max Score</th>\n",
       "      <th>Paper Num</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Paper Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic1</th>\n",
       "      <td>T1</td>\n",
       "      <td>1.64138</td>\n",
       "      <td>991</td>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "      <td>A Bound on the Error of Cross Validation Using \\nthe Approximation and Estimation Rates, with \\nConsequences for the Training-Test Split \\nMichael Kearns \\nAT&amp;T Research \\nABSTRACT\\n1 INTRODUCTION...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic2</th>\n",
       "      <td>T2</td>\n",
       "      <td>3.58149</td>\n",
       "      <td>383</td>\n",
       "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
       "      <td>Signal Processing by Multiplexing and \\nDemultiplexing in Neurons \\nDavid C. Tam \\nDivision of Neuroscience \\nBaylor College of Medicine \\nHouston, TX 77030 \\ndtamCnext-cns.neusc.bcm.tmc.edu \\nAb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic3</th>\n",
       "      <td>T3</td>\n",
       "      <td>5.83072</td>\n",
       "      <td>1167</td>\n",
       "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...</td>\n",
       "      <td>Reinforcement Learning for Mixed \\nOpen-loop and Closed-loop Control \\nEric A. Hansen, Andrew G. Barto, and Shlomo Zilbersteln \\nDepartment of Computer Science \\nUniversity of Massachusetts \\nAmhe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic4</th>\n",
       "      <td>T4</td>\n",
       "      <td>3.93349</td>\n",
       "      <td>1731</td>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "      <td>Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic5</th>\n",
       "      <td>T5</td>\n",
       "      <td>2.98750</td>\n",
       "      <td>33</td>\n",
       "      <td>hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, tr...</td>\n",
       "      <td>5O5 \\nCONNECTING TO THE PAST \\nBruce A. MacDonald, Assistant Professor \\nKnowledge Sciences Laboratory, Computer Science Department \\nThe University of Calgary, 2500 University Drive NW \\nCalgary,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic6</th>\n",
       "      <td>T6</td>\n",
       "      <td>7.51003</td>\n",
       "      <td>5</td>\n",
       "      <td>cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property</td>\n",
       "      <td>317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic7</th>\n",
       "      <td>T7</td>\n",
       "      <td>4.89525</td>\n",
       "      <td>1318</td>\n",
       "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
       "      <td>Comparison of Human and Machine Word \\nRecognition \\nM. Schenkel \\nDept of Electrical Eng. \\nUniversity of Sydney \\nSydney, NSW 2006, Australia \\nschenkel@sedal.usyd.edu.au \\nC. Latimer \\nDept of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic8</th>\n",
       "      <td>T8</td>\n",
       "      <td>3.67982</td>\n",
       "      <td>235</td>\n",
       "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
       "      <td>232 Sejnowski, Yuhas, Goldstein and Jenkins \\nCombining Visual and \\nwith a Neural Network \\nAcoustic Speech Signals \\nImproves Intelligibility \\nT.J. Sejnowski \\nThe Salk Institute \\nand \\nDepart...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic9</th>\n",
       "      <td>T9</td>\n",
       "      <td>4.88831</td>\n",
       "      <td>948</td>\n",
       "      <td>control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant</td>\n",
       "      <td>An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic10</th>\n",
       "      <td>T10</td>\n",
       "      <td>2.95973</td>\n",
       "      <td>1690</td>\n",
       "      <td>circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp</td>\n",
       "      <td>Kirchoff Law Markov Fields for Analog \\nCircuit Design \\nRichard M. Golden * \\nRMG Consulting Inc. \\n2000 Fresno Road, Plano, Texas 75074 \\nRMG CONS UL T@A OL. COM, \\nwww. neural-network. corn \\nA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic11</th>\n",
       "      <td>T11</td>\n",
       "      <td>6.04910</td>\n",
       "      <td>987</td>\n",
       "      <td>spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution</td>\n",
       "      <td>Information through a Spiking Neuron \\nCharles F. Stevens and Anthony Zador \\nSalk Institute MNL/S \\nLa Jolla, CA 92037 \\nzador@salk.edu \\nAbstract \\nWhile it is generally agreed that neurons tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic12</th>\n",
       "      <td>T12</td>\n",
       "      <td>6.18613</td>\n",
       "      <td>902</td>\n",
       "      <td>rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned</td>\n",
       "      <td>Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic13</th>\n",
       "      <td>T13</td>\n",
       "      <td>3.55312</td>\n",
       "      <td>1665</td>\n",
       "      <td>node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation</td>\n",
       "      <td>Boosting with Multi-Way Branching in \\nDecision Trees \\nYishay Mansour \\nDavid McAllester \\nAT&amp;T Labs-Research \\n180 Park Ave \\nFlorham Park NJ 07932 \\n{mansour, dmac}@research.att.com \\nAbstract ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic14</th>\n",
       "      <td>T14</td>\n",
       "      <td>3.98397</td>\n",
       "      <td>250</td>\n",
       "      <td>feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra</td>\n",
       "      <td>266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic15</th>\n",
       "      <td>T15</td>\n",
       "      <td>5.13757</td>\n",
       "      <td>679</td>\n",
       "      <td>classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian</td>\n",
       "      <td>A Boundary Hunting Radial Basis Function \\nClassifier Which Allocates Centers \\nConstructively \\nEric I. Chang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173-0073, USA \\nAb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic16</th>\n",
       "      <td>T16</td>\n",
       "      <td>2.71369</td>\n",
       "      <td>1124</td>\n",
       "      <td>distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum</td>\n",
       "      <td>Discovering Structure in Continuous \\nVariables Using Bayesian Networks \\nReimar Hofmann and Volker Tresp* \\nSiemens AG, Central Research \\nOtto-Hahn-Ring 6 \\n81730 Mfinchen, Germany \\nAbstract \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic17</th>\n",
       "      <td>T17</td>\n",
       "      <td>5.55943</td>\n",
       "      <td>1102</td>\n",
       "      <td>motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed</td>\n",
       "      <td>A model of transparent motion and \\nnon-transparent motion aftereffects \\nAlexander Grunewald* \\nMax-Planck Institut fiir biologische Kybernetik \\nSpemannstral]e 38 \\nD-72076 Tiibingen, Germany \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic18</th>\n",
       "      <td>T18</td>\n",
       "      <td>5.65911</td>\n",
       "      <td>537</td>\n",
       "      <td>object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint</td>\n",
       "      <td>Linear Operator for Object Recognition \\nPonen Basil Shimon Ullman* \\nM.I.T. Artificial Intelligence Laboratory \\nand Department of Brain and Cognitive Science \\n545 Technology Square \\nCambridge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic19</th>\n",
       "      <td>T19</td>\n",
       "      <td>3.73027</td>\n",
       "      <td>911</td>\n",
       "      <td>field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation</td>\n",
       "      <td>Ocular Dominance and Patterned Lateral \\nConnections in a Self-Organizing Model of the \\nPrimary Visual Cortex \\nJoseph Sirosh and Risto Miikkulainen \\nDepartment of Computer Sciences \\nUniversity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic20</th>\n",
       "      <td>T20</td>\n",
       "      <td>6.01090</td>\n",
       "      <td>72</td>\n",
       "      <td>memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol</td>\n",
       "      <td>73O \\nAnalysis of distributed representation of \\nconstituent structure in connectionist systems \\nPaul Smolensky \\nDepartment of Computer Science, University of Colorado, Boulder, CO 80309-0430 \\...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topic  Max Score  Paper Num  \\\n",
       "Topic1              T1    1.64138        991   \n",
       "Topic2              T2    3.58149        383   \n",
       "Topic3              T3    5.83072       1167   \n",
       "Topic4              T4    3.93349       1731   \n",
       "Topic5              T5    2.98750         33   \n",
       "Topic6              T6    7.51003          5   \n",
       "Topic7              T7    4.89525       1318   \n",
       "Topic8              T8    3.67982        235   \n",
       "Topic9              T9    4.88831        948   \n",
       "Topic10            T10    2.95973       1690   \n",
       "Topic11            T11    6.04910        987   \n",
       "Topic12            T12    6.18613        902   \n",
       "Topic13            T13    3.55312       1665   \n",
       "Topic14            T14    3.98397        250   \n",
       "Topic15            T15    5.13757        679   \n",
       "Topic16            T16    2.71369       1124   \n",
       "Topic17            T17    5.55943       1102   \n",
       "Topic18            T18    5.65911        537   \n",
       "Topic19            T19    3.73027        911   \n",
       "Topic20            T20    6.01090         72   \n",
       "\n",
       "                                                                                                                                                                                                           Topic  \\\n",
       "Topic1                           bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
       "Topic2               neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing   \n",
       "Topic3   state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...   \n",
       "Topic4                                image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
       "Topic5   hidden, layer, net, hidden unit, task, hidden layer, architecture, back, propagation, trained, connection, back propagation, activation, representation, generalization, output unit, neural net, tr...   \n",
       "Topic6                                       cell, firing, direction, head, rat, response, layer, synaptic, activity, spatial, inhibitory, synapsis, ii, cue, cortex, simulation, lot, active, complex, property   \n",
       "Topic7              word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state   \n",
       "Topic8                                       signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig   \n",
       "Topic9                            control, controller, trajectory, motor, dynamic, movement, forward, task, feedback, arm, inverse, position, robot, architecture, hand, force, adaptive, change, command, plant   \n",
       "Topic10                                 circuit, chip, current, analog, voltage, vlsi, gate, threshold, transistor, pulse, design, implementation, synapse, bit, digital, device, analog vlsi, element, cmos, pp   \n",
       "Topic11              spike, rate, firing, stimulus, train, spike train, firing rate, response, frequency, neuron, potential, current, fig, signal, temporal, synaptic, probability, change, timing, distribution   \n",
       "Topic12                       rule, learning rule, knowledge, category, condition, domain, symbolic, change, fuzzy, step, extraction, table, interval, eq, expert, trained, learn, activation, language, learned   \n",
       "Topic13                      node, tree, decision, level, graph, structure, decision tree, layer, probability, leaf, bound, path, variable, activation, parent, child, split, routing, architecture, propagation   \n",
       "Topic14  feature, map, task, search, classification, experiment, part, representation, target, location, feature vector, feature space, attention, test, feature map, dimensional, kernel, cluster, block, extra   \n",
       "Topic15     classifier, class, classification, decision, region, rbf, rate, error rate, test, center, nearest, layer, probability, neighbor, nearest neighbor, boundary, sample, training set, trained, gaussian   \n",
       "Topic16   distribution, probability, gaussian, mixture, variable, density, likelihood, prior, bayesian, component, posterior, em, log, estimate, sample, approximation, estimation, matrix, conditional, maximum   \n",
       "Topic17                                     motion, direction, velocity, visual, moving, target, stage, stimulus, eye, flow, filter, head, movement, location, response, signal, position, field, spatial, speed   \n",
       "Topic18                    object, view, recognition, representation, layer, visual, 3d, 2d, human, part, position, object recognition, transformation, scheme, image, aspect, frame, shape, rotation, viewpoint   \n",
       "Topic19  field, visual, orientation, stimulus, response, map, cortex, cortical, receptive, receptive field, eye, activity, center, spatial, connection, ocular, dominance, ocular dominance, region, correlation   \n",
       "Topic20  memory, representation, structure, capacity, sequence, associative, role, distributed, matrix, associative memory, bit, activity, stored, product, binding, code, local, connection, activation, symbol   \n",
       "\n",
       "                                                                                                                                                                                                      Paper Name  \n",
       "Topic1   A Bound on the Error of Cross Validation Using \\nthe Approximation and Estimation Rates, with \\nConsequences for the Training-Test Split \\nMichael Kearns \\nAT&T Research \\nABSTRACT\\n1 INTRODUCTION...  \n",
       "Topic2   Signal Processing by Multiplexing and \\nDemultiplexing in Neurons \\nDavid C. Tam \\nDivision of Neuroscience \\nBaylor College of Medicine \\nHouston, TX 77030 \\ndtamCnext-cns.neusc.bcm.tmc.edu \\nAb...  \n",
       "Topic3   Reinforcement Learning for Mixed \\nOpen-loop and Closed-loop Control \\nEric A. Hansen, Andrew G. Barto, and Shlomo Zilbersteln \\nDepartment of Computer Science \\nUniversity of Massachusetts \\nAmhe...  \n",
       "Topic4   Image representations for facial expression \\ncoding \\nMarian Stewart Bartlett* \\nU.C. San Diego \\nmarnisalk. edu \\nJavier R. Movellan \\nU.C. San Diego \\nmovellancogsc. ucsd. edu \\nPaul Ekman \\n...  \n",
       "Topic5   5O5 \\nCONNECTING TO THE PAST \\nBruce A. MacDonald, Assistant Professor \\nKnowledge Sciences Laboratory, Computer Science Department \\nThe University of Calgary, 2500 University Drive NW \\nCalgary,...  \n",
       "Topic6   317 \\nPARTITIONING OF SENSORY DATA BY A COPTICAI, NETWOPK  \\nRichard Granger, Jos Ambros-Ingerson, Howard Henry, Gary Lynch \\nCenter for the Neurobiology of Learning and Memory \\nUniversity of...  \n",
       "Topic7   Comparison of Human and Machine Word \\nRecognition \\nM. Schenkel \\nDept of Electrical Eng. \\nUniversity of Sydney \\nSydney, NSW 2006, Australia \\nschenkel@sedal.usyd.edu.au \\nC. Latimer \\nDept of ...  \n",
       "Topic8   232 Sejnowski, Yuhas, Goldstein and Jenkins \\nCombining Visual and \\nwith a Neural Network \\nAcoustic Speech Signals \\nImproves Intelligibility \\nT.J. Sejnowski \\nThe Salk Institute \\nand \\nDepart...  \n",
       "Topic9   An Integrated Architecture of Adaptive Neural Network \\nControl for Dynamic Systems \\nLiu Ke '2 Robert L. Tokaf Brian D.McVey z \\nCenter for Nonlinear Studies, 2Applied Theoretical Physics Divis...  \n",
       "Topic10  Kirchoff Law Markov Fields for Analog \\nCircuit Design \\nRichard M. Golden * \\nRMG Consulting Inc. \\n2000 Fresno Road, Plano, Texas 75074 \\nRMG CONS UL T@A OL. COM, \\nwww. neural-network. corn \\nA...  \n",
       "Topic11  Information through a Spiking Neuron \\nCharles F. Stevens and Anthony Zador \\nSalk Institute MNL/S \\nLa Jolla, CA 92037 \\nzador@salk.edu \\nAbstract \\nWhile it is generally agreed that neurons tran...  \n",
       "Topic12  Extracting Rules from Artificial Neural Networks \\nwith Distributed Representations \\nSebastian Thrun \\nUniversity of Bonn \\nDepartment of Computer Science III \\nR6merstr. 164, D-53117 Bonn, Germa...  \n",
       "Topic13  Boosting with Multi-Way Branching in \\nDecision Trees \\nYishay Mansour \\nDavid McAllester \\nAT&T Labs-Research \\n180 Park Ave \\nFlorham Park NJ 07932 \\n{mansour, dmac}@research.att.com \\nAbstract ...  \n",
       "Topic14  266 Zemel, Mozer and Hinton \\nTRAFFIC: Recognizing Objects Using \\nHierarchical Reference Frame Transformations \\nRichard S. Zemel \\nComputer Science Dept. \\nUniversity of Toronto \\nToronto, ONT M...  \n",
       "Topic15  A Boundary Hunting Radial Basis Function \\nClassifier Which Allocates Centers \\nConstructively \\nEric I. Chang and Richard P. Lippmann \\nMIT Lincoln Laboratory \\nLexington, MA 02173-0073, USA \\nAb...  \n",
       "Topic16  Discovering Structure in Continuous \\nVariables Using Bayesian Networks \\nReimar Hofmann and Volker Tresp* \\nSiemens AG, Central Research \\nOtto-Hahn-Ring 6 \\n81730 Mfinchen, Germany \\nAbstract \\n...  \n",
       "Topic17  A model of transparent motion and \\nnon-transparent motion aftereffects \\nAlexander Grunewald* \\nMax-Planck Institut fiir biologische Kybernetik \\nSpemannstral]e 38 \\nD-72076 Tiibingen, Germany \\n...  \n",
       "Topic18  Linear Operator for Object Recognition \\nPonen Basil Shimon Ullman* \\nM.I.T. Artificial Intelligence Laboratory \\nand Department of Brain and Cognitive Science \\n545 Technology Square \\nCambridge...  \n",
       "Topic19  Ocular Dominance and Patterned Lateral \\nConnections in a Self-Organizing Model of the \\nPrimary Visual Cortex \\nJoseph Sirosh and Risto Miikkulainen \\nDepartment of Computer Sciences \\nUniversity...  \n",
       "Topic20  73O \\nAnalysis of distributed representation of \\nconstituent structure in connectionist systems \\nPaul Smolensky \\nDepartment of Computer Science, University of Colorado, Boulder, CO 80309-0430 \\...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Leveraging the document-topic matrix, we can determine the most relevant paper\n",
    "# for each topic based on the topic dominance scores by using the following code.\n",
    "pd.options.display.float_format = '{:,.5f}'.format\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "max_score_topics = dt_df.max(axis=0)\n",
    "dominant_topics = max_score_topics.index\n",
    "term_score = max_score_topics.values\n",
    "document_numbers = [dt_df[dt_df[t] == max_score_topics.loc[t]].index[0]\n",
    "                       for t in dominant_topics]\n",
    "documents = [papers[i] for i in document_numbers]\n",
    "\n",
    "results_df = pd.DataFrame({'Dominant Topic': dominant_topics, 'Max Score': term_score,\n",
    "                          'Paper Num': document_numbers, 'Topic': topics_df['Terms per Topic'], \n",
    "                          'Paper Name': documents})\n",
    "results_df\n",
    "# Viewing each topic and corresponding paper with its maximum contribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs depicted in Figure 6-18 clearly show that the NMF model is much better than the LDA model, with each topic being strongly correlated as the central theme of the research paper where it has maximum dominance.\n",
    "\n",
    "What we have observed is that non-negative matrix factorization works the best even with small corpora, with few documents compared to the other methods. But again, this depends on the type of data you are dealing with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Topics for New Research Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total New Papers: 4\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "# papers manually downloaded from NIPS 16\n",
    "# https://papers.nips.cc/book/advances-in-neural-information-processing-systems-29-2016\n",
    "\n",
    "new_paper_files = glob.glob('./test_data/nips16*.txt')\n",
    "new_papers = []\n",
    "for fn in new_paper_files:\n",
    "    with open(fn, encoding='utf-8', errors='ignore', mode='r+') as f:\n",
    "        data = f.read()\n",
    "        new_papers.append(data)\n",
    "              \n",
    "print('Total New Papers:', len(new_papers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "Vocabulary not fitted or provided",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-79e8eecd0567>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnorm_new_papers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnormalize_corpus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_papers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcv_new_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnorm_new_papers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcv_new_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1265\u001b[0m                 \u001b[1;34m\"Iterable over raw text documents expected, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1266\u001b[0m                 \"string object received.\")\n\u001b[1;32m-> 1267\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         \u001b[1;31m# use the same matrix-building strategy as fit_transform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Vocabulary not fitted or provided\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
     ]
    }
   ],
   "source": [
    "# The next step in the pipeline is to preprocess these documents and extract features\n",
    "# using the same sequence of steps we followed when building the topic models.\n",
    "norm_new_papers = normalize_corpus(new_papers)\n",
    "cv_new_features = cv.transform(norm_new_papers)\n",
    "cv_new_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1.312), (7, 0.966)],\n",
       " [(2, 4.121), (0, 0.864)],\n",
       " [(3, 2.154), (1, 1.335)],\n",
       " [(3, 3.074), (6, 2.19)]]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now use our NMF topic model to predict the topics for these new research\n",
    "# papers using the following code (we predict the top two topics for each paper).\n",
    "topic_predictions = nmf_model.transform(cv_new_features)\n",
    "best_topics = [[(topic, round(sc, 3)) \n",
    "                    for topic, sc in sorted(enumerate(topic_predictions[i]), \n",
    "                                            key=lambda row: -row[1])[:2]] \n",
    "                        for i in range(len(topic_predictions))]\n",
    "best_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dominant Topics</th>\n",
       "      <th>Topic Score</th>\n",
       "      <th>Topic Desc</th>\n",
       "      <th>Paper Desc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Papers</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>131.20000</td>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "      <td>Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>96.60000</td>\n",
       "      <td>signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig</td>\n",
       "      <td>Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>412.10000</td>\n",
       "      <td>state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...</td>\n",
       "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>86.40000</td>\n",
       "      <td>bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum</td>\n",
       "      <td>PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>215.40000</td>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>133.50000</td>\n",
       "      <td>neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing</td>\n",
       "      <td>Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>307.40000</td>\n",
       "      <td>image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface</td>\n",
       "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>219.00000</td>\n",
       "      <td>word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state</td>\n",
       "      <td>Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Dominant Topics  Topic Score  \\\n",
       "Papers                                 \n",
       "1                     1    131.20000   \n",
       "1                     8     96.60000   \n",
       "2                     3    412.10000   \n",
       "2                     1     86.40000   \n",
       "3                     4    215.40000   \n",
       "3                     2    133.50000   \n",
       "4                     4    307.40000   \n",
       "4                     7    219.00000   \n",
       "\n",
       "                                                                                                                                                                                                     Topic Desc  \\\n",
       "Papers                                                                                                                                                                                                            \n",
       "1                               bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
       "1                                           signal, noise, source, filter, component, frequency, channel, speech, matrix, independent, separation, sound, ica, phase, eeg, blind, auditory, dynamic, delay, fig   \n",
       "2       state, action, policy, step, optimal, reinforcement, transition, reinforcement learning, probability, reward, dynamic, value function, markov, machine, task, agent, finite, iteration, sequence, de...   \n",
       "2                               bound, generalization, size, let, optimal, solution, theorem, equation, approximation, class, gradient, xi, loss, rate, matrix, convergence, theory, dimension, sample, minimum   \n",
       "3                                    image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
       "3                   neuron, synaptic, connection, potential, dynamic, synapsis, activity, excitatory, layer, synapse, simulation, inhibitory, delay, biological, equation, state, et, et al, activation, firing   \n",
       "4                                    image, face, pixel, recognition, local, distance, scale, digit, texture, filter, scene, vision, facial, pca, edge, region, visual, representation, transformation, surface   \n",
       "4                  word, recognition, speech, context, hmm, speaker, speech recognition, character, phoneme, probability, frame, sequence, rate, test, level, acoustic, experiment, letter, segmentation, state   \n",
       "\n",
       "                                                                                                                                                                                                     Paper Desc  \n",
       "Papers                                                                                                                                                                                                           \n",
       "1       Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...  \n",
       "1       Correlated-PCA: Principal Components’ Analysis\\nwhen Data and Noise are Correlated\\nNamrata Vaswani and Han Guo\\nIowa State University, Ames, IA, USA\\nEmail: {namrata,hanguo}@iastate.edu\\nAbstract...  \n",
       "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...  \n",
       "2       PAC Reinforcement Learning with Rich Observations\\nAkshay Krishnamurthy\\nUniversity of Massachusetts, Amherst\\nAmherst, MA, 01003\\nakshay@cs.umass.edu\\nAlekh Agarwal\\nMicrosoft Research\\nNew York,...  \n",
       "3       Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...  \n",
       "3       Automated scalable segmentation of neurons from\\nmultispectral images\\nUygar Sümbül\\nGrossman Center for the Statistics of Mind\\nand Dept. of Statistics, Columbia University\\nDouglas Roossien Jr.\\...  \n",
       "4       Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...  \n",
       "4       Unsupervised Learning of Spoken Language with\\nVisual Context\\nDavid Harwath, Antonio Torralba, and James R. Glass\\nComputer Science and Artificial Intelligence Laboratory\\nMassachusetts Institute...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember that we don’t get proportion of dominance of each topic here, like\n",
    "# with the LDA model, but we get absolute scores. Let’s view the results in an easy-to-\n",
    "# understand format.\n",
    "results_df = pd.DataFrame()\n",
    "results_df['Papers'] = range(1, len(new_papers)+1)\n",
    "results_df['Dominant Topics'] = [[topic_num+1 for topic_num, sc in item] for item in best_topics]\n",
    "res = results_df.set_index(['Papers'])['Dominant Topics'].apply(pd.Series).stack().reset_index(level=1, drop=True)\n",
    "results_df = pd.DataFrame({'Dominant Topics': res.values}, index=res.index)\n",
    "results_df['Topic Score'] = [topic_sc for topic_list in \n",
    "                                        [[round(sc*100, 2) \n",
    "                                              for topic_num, sc in item] \n",
    "                                                 for item in best_topics] \n",
    "                                    for topic_sc in topic_list]\n",
    "\n",
    "results_df['Topic Desc'] = [topics_df.iloc[t-1]['Terms per Topic'] for t in results_df['Dominant Topics'].values]\n",
    "results_df['Paper Desc'] = [new_papers[i-1][:200] for i in results_df.index.values]\n",
    "\n",
    "results_df\n",
    "# Predicting topics for new papers with our NMF model\n",
    "# make sense and our NMF model is working quite well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persisting Model and Transformers\n",
    "\n",
    "### This is just for visualizing the topics in the other notebook (since PyLDAViz expands the notebook size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill\n",
    "\n",
    "with open('nmf_model.pkl', 'wb') as f:\n",
    "    dill.dump(nmf_model, f)\n",
    "with open('cv_features.pkl', 'wb') as f:\n",
    "    dill.dump(cv_features, f)\n",
    "with open('cv.pkl', 'wb') as f:\n",
    "    dill.dump(cv, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
